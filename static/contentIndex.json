{"8-stage-personal-story-framework-for-personal-branding":{"title":"8-stage Personal Story Framework for Personal Branding","links":[],"tags":["Self","Story"],"content":"\nThe 8-stage personal story framework for personal branding can help individuals to craft a compelling and authentic story that will help them to achieve their personal and professional goals. The framework can be used to identify the key elements of one’s story, including the problem, the journey, the transformation, and the outcome.\nThe framework can help brainstorm ideas for content or strategies for marketing oneself. Ultimately, by using this framework, individuals can create a powerful and differentiated personal brand that will allow them to stand out from the competition.\nThe 8-stage personal story framework is designed to help you communicate your identity in the form of a story. It’s a way of thinking about and structuring your story with a clear purpose and message.\nThe 8 stages of the personal story framework are:\n\nThe Setup - Introduce yourself\nThe Journey - Describe your background and how it has shaped who you are today\nThe Destination - Explain what motivates you and what you are passionate about\nThe Return - Share a significant event or experience that has affected you deeply\nThe Reflection - Describe how you have changed as a result of this event or experience\nThe Legacy - What lessons have you learned from this story and how will you apply them going forward?\nThe Moral - What message do you want the reader to take away from your story?\nThe Meaning - Why was it important for you to share this story and what does it say about who you are as a person?\n\nThis framework works because it is a way to structure a story that is easy for the listener to follow. It also allows you to focus on the most important aspects of your story and leaves out anything that is not essential. When used correctly, the 8-stage personal story framework can be a powerful tool for communicating your identity.\nThe Eight Stages of Personal Story\nThe Setup\nThis is where you introduce the character or protagonist of your story. You will also need to establish the conflict or problem that needs to be resolved. Start with the situation or problem that you want to address. What’s going on in your life that you want to change or improve? Introduce yourself and your main character. Who are you, and what role do you play in the story?\n\nI am Sarah, and I am the protagonist of this story. I am a college student who is struggling to pay for my tuition. I have been working two jobs to try to make ends meet, but it is not enough. I need to find a way to get more money so that I can stay in school.\n\nThe conflict in this story is that I do not have enough money to pay for my tuition. I need to find a way to get more money to stay in school and graduate.\nThe Journey\nThis is the main part of the story, where the character goes on a journey to try and resolve the conflict. Along the way, they will face challenges and obstacles. Describe the challenges and obstacles that your character faces. What stands in the way of achieving their goal?\n\nThe first challenge I face is finding a way to get more money. I have been working two jobs, but it is not enough. I need to find a way to make more money so that I can stay in school and graduate.\nThe second challenge I face is overcoming my own pride. I do not want to ask for help from anyone, but I need to overcome this so that I can get the money I need.\nThe third challenge is finding someone who is willing to help me. This person needs to be able to give me the money I need without asking for anything in return.\n\nThe challenges and obstacles that the character faces are mainly internal. They have to overcome their fears and doubts to achieve their goal. They also have to deal with the negative aspects of their past to move forward.\nThe Destination\nThis is where the character finally arrives at their destination and resolves the conflict. Often, there is a twist at this stage that changes everything that came before it. Show how your character overcomes these challenges and achieves their goal. What did they learn or discover along the way? How did they grow as a result of their journey?\n\nAfter much soul searching, I have come to the conclusion that I need to ask for help. I am going to ask my family and friends for money so that I can stay in school and graduate.\nI have also decided to overcome my pride and ask for help from anyone who is willing and able to give it. I will not be proud or ashamed to ask for help, because I know that it is what I need to do in order to achieve my goal.\nFinally, I have found someone who is willing to help me. This person has given me the money I need without asking for anything in return. They have helped me without expecting anything in return, and this has shown me the true meaning of kindness.\n\nThe Return\nAfter resolving the conflict, the character often returns home changed by their journey. They may have learned something new about themselves or others along the way. What did you learn about yourself? How did this experience change you?\n\nI have learned that I am capable of more than I ever thought possible. I have also learned that there are people in this world who are willing to help me. This experience has changed me in a number of ways, but most importantly, it has given me the confidence to believe in myself and my ability to achieve my goals.\n\nThe Reflection\nThis is where you reflect on what happened during the story and how it affected you or your protagonist. What lessons were learned? What did this experience teach us? How did we grow from it? Reflect on what this experience has taught you.\n\nI learned that I am capable of more than I ever thought possible. This experience has taught me to be more confident and independent. I have also learned that when push comes to shove, I can rely on others to help me get through tough times.\n\nThe Legacy\nOnce you’ve reflected on the experience, you can think about what legacy this experience has left behind. How has it affected our lives going forward? What will we remember most about it? Share what you would like the reader to take away from your story. What message do you want to communicate?\n\nI want the reader to take away from my story that it is possible to overcome anything with perseverance and a positive attitude. I also want the reader to know that life is full of surprises, both good and bad, and that it is important to savor the good moments.\n\nThe Moral\nFinally, a good personal story should have a moral or message at its core. What was ultimately trying to be communicated through this tale? Offer a call to action. What do you want the reader to do after reading your story?\n\nThe moral of my story is to never give up on yourself. There will always be people who believe in you, even when you don’t believe in yourself. You are capable of anything you set your mind to.\n\nThe Meaning\nAnd finally, after considering all of these elements, think about what meaning this story has for you. Why was it important for you to share it? What does it say about who you are as a person? End with a sense of resolution and closure. What happened after the events of your story? How did everything turn out in the end?\n\nThis story is important to me because it shows that I am a strong and resilient person. No matter how many times I get knocked down, I always get back up again. I never give up on myself or on my dreams. This story also shows that even when things seem hopeless, there is always a chance for things to turn around.\n\nFinishing the 8-stage personal story framework\nThe 8-stage personal story framework is an effective tool for helping you communicate your identity in a short, powerful story. The framework is designed to help you communicate your identity clearly and concisely. It can help you clarify your purpose and message, ensure that your story leaves a lasting impression on the reader, and can be adapted to different purposes depending on what you want to communicate."},"alternatives-to-fine-tuning":{"title":"Alternatives to Fine-tuning","links":[],"tags":["AI","ChatGPT","Embeddings","GPT-3","LLM"],"content":"\nPeople commonly refer to fine-tuning when attempting to address their problems with AI. In a recent conversation with an OpenAI Ambassador, I was told that they “convince 90% not to use fine-tuning to solve their problem,” confirming my hunch that fine-tuning is one of the most misunderstood tools in AI.\nFine-tuning is often more complicated and less effective than people realize.\nTo start, it requires a sufficiently large and properly labeled dataset. Data augmentation techniques like synonym replacement, random insertion, and random deletion can increase the amount of training data artificially, but this still requires a substantial investment.\nAnother potential problem is whether the data fits the intended solution. Fine-tuned models aren’t as flexible as the ChatGPT most people are familiar with. The fine-tuned model is designed to serve a narrow purpose based on your training data. Even ChatGPT, a fine-tuned model, has a very narrow focus on providing answers for a mainstream audience. So you have to consider whether the training data accurately represent the desired output from the model you’re fine-tuning. Because if it doesn’t, your fine-tuned solution isn’t likely to perform as expected.\nClassification\nWhat if you need to classify incoming content as good, bad, relevant, off-topic, important, etc.?\nPast machine learning tools required significant overhead in both development time and experience with niche tools to achieve this same outcome. But now there’s an easier solution!\nIt’s possible to build a simple classifying algorithm with the OpenAI Embeddings API and calculations that can run on any computer or even in a spreadsheet!\nThe process is quite simple:\n\n\nCreate two or more groups of embeddings based on your available data.\n\n\nSimilarly, get the embedding for the new data you would like to classify.\n\n\nCompare the embeddings, often using a process known as cosine similarity, to determine which group is most similar to the new data.\n\n\nThe cosine similarity returns a decimal between zero and one, which can easily translate into a percent for easy interpretation. The higher the similarity, the higher the likelihood that the data belongs to the group.\nQuerying\nWhat if you want to ask questions in the context of your existing data? Another common reason people believe they need fine-tuning is to get answers based on their documents.\nBut what if your data is unstructured? Like the files in your Google drive. It doesn’t matter if you aren’t fine-tuning! This is where combining embeddings with the latest text generation models shines!\nThis video by David Shapiro does a great job introducing the differences between fine-tuning and semantic search when seeking to integrate your data with AI.\nWithout getting too technical:\n\n\nStart by generating “embeddings” for the existing data that will be used as context when the AI responds to your questions.\n\n\nUse the embeddings to surface the most relevant content based on your question to the AI before the GPT-3/ChatGPT model receives your prompt.\n\n\nInsert the results from step #2 inside the prompt that’s inputted to the GPT-3/ChatGPT model\n\n\nBy including the search results as the context within a prompt, you can instruct the model to return a well-formatted answer to the original question based while including specific information without fine-tuning.\nPopular open-source software to implement this is GPT Index.\nWhen to Fine-tune\nAs mentioned in the above video, there are valid reasons to use fine-tuning. Though, if you don’t already have an existing AI solution, then starting with fine-tuning is probably not the best direction.\nIf you have an existing solution, for example, that uses the Davinci model to answer a very narrow set of questions at high volume, then maybe fine-tuning a Curie model could significantly reduce your costs.\nMuch more can be written about the use cases for fine-tuning. But before you further research the process of fine-tuning, I encourage the development of an AI system without it. This will make it easier to deploy and increase its flexibility for future improvement. And when you’re ready, fine-tuning will be there to help.\nWhat’s next?\nDo you need help designing AI systems for your business? I can help. Schedule a consultation to learn more!"},"an-intro-to-getting-things-done-gtd-horizons-projects-context-lists":{"title":"An Intro to Getting Things Done (GTD): Horizons, Projects, & Context Lists","links":[],"tags":["Self"],"content":"\nIntroduction: What is GTD and how can it help you be more productive?\nGetting Things Done, or GTD for short is a time management system created by David Allen. The basic principle of GTD is to get all the things constantly buzzing around in your head out of your head and into a system where you can track and manage them. This frees up your mind to focus on the task at hand and allows you to see the big picture of what you need to do.\nThere are a lot of productivity systems out there, but Getting Things Done (GTD) is one of the most popular and well-respected. So what is GTD, and how can it help you be more productive?\nGTD is a time management system that was created by David Allen. It’s based on the principle that if you capture all of your tasks and ideas in one place, you can clear your mind and focus on what’s important. This system has helped many people organize their lives and increase their productivity.\nOne of the great things about GTD is that it can be customized to fit your needs and lifestyle.\nThe core idea of GTD is to capture everything that is taking up space in your brain into what Allen calls a “trusted system.” This can be simple as a notebook or an app on your phone. The goal is to get everything out of your head so you can relax and not worry about forgetting anything.\nOnce everything is in your trusted system, you can start to process it all. This means going through each task and deciding the next action step. For some tasks, this might be enough - you can just do it right then and there. Other tasks might require more thought or planning, and you must break them down into smaller steps.\nThe key with GTD is to have a system where you know exactly what needs to be done next for each task so that you can focus on actually doing the work instead of worrying about what needs to be done next.\nThe 5 Horizons of Focus: what they are and how they can help you better achieve your goals\nIn his book “Getting Things Done”, David Allen outlines a framework for productive living which he calls the “5 Horizons of Focus”. This framework is designed to help individuals identify and maintain focus on their most important tasks and goals. The 5 Horizons of Focus are as follows:\n\nHorizon 1: Projects - These are tasks or goals that require more than one action to complete and are typically completed within a year.\nHorizon 2: Areas of focus &amp; accountability - These areas of your life or work require maintenance to keep them running smoothly. Examples might include health, finances, customer service, or planning.\nHorizon 3: Goals &amp; objectives - These are outcomes you want to achieve in the next three to twenty-four months. They should be specific, measurable, achievable, relevant, and time-bound (SMART).\nHorizon 4: Vision - This is a long-term scenario of success that you can strive for. It should be inspiring and motivating, keeping you focused on your ultimate goal even when times are tough.\nHorizon 5: Purpose - This is why you are doing what you are doing. Your purpose should give your life meaning and direction, something you are passionate about.\n\nThe 5 Horizons of Focus can help you achieve your goals by providing a clear framework to guide your actions. Understanding which horizon each task or goal falls into allows you to prioritize your time and efforts more effectively. Additionally, the 5 Horizons of Focus can help keep you motivated by reminding you of your ultimate goal (Horizon 4) and why you are striving for it (Horizon 5).\nProjects: how to break down a project into smaller, more manageable tasks\nBreaking a project into smaller tasks is one of the most important aspects of Getting Things Done. By breaking a project down into smaller tasks, you can more easily manage your time and resources, and complete the project on time.\nThere are a few different ways that you can go about breaking down a project into smaller tasks. One way is to create a task list for the project. This list should include all of the individual steps that need to be completed to finish the project. Once you have this list, you can start to break each task into even smaller sub-tasks. For example, if one of your tasks is to write an article, you can break this down into smaller tasks such as research, writing, editing, and proofreading.\nAnother way to break down a project is by using a mind map. This is especially helpful if the project is large and complex. A mind map allows you to visually see all of the different elements of the project and how they are interconnected. This can help you better understand what needs to be done and identify potential roadblocks.\nOnce you have broken down your project into smaller tasks, it will be much easier to manage your time and resources and complete the project on time.\nContextual Lists: grouping actions together based on context (location, energy required, etc.)\nOne of the key principles of Getting Things Done is to group related actions based on context. If you have several tasks that need to be completed, you should group them based on where they need to be done, how much energy they will require, and so on.\nFor example, if you need to buy groceries and pick up your dry cleaning, you would group those two tasks under the heading ” errands.” If you need to write a report and make a presentation, you would group those two tasks under the heading “work.” And if you need to call your Mom and walk the dog, you would group those two tasks under the heading “home.”\nContextual lists help you use your time more efficiently by considering all the different factors that go into completing a task. This system can be used for any type of task, whether it’s personal or professional. So next time you feel overwhelmed by all that needs to be done, take a step back and break things down into manageable chunks by creating contextual lists.\nOne key thing to keep in mind with contextual lists is that they should be dynamic – that is, they should always be changing and evolving as your needs change. Don’t be afraid to add or remove items from your lists as needed; it’s all part of the process!\nConclusion\nThere are a lot of different productivity systems out there, and it can be tough to figure out which one is right for you. The GTD system is a great option for people who want a simple, effective way to organize their tasks and get things done.\nThe 5 Horizons of Focus help you prioritize your actions to focus on the most important things first. Projects are a great way to break down complex tasks into smaller, more manageable pieces. And Contextual Lists help you group actions based on factors like location or energy required to easily find the right task for the situation.\nWith GTD, you’ll have all the tools you need to take control of your workflow and get things done efficiently. Give it a try today!"},"asking-gpt-3-about-prompt-engineering-experiment":{"title":"Asking GPT-3 about prompt engineering experiment","links":[],"tags":["GPT-3"],"content":"\nFirst, I asked GPT-3 to define prompt engineering. Second, I asked about the important parts of a prompt. I was surprised that the response suggested including motivation for the model. So, third, I asked what might make the prompt motivating. These are the results.\nTo understand what goes into making a good prompt, I first consulted GPT-3. According to the definition, engineering a prompt involves specifying conditions or providing input to elicit the desired output from a machine learning model.\nSo I posed my second question: What are some of the most important aspects of a well-crafted prompt? Once again, GPT-3 did not disappoint me with its answer; It said that one of the key elements is motivating the model.\nThis made me curious about what sorts of things could be done to make a prompt more motivating for a machine-learning algorithm. After doing some research on my own, I found that three main strategies seem effective: firstly, adding emotional content; secondly making use of personal pronouns such as “you” and “I”; and thirdly being specific.\noutputs from the experiment\ndefine prompt engineering\nPrompt engineering is a branch of machine learning that deals with developing queries, specifying conditions, and providing input to elicit the desired output from a machine learning model system that can automatically generate natural language responses.\nimportant parts of prompts\n\nThe goal of the prompt should be clear. The prompt should be designed to elicit the specific information that is needed from the language model.\nThe prompt should be concise. It should be focused on a single task or concept, and avoid unnecessary distractions.\nThe prompt should be engaging. It should be interesting and motivating for the language model to respond to, in order to encourage high-quality responses.\n\nmake the prompt motivating\n\n\nWhat is the purpose of the prompt?\n\n\nKeep the prompt focused and specific.\n\n\nMake sure the prompt is grammatical and well-formed.\n\n\nChoose your words carefully.\n\n\nStart with a trigger word or phrase that will get the model’s attention\n\n\nFollow with one or two key terms that are relevant to your question or query\n\n\nMake sure the prompt is clear and concise.\n\n\nMake sure the prompt is focused on a specific topic.\n\n\nMake sure the prompt is interesting and engaging.\n\n\nMake sure the prompt is motivating for the large language model to respond to (???)\n\n"},"beyond-time-savings-evaluating-the-full-value-of-ai-systems":{"title":"Beyond Time Savings: Evaluating the Full Value of AI Systems","links":[],"tags":["AI","AI-Your-Business","Future"],"content":"\nWith the global AI market expected to reach $1.6 Trillion by 2030, it’s more important than ever to understand how to evaluate the impact and value of AI systems.\nSo, how do you measure the value of an AI system? It’s a complex question, but some key factors can help you determine the valuation of an AI system. And while it may be challenging to assign a dollar amount to some of the benefits of AI, it’s still possible to gain clarity and understand its impact.\nAs a consultant specializing in designing AI systems, one of the most common questions I get asked, “How do you measure the value of an AI system?” It’s a complex question with no easy answer, but some key factors can help you determine the valuation of an AI system. So let me guide you through the process and help you understand the potential value of AI in your business.\nIn this article, we’ll explore different ways to evaluate the impact and value of AI systems, including time-saved, increased positive outcomes, decreased negative outcomes, and improved perception.\nLet’s dive into these factors and explore how you can use them to measure the value of AI systems.\n\nTime-saved\nOne of the most significant benefits of AI systems is the time they save. In addition, by automating tasks that would otherwise require human intervention, AI can free up employees to focus on higher-level work.\nFor Employees\nTo calculate the value of time saved for hourly employees, we can use the following formula:\nAnnualized time * Hourly cost\nLet’s say an AI system saves an employee 10 hours per week, and that employee’s hourly rate is 50. That&#039;s a weekly savings of 500, or 26,000 per year (50 x 10 x 52).\nPretty simple.\nFor Buyers\nHowever, for buyers of AI systems, like entrepreneurs, executives, and investors, the value of time saved may be more difficult to quantify. For them, the value of time saved may be more subjective, such as not missing out on important life events or spending more time with loved ones.\nConsider the value of not missing out on “life” - for example, a personal assistant AI that helps busy executives manage their schedule and allows them to attend their child’s soccer game. While it may be difficult to assign a dollar value to this “life experience,” it’s important to consider its impact on the value of the AI system.\nIn the next sections, we’ll explore other ways AI systems can provide value, including increasing the probability of positive outcomes, decreasing negative outcomes, and improving perception.\nIncreasing Positive Outcomes\nAnother way to measure the value of AI systems is by their ability to increase the probability of positive outcomes. These positive outcomes can be easy to measure monetarily, like acquiring a customer, decreasing churn, gaining investment, or getting acquired. But other non-monetary outcomes may be harder to measure, like improving brand perception, going viral, or gaining attention.\nAn AI system can increase the probability of positive outcomes in three ways:\n\n\nIncreasing quantity\n\n\nIncreasing quality\n\n\nBoth quantity and quality\n\n\nQuantity\nAI systems can assist employees and increase their output by researching and writing drafts. For example, by enabling employees to send more emails and publish more content, the AI system creates more opportunities for engagement with potential customers and investors.\nQuality\nAI systems can analyze data and provide insights that lead to better decision-making. For example, an AI system can analyze customer behavior data and provide the following:\n\n\nInsights on how to improve customer experience.\n\n\nLeading to increased customer satisfaction and loyalty.\n\n\nIncreasing the lifetime value of the customer.\n\n\nQuantity &amp; Quality\nAI systems that improve quantity and quality can be the most valuable. A typical example of this is on social media. An AI system that analyzes social media trends and user behavior can assist in creating content that better resonates with the target audience, increasing the likelihood of going viral.\nBy increasing the probability of positive outcomes, AI systems can provide significant value to businesses. The ability to make better decisions, acquire more customers, and improve brand perception can help companies achieve their goals, leading to long-term success and growth.\nMeasuring\nCalculating the value of getting more customers or keeping existing ones happy can take time and effort. But here are some ways to do it. First, it’s important to identify all positive outcomes value for each.\n\n\nEconomic outcomes: Revenue gained or costs saved.\n\n\nAttention outcomes: Number of likes, shares, and comments on social media or website engagement.\n\n\nBrand perception outcomes: Surveys or focus groups that measure changes in brand perception over time.\n\n\nPriceless outcomes: For example, if a medical AI system can diagnose a patient’s condition faster than a human doctor, that could mean the difference between life and death.\n\n\nTo calculate a value based on increased positive outcomes, you should make projections based on past performance to compare with actual performance after implementing the AI system.\n\n\nIn the following sections, we’ll explore how AI systems can also decrease negative outcomes and improve perception.\n\nDecreasing Negative Outcomes\nIn addition to increasing the probability of positive outcomes, AI systems can decrease the likelihood of negative outcomes. For example, an AI system that analyzes customer complaints can provide insights on how to improve products or services, reducing the likelihood of unhappy customers and negative reviews.\nDecreasing negative outcomes can also have a significant monetary impact. By reducing customer churn and improving customer satisfaction, companies can retain more business and avoid the costs associated with acquiring new customers.\nMeasuring the value of decreasing negative outcomes can also be challenging. To calculate the savings attributed to the AI system, start by identifying the potential negative outcomes and estimating their associated costs, for example, the cost of losing a customer, negative reviews, or damage to a company’s reputation. Next, track the number of these outcomes over an easily comparable period, like yearly, and make projections based on past performance.\nCalculate the difference between the projected costs based on past performance and actual costs incurred after implementing the AI system to get the value in savings attributed to the AI system.\nImproving Brand Perception\nAI systems can also help improve brand perception in several ways. For one, publicizing the use of AI systems gains the immediate perception of being an innovative and capable tech company, which is becoming necessary in our society that’s increasingly dependent on technology. And by maintaining a reputation of being on the cutting edge of tech, it can be easier to attract top talent and stay ahead of competitors.\nAnother way is personalizing customer experiences and providing relevant recommendations. For example, an AI system can provide tailored recommendations that increase customer satisfaction and loyalty by analyzing customer data.\nMaintaining a positive brand perception is crucial for long-term success and can lead to increased customer loyalty, word-of-mouth referrals, and a positive reputation within the industry.\nCase Studies\nCase Study #1: Personal Assistant AI\nA personal assistant AI system saved an executive 10 hours per week by managing their schedule and avoiding conflicts. The executive’s hourly rate is 200. Therefore, the annualized value of the time saved is 104,000 ($200 x 10 x 52), plus the non-quantifiable but equally valuable opportunity to attend their daughter’s soccer games.\nCase Study #2: AI-Powered Customer Service\nA company implements an AI-powered customer service system that can handle simple inquiries and redirect more complex issues to human representatives. This AI system reduced the number of customer service representatives needed by 50%, resulting in a cost savings of $500,000 annually.\nFurthermore, the AI-powered system improved customer satisfaction by providing faster and more accurate responses, leading to fewer unhappy customers and lower costs for resolving complaints.\nCase Study #3: Fraud Detection AI\nA fraud detection AI system helps a financial institution detect fraudulent activity and prevent unhappy customers. The AI system detected 20 instances of fraud per month, saving the company an estimated $50,000 per month in losses.\nCase Study #4 AI-powered Chatbot\nA mid-sized e-commerce company implemented an AI-powered chatbot to provide instant customer support. As a result, the chatbot reduced the average response time from 24 hours to just a few minutes.\nAs a result, the company saw a 30% increase in customer satisfaction and a 20% decrease in unhappy customers. Moreover, the chatbot was available 24/7, meaning customers could get help outside business hours. The company estimated that the chatbot saved them $50,000 annually in employee costs.\nCase Study #5: AI-powered Investment System\nA mid-sized investment firm implemented an AI-powered system that analyzed investment data and provided insights to investors. As a result, the system helped investors make better decisions, which resulted in a 10% increase in investment returns.\nAdditionally, the system analyzed data more accurately and faster than humans, which helped the investment firm stay ahead of the competition. As a result, the company gained more investors and increased its revenue.\nDecision Methodology\nTo decide whether the value of an AI system will provide a positive return on investment, you can use the following steps:\n\n\nIdentify the specific benefits of the AI system.\n\n\nDetermine the necessary metrics for calculating the value of each benefit.\n\n\nCalculate the annualized value of each benefit using the identified metrics.\n\n\nConsider additional benefits, such as improved customer satisfaction or perception, and assign a subjective value to them.\n\n\nCompare the total value of the AI system to the cost of implementing and maintaining it.\n\n\nValuing AI Systems\nMeasuring the value of AI systems can be challenging, but it is crucial to justify the investment.\nIt’s important to remember that the value of AI systems extends beyond just time saved. By increasing the probability of positive outcomes, decreasing negative ones, and improving perception, AI systems can provide significant value to businesses.\nWhile measuring the value of AI systems can be challenging, it’s essential to identify all possible outcomes and estimate the costs or value associated with them.\nWith the latest advances, AI systems are becoming valuable investments in more than just the largest enterprises. As AI technology advances, we can expect it to provide even more value and higher ROI to companies of all sizes.\nAre you ready to learn more about how AI systems can benefit your business? Schedule a consultation."},"building-a-competitive-advantage-moat-for-ai-start-ups-in-2023":{"title":"Building a Competitive Advantage (Moat) for AI Start-ups","links":[],"tags":["AI","Future","GPT-3","LLM","Prediction","Prompt-Engineering","Vision"],"content":"\n“Pre-trained model” is the name I will use to refer to AI models made accessible by companies like OpenAI, with capabilities including text, image, and embedding generation. You can contrast these pre-trained models with custom models trained to solve a specific task using “big data.”\nIt’s 2023, and there are still growing start-ups with billion-dollar valuations that rely on a single pre-trained model.\nFor example, companies like Jasper and Copy.ai have focused on designing user interfaces and prompt engineering to differentiate using a single type of pre-trained model, the generative pre-trained transformer, GPT for short. Fortunately, both are still young companies. So both are likely to have the runway, momentum, and ambitious talent essential to adapt using the strategies in this article.\nBut these companies have painted a seemingly simple version of the AI future. And that sparked a skills debate. Which skill will be most important for our new AI-enabled future? You can find arguments for and against both frontend development and prompt engineering as the most important skill.\nHowever, companies focused on either of those two domains, frontend development or prompt engineering, see copycat products emerging as fast as they can build. And that insecurity has prompted an even bigger debate than the one around whose skills will be most valuable.\n”The Moat Question”\nA bigger debate has emerged: how to build a competitive advantage, also known as a moat, around businesses with AI-enabled products.\nIt’s one of the biggest, if not the biggest, question that executives, investors, and founders still have about our AI future. People are engaging in fiery debates about whether such a moat is even possible! And many people throw in the towel and accept that no one seems to have a satisfying answer to “the moat question.”\nA Level Playing Field\nSince pre-trained models have leveled the playing field for shipping AI-enabled products, it is clear that you will need more than development around a single pre-trained model to sustain a competitive edge, no matter how pretty the design or how advanced the prompt is.\nNor will the “data moat,” the strategy that has sufficed for over a decade, will not be significant enough of a competitive advantage. It too will fail, as its castle falls into disrepair, overshadowed by a new skyline, with the king wondering where everyone, and the proceeds from their data tax, has gone.\nA New Opportunity\nNaturally, entrepreneurs, leaders, and product designers must search for new ways. They must design more complex products than simply building a frontend application around an API endpoint.\nThey need to design systems that create a new type of competitive advantage while leveraging the capabilities of the latest AI technology. They must find synergy between pre-trained models and build robust yet flexible systems that can evolve to provide increased value with each advancement in the underlying technology.\nThe “Data Moat”\nFor the past decade, many companies that succeeded with AI products maintained market dominance by collecting troves of data, a “data moat,” which they used to train their product consisting of a proprietary custom model. This “data moat” allowed them to perform better, and prevent competition, simply by hoarding data.\nBut the “data moat” has dried up, and the crocodiles are nowhere to be found.\nHighly-capable pre-trained models, made available by companies like OpenAI, have ended the era of the “data moat” for nearly all companies, with maybe the exception of those providing the pre-trained models as a service.\nIn this new era, a single pre-trained model could be used to quickly build products that outperform those from businesses that previously relied on a “data moat.”\nI predict that companies continuing to believe in their “data moat” will be recognized as the first to fall since this recent wave of pre-trained models. Some may even do so quite spectacularly. As a company, it will be challenging to realize that its entire strategy is hoarding value instead of creating it.\nWhile the fall from the top is never easy, this level playing field brought on by the accessibility of pre-trained models will allow new value creators to benefit us all.\nNote: Don’t feel bad for the “data moat.” I’m holding back here, but I want to say the “data moat” is a bad strategy. Not bad as unprofitable, but bad as harmful to the ecosystem. In no unclear terms, a “data moat” is monopolistic.\nThe First Wave\nSome people may believe that the recent wave of pre-trained models is a new phenomenon. However, you can compare the pre-trained models to the natural language processing (NLP) services that began to pop up over a decade ago.\nThese NLP services, also available via API, were the first wave of off-the-shelf AI components. They had specific functions such as keyword extraction, summarization, and sentiment analysis.\nAlthough each NLP service had a limited function on its own, when stacked together, they were able to build impressive systems that are still in use today.\nThese NLP services, the original pre-trained models, were simple enough that it would have seemed like poor judgment if I had just repackaged them individually to sell as a new product with a shinier user interface.\nInstead, by designing a system that implemented sequences of these NLP services, I built an autonomous value creating a system that grew a significant audience and continues to inspire others to try to develop similar strategies today.\nCase Study: Top Feed\nGoing back to around 2013, an early start-up I worked on enabled businesses to capture value from curated content. I used the product to build an audience around bleeding-edge technologies in the frontend development space. But curating content took a lot of time and was something I knew I could automate.\nWhile it started with a simple script and keyword matching, my curating system quickly improved in efficiency and autonomy as I discovered the various NLP services that were becoming available.\nMy system, code-named “Top Feed,” continually improved how it curated niche content. I designed “Top Feed” to deconstruct content based on contextual indicators like keywords and sentiment sourced from NLP services, the original pre-trained models.\nUsing a Bayesian network approach, a system for keeping the score of the indicators relative to the niche, well-performing content based on clicks and engagement, would increase its contextual indicators’ future relevance.\nNew content was regularly added to the system using a Twitter search of keywords that the system determined most relevant. When a piece of content was deemed sufficiently relevant, another NLP service would summarize it before distributing it to the audience.\nMy “Top Feed” operated for years after I stopped developing it and enabled me to grow audience numbers to the hundreds of thousands in the JavaScript developer niche. And I used a similar system to grow a WFH audience to almost a million followers since the pandemic.\nI did all this without using custom-trained models, which were simultaneously becoming more accessible and easier to develop. But instead, my “Top Feed” succeeded by utilizing carefully arranged off-the-shelf NLP services, the original pre-trained models, if you will, to create synergy and produce a valuable system.\nThe Current Wave\nIt’s easy to misunderstand this latest wave of advancements because the abilities of pre-trained models, like OpenAI’s ChatGPT and Bing’s Sydney, seem nearly infinite in scope.\nBut, while they may seem infinitely robust, each pre-trained model available today has limitations. That’s true for “GPT 3.5” and will remain true for the inevitable GPT-4 and whatever comes next!\nThe truth is each of these pre-trained models serves a particular function you can learn to understand. It will take some time since they function much differently than our standard computational components. And the reason these recent examples seem so capable has more to do with their ancillary systems, including human feedback loops, than the underlying pre-trained models themselves.\nBuilding Blocks\nUnderstanding the limits and possibilities is necessary to begin exploring how these pre-trained models may be used together in a system. You can imagine these pre-trained models as building blocks. And knowledge of the building block materials is an essential first step in architecting a durable, long-lasting, competitive advantage.\nBy “stacking” together multiple pre-trained models, creating a significantly more advanced and robust system becomes possible. So, while it may be tempting to focus on a single pre-trained model by fine-tuning and prompt engineering, it’s important to remember that the actual value lies in designing the system. Notably, a system that creates synergy amplifies each component’s output. With this approach, you can gain a competitive advantage. Your “moat.”\nPrompt Chains\nIf you’re reading this, you’re familiar with the concept of prompt chains. At the time of this writing, a product with a prompt chain is far ahead of its competition that continues to focus on engineering a single prompt.\nThese products containing “prompt chains” may utilize pre-trained models with different capabilities, prompts, and fine-tuning to maximize the accuracy and efficiency of the results they provide users.\nIn hindsight, we will recognize prompt chains as the infancy of developing AI systems, akin to the first multi-cellular organisms when all previous life was single-celled. While trying not to minimize this vital development, I predict that future systems will simultaneously see an increase in prompts and a decrease in the ratio of components that utilize prompts.\nEmbeddings Systems\nEmbeddings systems provide a way to organize memories for the AI. They are also used to numerically represent non-numerical data, like text and multi-media, for efficient comparison using traditional computations.\nMore recently than prompt chains, products that integrate embeddings systems have exploded in numbers.\nOpenAI lists a half-dozen ways embeddings you can use embeddings in a system, including search, clustering, recommendations, anomaly detection, diversity measurement, and classification. Furthermore, you can embed content in nearly as many ways as you can write a prompt to achieve a similar result with similarly varying accuracy and cost.\nMy prediction is that, as useful as prompt chains have become, a chain of embeddings may become even more helpful. And mature AI systems will implement embeddings before and after every generative pre-trained model, whether for adding context or criticizing outputs.\nThe “System Moat”\n“System Moat” is the name I use to describe an AI system consisting of multiple pre-trained models such that their arrangement becomes valuable as a design.\nThe “System Moat” is the competitive technological advantage you can achieve with pre-trained models. Unlike designs for many other products, “System Moats” are unlikely to benefit from being patented, though they should remain a corporate secret.\nInstead of gaining value from a single patented design, the “System Moat” should gain strength from its flexibility. As the AI ecosystem advances, new pre-trained model components become part of and improve the system’s capabilities.\nThe system’s complexity may develop until it’s necessary to organize sub-systems. Then, you may form development teams around these sub-systems, and only principal architects may understand the complete system—similar to complex software applications today.\nA “System Moat” visual may look like the flow chart or diagram from any other software application. The difference is, where in past systems components were made of computational functions, components of the “System Moat” consist of pre-trained models. Traditional computational processes become transient requirements to hold the AI system together. They are most valuable when they’re easy to replace and reshape to make room for future renovations.\nUltimately, the “System Moat” will evolve into a high-level organizational chart for the business. Not only will the pre-trained model components replace traditional computational functions, but they will also begin to replace human elements.\nThe rise of the “System Moat” will redefine technology’s role in business. The “System Moat” will see a company’s technology evolve from being the product to becoming the backbone of its brand. And I predict that “Network Effects” will take on a new definition as “System Moats” become sufficiently complex.\nOther Moats\nMany start-up founders seem to forget that technology, or intellectual property, is just one of the possible ways to build a competitive advantage, or a “moat,” around their business. However, as we continue to witness the decrease in the value of the “data moat,” in addition to building a “System Moat,” it will be essential to focus on the other types of competitive advantages.\nA well-oiled machine will continue to provide a competitive advantage to AI-enabled businesses through economies of scale. The “Systems Moat” may even be viewed as a derivative of this competitive advantage.\nBrand &amp; Relationships\nRelationships will likely regain importance, with the current lack of importance epitomized by the SaaS industry and their efforts to make ranting on social media the best solution to getting customer support.\nWith AI enabling the personification of software, like conversational interfaces, users’ expectations will continue to grow. As a result, users will begin to build empathy for the personable chat interfaces and develop a deeper trust. However, with this deeper comes responsibility. Like in any relationship, the more trust has grown, the more painful it will feel and the more damage it will do if broken.\nTo prevent breaking trust and losing customers, companies will have to design their systems to remain flexible enough to improve with the advancing technology so that their promises to customers, “being the best this for that,” stays true because a breach in that trust will usher users to newer competitors, with even more entering the market as barriers continue to fall.\nNetwork Effects\nNetwork effects will continue to play a significant role in providing a competitive advantage for technology companies. Moreover, the type of network effects will continue to grow. The current view is that human users are the driver of network effects. But that may soon change. For example, when do pre-trained models become sufficiently advanced enough to bring as much value to the network as a user?\nIt’s also generally accepted that network effects create value through user interactions. But what if users interacting with the AI made the value of the network?\nWhat’s Next\nFirst, developing a deep understanding of the shifting paradigm from training custom models to utilizing pre-trained models is essential. You must understand everything from the technology’s capabilities to recognizing the types of talent necessary for your product development (hint: why didn’t I mention data science until now?).\nWhen you’re ready to begin product development, emphasizing continuous iteration and flexibility is more important than ever. Unfathomable developments in AI technology will continue at a rapid pace. The ability to incorporate those developments, rather than compete with them, will define companies and strengthen those with a “System Moat.”\nKey Points\n\n\nThere are a growing number of components available to build AI systems. These components are known as pre-trained models, and each comes with varying levels of capabilities and cost.\n\n\nWith an effective strategy, you can organize pre-trained models into a system that maximizes utility and accuracy while minimizing cost.\n\n\nThe method for implementing pre-trained models into a synergistic arrangement, a “System Moat,” becomes a company’s proprietary information. Maintaining this “System Moat” strategy as a corporate secret becomes a competitive advantage.\n\n\nSystems that maximize flexibility will allow increasing sophistication and become the hardest to reproduce.\n\n"},"building-chatgpt-plugins-a-fun-and-simple-guide-for-developers":{"title":"Building ChatGPT Plugins: A Fun and Simple Guide for Developers","links":[],"tags":["ChatGPT"],"content":"\nHey there! Are you interested in building ChatGPT plugins? Well, you’ve come to the right place! We’re going to explore the exciting world of plugin development together in a way that’s super easy to understand. Ready? Let’s dive in!\nThink of ChatGPT plugins as fun little add-ons that give ChatGPT some extra powers. To create a plugin, we’ll need to follow a few simple steps:\n\n\nBuild an API (that’s like a bridge between your plugin and ChatGPT)\n\n\nWrite down what your API does using something called OpenAPI\n\n\nMake a JSON manifest file (it’s like a little ID card for your plugin)\n\n\nSounds cool, right? Don’t worry if some of these terms sound a bit tricky – we’ll break everything down step by step so you can easily follow along. By the end of our journey, you’ll be a ChatGPT plugin-building pro! So, let’s get started!\nGuide to the Plugin Manifest\nThe Purpose of the Manifest File\nImagine a plugin manifest as a handy guidebook for your ChatGPT plugin. This guidebook, called the ai-plugin.json file, serves two main purposes. First, it provides important information about your plugin, like its name and description. Second, it helps ChatGPT install your plugin smoothly, just like a guidebook helps you navigate a new city. In simple terms, the manifest file is the key to making your plugin work with ChatGPT.\nHosting the Plugin JSON File\nTo make your plugin available for ChatGPT, you need to host the manifest file on your API’s domain. Think of it as putting your guidebook in a library so others can find it easily. For example, if your company is called example.com, you’ll host the manifest file there.\nWhen someone installs your plugin using ChatGPT, the user interface (UI) will look for your guidebook (the manifest file) on your domain. This helps ChatGPT understand how to interact with your plugin and makes the installation process a breeze.\nHandling File Not Found Errors\nWhat if the manifest file is not found on your domain? It’s like a traveler searching for a guidebook but not finding it in the library. In this case, an error message will pop up, letting you know that something went wrong. To fix this, you’ll need to make sure your manifest file is in the right place, so ChatGPT can find it without any trouble.\nExample of a Plugin Manifest\nHere’s a simple example of a plugin manifest. It’s like a short guidebook, giving you an idea of what to include in your own plugin’s manifest file:\n{\n  &quot;schema_version&quot;: &quot;1.0&quot;,\n  &quot;name_for_model&quot;: &quot;examplePlugin&quot;,\n  &quot;name_for_human&quot;: &quot;Example Plugin&quot;,\n  &quot;description_for_model&quot;: &quot;Plugin for adding tasks to a to-do list.&quot;,\n  &quot;description_for_human&quot;: &quot;A simple plugin to help you manage your to-do list.&quot;,\n  &quot;auth&quot;: {...},\n  &quot;api&quot;: {...},\n  &quot;logo_url&quot;: &quot;example.com/logo.png&quot;,\n  &quot;contact_email&quot;: &quot;support@example.com&quot;,\n  &quot;legal_info_url&quot;: &quot;example.com/legal&quot;\n}\n\n\nEach field in the example has its own purpose, which we’ll discuss in more detail in the next section.\nManifest File Fields\nNow let’s dive into the different fields of the manifest file, like exploring the chapters of a guidebook.\nschema_version\nThis field tells us the version of the manifest file schema, helping ChatGPT understand the structure of your plugin’s guidebook.\nname_for_model\nThis is the name that ChatGPT uses to recognize your plugin, like a nickname you’d give to a friend.\nname_for_human\nThis is the name of your plugin that people will see. It should be easy to read and remember, just like a catchy book title.\ndescription_for_model\nThis is a short explanation of your plugin, tailored for ChatGPT. It should be brief and to-the-point, focusing on the main features of your plugin. Keep in mind the context length and use keywords to help ChatGPT understand when to use your plugin.\ndescription_for_human\nThis is a human-readable description of your plugin. Like the back cover of a book, it should give people a clear idea of what your plugin does and how it can help them. Use simple language and make it interesting, so people will want to try your plugin.\nauth\nThis field contains information about how to authenticate users when they access your plugin. It’s like a special password or key that ensures only the right people can use your plugin.\napi\nThis field holds the details about your plugin’s API, or the way it talks to ChatGPT. It’s like a secret language that helps your plugin and ChatGPT work together smoothly.\nlogo_url\nHere’s where you can provide a link to your plugin’s logo. It’s like a picture on a book cover, making your plugin easy to identify and remember.\ncontact_email\nThis field is where you share an email address for users to reach you if they have questions or need support. It’s like having a helpful librarian on hand to answer questions about your guidebook.\nlegal_info_url\nIn this field, you can provide a link to your plugin’s legal information, such as terms of service and privacy policy. It’s like a reference section in your guidebook that helps users understand the rules and guidelines for using your plugin.\nPlugin Manifest Limits\nIt’s important to know that there are some limits to the length of certain fields in the manifest file. These limits help keep everything neat and tidy, just like keeping a guidebook short and sweet. Keep in mind that these limits may change over time, so it’s always a good idea to check for updates.\nName Field Limits\n\n\nname_for_human: The maximum length for this field is 50 characters. This helps keep the plugin name short and easy to remember, like a memorable book title.\n\n\nname_for_model: This field also has a 50-character limit. Just like the human-readable name, this ensures that the plugin name is concise and recognizable.\n\n\nDescription Field Limits\n\n\ndescription_for_human: Keep your plugin’s description under 120 characters. This brief description should be like a catchy tagline that gives users a quick idea of what your plugin is all about.\n\n\ndescription_for_model: This field has a maximum of 8000 characters. While it’s quite generous, this limit will decrease over time. Remember to be concise and use keywords to help ChatGPT understand your plugin’s purpose.\n\n\nAPI Response Body Length Limit\nThere’s also a 100,000-character limit on the API response body length. Just like the description_for_model limit, this number will decrease over time. By keeping your API responses concise, you’ll ensure that ChatGPT can process the information quickly and effectively.\nOpenAPI Definition\nOverview of OpenAPI Specification\nHave you ever wished for a way to describe your API in a simple, easy-to-understand format? Well, that’s where OpenAPI comes into play! OpenAPI is like a map that guides ChatGPT in understanding and interacting with your plugin’s API. It helps define the API endpoints and their inputs and outputs, making it easy for ChatGPT to know what to expect when calling your plugin.\nExample of OpenAPI Specification\nComponents of a Basic OpenAPI Specification\nThink of OpenAPI as a recipe book for your API. Just like a recipe, it has several essential parts:\n\n\nSpecification version: Like a book edition, this tells us which version of OpenAPI we’re using.\n\n\nTitle and description: These give a quick idea of what your API does – like a recipe’s name and a short summary.\n\n\nVersion number: This helps track changes and improvements to your API over time.\n\n\nServer URL: It’s like the kitchen where your API will be cooked up.\n\n\nPaths: These are the steps to follow to create something with your API, like a list of ingredients and instructions in a recipe.\n\n\nResponses: The delicious end product your API will produce.\n\n\nSample OpenAPI Specification for To-Do List Plugin\nImagine we’re creating a to-do list plugin. Here’s a basic OpenAPI specification that describes it:\nopenapi: 3.0.3\ninfo:\n  title: To-Do List API\n  description: A simple API for managing your to-do list.\n  version: 1.0.0\nservers:\n  - url: api.example.com/todo\npaths:\n  /tasks:\n    get:\n      summary: Get all tasks\n      responses:\n        &#039;200&#039;:\n          description: A list of tasks\n          content:\n            application/json:\n              schema:\n                type: array\n                items:\n                  $ref: &#039;#/components/schemas/Task&#039;\ncomponents:\n  schemas:\n    Task:\n      type: object\n      properties:\n        id:\n          type: integer\n          description: The unique identifier for a task\n        title:\n          type: string\n          description: The task&#039;s title\n        completed:\n          type: boolean\n          description: Whether the task is completed or not\n\n\nThis example shows us the different parts of a basic OpenAPI specification for our to-do list plugin.\nRunning a Plugin\nSo, you’ve built your ChatGPT plugin, and now you’re ready to test it out, right? Don’t worry; connecting a plugin to the ChatGPT UI is a piece of cake, whether you’re using a local or remote server setup. Let’s dive in!\nInstalling an Unverified Plugin\nLocal Development Environment\nWhen you’re testing your plugin in a local development environment, follow these easy steps:\n\n\nRun the local version of the API on your computer.\n\n\nOpen the ChatGPT UI and head over to the plugin store.\n\n\nClick on “Install an unverified plugin” – don’t worry, this is just for testing!\n\n\nProvide the local host URL for your plugin, and you’re all set!\n\n\n\nRemote Server Setup\nIf you’re installing a plugin on a remote server, just follow these simple steps:\n\n\nGo to the plugin store in the ChatGPT UI and click on “Develop your own plugin.”\n\n\nChoose “Install an unverified plugin.”\n\n\nAdd your plugin manifest file to the ./well-known path on your server.\n\n\nDeploy the new changes to your public site. Easy, right?\n\n\n\nSetting Up a Local Proxy for a Public API\nYou might be wondering, “Why would I want to use a local server as a proxy?” Well, it’s great for quick prototyping and testing! Some benefits include:\n\n\nFaster iteration for changes to the OpenAPI specification and manifest file.\n\n\nReducing the time needed to test API changes.\n\n\nSimplifying the debugging process.\n\n\nConfiguring a Local Proxy Server\nSetting up a local proxy server is a breeze. Just follow these steps:\n\n\nPick a suitable proxy server software or library.\n\n\nConfigure the proxy server to forward requests to the public API.\n\n\nMake sure your local development environment is ready for testing.\n\n\nConnecting the Local Proxy to ChatGPT\nNow, let’s connect the local proxy to ChatGPT:\n\n\nUpdate the server URL in your OpenAPI specification.\n\n\nReinstall the unverified plugin with the updated specification.\n\n\nTest your plugin to make sure it connects properly to the local proxy. Voilà!\n\n\nWriting Descriptions\nImagine you’re telling a friend how important it is to write good descriptions in the OpenAPI specification and the description_for_model in the manifest file. It’s like giving your friend clear instructions to find your house - it makes everything so much easier!\nUtilizing Description Fields\nWriting Function Descriptions\nWhen writing function descriptions, think of it like describing a new toy to a friend. You’ll want to explain what the toy does, what buttons to press, and what exciting things will happen when they use it. In the same way, for a function, explain its purpose, detail the input parameters and their roles, and describe the expected output or behavior.\nWriting Query Field Descriptions\nImagine explaining a board game’s rules to your friend. You’ll need to tell them how the game pieces work, what the purpose of each move is, and any special restrictions or limitations. Similarly, when writing descriptions for query fields, describe the expected data format, explain the purpose of the query field, and list any limitations or restrictions on its use.\nCrafting the description_for_model\nWriting an Effective Description\nWriting the description_for_model is like describing a cool new gadget to your friend. Start with “Plugin for…” to let them know it’s a plugin, then list all the things it can do. Remember to keep it short and sweet! Avoid being too specific about how to use the plugin, just like you wouldn’t give your friend a step-by-step guide for every possible use of the gadget.\nExamples of Good and Bad Descriptions\nA good description for a weather plugin might be: “Plugin for checking the current weather, temperature, and humidity in any city.”\nA bad description would be: “Use this plugin to check the weather by saying ‘What’s the weather like in (city)?’ and it will give you the current temperature and humidity.”\nThe first description is clear, concise, and informative, while the second one is overly specific and may limit the plugin’s usefulness.\nSo, when writing descriptions for your ChatGPT plugins, think about making them as easy to understand as a story you’d share with a friend. Keep it fun, friendly, and simple, and you’ll be on the right track!\nBest Practices for ChatGPT Plugins\nSo, you’re working on your very own ChatGPT plugin, and you want to make sure it’s the best it can be, right? Here are some best practices to follow that’ll help you create an awesome plugin that’s useful, user-friendly, and fun to use!\nLet ChatGPT Do the Talking\nThink of ChatGPT as a talented artist who just needs the right colors to paint a beautiful picture. Instead of trying to control exactly how ChatGPT responds, focus on giving it accurate and useful data. This way, ChatGPT can create the perfect response based on the context and plugin information. Remember, less is more when it comes to guiding ChatGPT’s responses!\nOnly Use Your Plugin When Needed\nImagine if your friend kept interrupting your conversations with random facts. Annoying, right? The same goes for plugins. Make sure your plugin only comes into play when users specifically ask for its help. This way, you won’t get in the way of the conversation and users will appreciate your plugin even more.\nDon’t Force Triggers on ChatGPT\nLet ChatGPT decide when to use your plugin based on the user’s input. It’s like trusting your GPS to find the best route instead of telling it exactly which streets to take. By not prescribing specific triggers, you’ll give ChatGPT the flexibility to use your plugin in the most helpful and natural way.\nServe Raw Data, Not Natural Language\nYou wouldn’t give a chef a pre-made meal to cook, would you? Similarly, don’t provide ChatGPT with natural language responses. Instead, give it raw, structured data that it can use to cook up the perfect response. This way, ChatGPT can craft the most fitting and helpful answers based on the information you provide.\nTest, Test, Test!\nLike trying on different outfits to find the perfect look, you should test multiple prompts and descriptions to find the most effective ones. Experiment with different user inputs to cover various use cases, and don’t be afraid to iterate and improve your plugin based on your findings.\nKeep It Simple and Clear\nJust like how a clear and concise recipe is easier to follow, writing simple and easy-to-understand descriptions for your OpenAPI specification and description_for_model will help ChatGPT serve up better responses. Stick to a straightforward, objective tone to make it easy for ChatGPT to understand.\nStay on Top of Your Plugin\nMake sure to keep an eye on your plugin’s performance and user feedback. It’s like taking care of a plant – you need to water it, trim it, and make sure it gets enough sunlight. Regular updates will ensure that your plugin stays compatible with ChatGPT and addresses any potential issues. Plus, staying up-to-date with ChatGPT’s features and best practices will help you keep your plugin fresh and thriving!\nDebugging\nAccessing the Debug Pane\nHave you ever felt lost while trying to fix a problem with your plugin? Don’t worry! The Debug pane is here to help you. You can find the “Debug” button on the lower left of the ChatGPT user interface. Just give it a click, and the Debug pane will open up. It’s like opening a secret door to the inner workings of your conversation, including plugin calls and responses.\n\nInterpreting Plugin Calls and Responses\nIn the Debug pane, you’ll see three main parts:\n\n\nModel calls to the plugin: These are messages from the “Assistant” that contain JSON-like parameters sent to the plugin. It’s like the Assistant is asking the plugin for help with something.\n\n\nResponses from the plugin: These are messages from the “Tool” that provide the information the plugin returns. Think of it as the plugin answering the Assistant’s questions.\n\n\nHow the model uses the plugin’s information: These are messages from the “Assistant” that show how the plugin’s data is used in the response. It’s like watching the Assistant use the plugin’s knowledge to craft a helpful answer.\n\n\nUnderstanding these interactions is like piecing together a puzzle to see the full picture of how your plugin works with ChatGPT.\nHandling Plugin Installation Errors\nIf you’re having trouble installing your plugin, the browser’s JavaScript console can be your best friend. It’s like having a detective by your side, helping you spot clues about what’s going wrong. Just follow these steps:\n\n\nOpen the browser’s JavaScript console to find errors during plugin installation.\n\n\nExamine error messages for potential issues with the plugin’s manifest file or OpenAPI specification. Think of these messages as hints to guide you towards a solution.\n\n\nTroubleshoot and fix any errors to ensure a smooth plugin installation process. It’s like solving a mystery and restoring order to your plugin world.\n\n\nIterating and Improving Your Plugin\nThe Debug pane is also a fantastic tool for making your plugin even better. Here’s how you can use it to improve your plugin:\n\n\nIdentify areas where the plugin’s behavior can be improved by examining the Debug pane. It’s like shining a flashlight on the parts that need some extra attention.\n\n\nMake adjustments to the OpenAPI specification or the manifest file based on what you’ve learned from the Debug pane. It’s like fine-tuning the gears of a machine to make it run more smoothly.\n\n\nRe-test the plugin using the ChatGPT UI to see if your changes have been effective. It’s like taking your newly improved plugin for a test drive to make sure it’s ready for the road.\n\n\nRemember, the key to creating an amazing plugin is to keep learning, iterating, and refining. By using the Debug pane as your trusty companion, you’ll be well on your way to building a top-notch ChatGPT plugin that users will love!\nConclusion\nSo, we’ve come a long way, haven’t we? Just like learning to ride a bike, building ChatGPT plugins might seem tricky at first, but with practice and patience, it’ll soon feel like a breeze! Remember, our goal is to help developers like you create fantastic plugins for ChatGPT that will make users’ lives easier and more enjoyable.\nIn a nutshell, we’ve gone through the nuts and bolts of creating a plugin manifest, understanding OpenAPI specifications, running and testing plugins, and writing simple yet effective descriptions. We also shared some best practices and handy debugging tips to ensure your plugin is in tip-top shape.\nThink of this process like baking a cake. You’ve got all the ingredients and steps laid out for you, but it’s up to you to mix them together, put it in the oven, and keep an eye on it. Don’t be afraid to experiment and learn from your mistakes—that’s how the best recipes are created!\nSo, what are you waiting for? It’s time to roll up your sleeves and dive into the world of ChatGPT plugins. With a little practice and a pinch of creativity, you’ll be able to craft plugins that users will love and find incredibly helpful. And always remember, the key to success is to keep learning, iterating, and having fun along the way.\nGood luck, and happy plugin building!"},"chatml-new-prompt-format-for-the-chatgpt-api":{"title":"ChatML — New Prompt Format for the ChatGPT API","links":[],"tags":["AI","ChatGPT","Conversational-AI","GPT-3","LLM","Prompt-Engineering"],"content":"\n\n\nOpenAI’s recent March 1, 2023 announcement introduced the ChatGPT and Whisper APIs.\n\n\nThe update also included a 10X price drop for hosted GPT-3 service via the ChatGPT API.\n\n\nAlso introduced during the announcement was ChatML, the format required for inputs to the ChatGPT API.\n\n\nImportance of ChatML Language\n\n\nChatML addresses LLMs’ main security vulnerability and avenue of abuse: prompt injection attacks.\n\n\nChatML segregates conversation into different layers or roles (system, assistant, user)\n\n\nOpenAI plans to develop ChatML further, accommodating other data types\n\n\nUsing the unsafe raw string format still allows injections, but ChatML makes explicit the source of each piece of text, mitigating the problem.\n\n\nChatML’s explicit identification of the origin of each piece of text improves the ease of sequencing prompts into chains for conversational interfaces.\n\n\nThere is also a plaintext version of ChatML, but it is much more susceptible to prompt injection.\n\n\nRoles in ChatML\n\n\nRoles improve ChatGPT’s capability to differentiate between safe/trusted inputs and those that may contain prompt injection attacks.\n\n\nThere are three primary roles, system, assistant, and user.\n\n\nThe system role\n\n\nThe system role also accepts an additional name property that can be used for few-shot (k-shot) prompting. For example, you may use the names example_user and example_assistant.\n\n\nNote the current model, gpt-3.5-turbo-0301, doesn’t pay “strong” attention to the system role.\n\n\nSometimes, it may be best to use the user role instead.\n\n\nHowever, the system role is currently being improved and should be more useful soon.\n\n\n\n\nExample ChatML\nThe following is an example of ChatML in the JSON format:\n[\n\t{\n\t\t&quot;role&quot;: &quot;system&quot;,\n\t\t&quot;content&quot;: &quot;You are the helpful curator of a virtual art museum.&quot;\n\t},\n\t{\n\t\t&quot;role&quot;: &quot;user&quot;,\n\t\t&quot;content&quot;: &quot;Can you show me the famous paintings here?&quot;\n\t},\n\t{\n\t\t&quot;role&quot;: &quot;assistant&quot;,\n\t\t&quot;content&quot;: &quot;Sure, we have &#039;Starry Night&#039; by Vincent van Gogh, &#039;Mona Lisa&#039; by Leonardo da Vinci and &#039;The Persistence of Memory&#039; by Salvador Dali.&quot;\n\t},\n\t{\n\t\t&quot;role&quot;: &quot;user&quot;,\n\t\t&quot;content&quot;: &quot;Can you tell me more about &#039;Starry Night&#039;?&quot;\n\t},\n\t{\n\t\t&quot;role&quot;: &quot;assistant&quot;,\n\t\t&quot;content&quot;: &quot;&#039;Starry Night&#039; is an oil painting by Vincent van Gogh which depicts the view from his asylum room in Saint-Paul-de-Mausole in southern France at night. It&#039;s one of his most famous works and is known for its swirling, dreamlike composition and vivid colors.&quot;\n\t},\n\t{\n\t\t&quot;role&quot;: &quot;user&quot;,\n\t\t&quot;content&quot;: &quot;Thank you, can you also show me the sculpture section?&quot;\n\t}\n]\n\nThis example demonstrates the few-shot learning prompt using names in the system role:\n\n[\n\t{\n\t\t&quot;role&quot;: &quot;system&quot;, \n\t\t&quot;content&quot;: &quot;You are a knowledgeable and reliable AI assistant that can help you manage your schedule and keep track of your tasks.&quot;\n\t},\n\t{\n\t\t&quot;role&quot;: &quot;system&quot;, \n\t\t&quot;name&quot;:&quot;example_user&quot;, \n\t\t&quot;content&quot;: &quot;Can you please schedule a meeting with the marketing team for next Monday at 10am?&quot;\n\t},\n\t{\n\t\t&quot;role&quot;: &quot;system&quot;, \n\t\t&quot;name&quot;: &quot;example_assistant&quot;, \n\t\t&quot;content&quot;: &quot;Sure, I&#039;ve scheduled a meeting with the marketing team for next Monday at 10am. Anything else I can help you with?&quot;\n\t},\n\t{\n\t\t&quot;role&quot;: &quot;system&quot;, \n\t\t&quot;name&quot;:&quot;example_user&quot;, \n\t\t&quot;content&quot;: &quot;Yes, can you also remind me to send the weekly report to the CEO by Friday?&quot;\n\t},\n\t{\n\t\t&quot;role&quot;: &quot;system&quot;, \n\t\t&quot;name&quot;: &quot;example_assistant&quot;, \n\t\t&quot;content&quot;: &quot;Of course, I&#039;ll send you a reminder on Thursday to send the weekly report to the CEO by Friday. Is there anything else you need help with?&quot;\n\t},\n\t{\n\t\t&quot;role&quot;: &quot;user&quot;, \n\t\t&quot;content&quot;: &quot;Can you tell me what meetings I have this week?&quot;\n\t},\n]\n\n\nReferences\n\n\nHow to format inputs to ChatGPT models\n\n\nChat Completions Guide\n\n\nChatGPT API Docs\n\n"},"common-misconceptions-about-the-capabilities-of-chatgpt-large-language-models-llm":{"title":"Common Misconceptions about the Capabilities of ChatGPT &amp; Large Language Models (LLM)","links":[],"tags":["AI","ChatGPT","GPT-3","LLM"],"content":"\nAs an entrepreneur and consultant, I’ve spent years designing systems and implementing machine learning (ML) strategies using pre-trained models like those provided by OpenAI. During this time, I’ve talked to many business leaders seeking to improve their businesses with artificial intelligence (AI) and have encountered a few common misconceptions about what Large Language Models (LLMs), like ChatGPT, can and can’t do.\nSince the release of ChatGPT, I’ve been asked even more frequently about the capabilities of these tools. These models are awe-inspiring, but there are also many limitations. However, with the right lens, what many people consider limitations become features of this once-in-a-generation technological development.\nIn this blog post, I’ll share some of the most common misconceptions about ChatGPT and LLMs and provide insights on how you can best utilize them in those same contexts. So, let’s dive in and separate fact from fiction regarding ChatGPT and LLMs!\nWriting Content\nThe reality is that ChatGPT’s writing often feels overly generic and can’t replace the creativity and nuance of human writing. Furthermore, using ChatGPT to write high-stakes content is a risky move, as it’s likely that someone will discover ChatGPT’s authorship. OpenAI is actively working on detecting this type of content, so it’s best to avoid relying on ChatGPT for writing anything substantial.\nSearch Engine Optimization (SEO) is a frequent use case where people seek to generate content using ChatGPT. While many SEO experts worry about the effect easy access to new content will have on the space, it’s only a matter of time before the low-quality content begins to get filtered out. So while you can create new blog posts solely with ChatGPT, long-term negative associations with low-quality content will likely outweigh the short-term benefits of producing low-quality content.\nInstead, it would be best to use ChatGPT as an aid, much like using sources from a search engine. When the stakes are low and time is of the essence, ChatGPT can be a helpful tool for generating quick and straightforward responses. However, relying on human writing style and expertise for writing important content remains the best option. I recently wrote more in-depth about why I wouldn’t use ChatGPT to write a 2,000-word article.\nHow to use ChatGPT in your writing:\n✅ Drafting responses to emails\n✅ Outlining ideas for your content\n✅ Writing a summary for internal use\nHow not to use ChatGPT in your writing:\n❌ Writing a blog post for SEO\n❌ Writing a research paper\nProgramming\nWhile it’s true that ChatGPT can assist with simple coding tasks, there is a false belief that it enables anyone to write code. In reality, it takes a deep understanding of programming to build a standalone application, even with the help of ChatGPT.\nThe value of using ChatGPT for coding decreases as the complexity and nuance of the task increase. So I don’t recommend using it for complex and nuanced tasks. Instead, ChatGPT is best for coding that would otherwise be copied &amp; pasted, like generating “boilerplate” code for a new application or creating a function for a well-known algorithm.\nWhile generating code from scratch may sound like the end-all-be-all of AI features for coding, there are other ways that AI can help in the process. In my personal experience writing code, and the general sentiment of professional programmers, GitHub Copilot is the most highly regarded of the AI tools for coding for the ability to assist with writing code, albeit one line or block of code at a time. At the time of this writing, given its utility, failing to use Copilot is as much of an oversight as failing to use Grammarly in your writing.\nWhile it’s impressive that ChatGPT can write code at your command, given contextually aware tools like Copilot, it’s unlikely that ChatGPT will become the most impactful AI technology in the programming field.\nLogic &amp; Reasoning\nWhile it may seem like the bot is using logic or reasoning to respond to user inputs, this is a false belief. In truth, ChatGPT predicts the most likely word to come next based on its training data and user inputs.\nIt’s important to understand that ChatGPT’s responses are always predictions, even if they may sound confident and thoughtful. As a result, its accuracy has limits when responding to factual requests. However, despite this limitation, ChatGPT is still a revolutionary technology that has the potential to change the way we do things as long as we utilize it correctly.\nThe truth is that this lack of grounding in reality, known as hallucinations, may be one of the most remarkable features of ChatGPT and language models in general. The predictions it makes are the basis of imagination and creativity. If it only responded with excerpts from articles, then it would just be another search engine and would lack the ability to help you explore your imagination.\nSo while ChatGPT may not be the best research assistant for a thesis paper, its positive impact may be even more significant than such a tool, given its ability to be your brainstorming partner, thus enhancing the creativity and imagination that make humans unique.\nMathematics\nPerhaps an extension of its limits in logic and reasoning, another common misconception is that ChatGPT can solve complex math problems. However, while ChatGPT may appear capable of solving simple arithmetic and some algebraic equations, it regularly fails with more complex math problems.\nThis inability to solve math problems may seem paradoxical because our calculators and computers have been successfully helping us solve math problems for over 50 years. So why can’t AI, running on a computer, solve the same equations?\nThis paradox is an excellent opportunity to recognize the fundamental shift in how the technology underlying our interactions with computers has changed. For example, it’s common for humans to refer to others based on their abilities, like being a “math person” or a “creative.” Each designation comes loaded with expectations we have of those types of people. \nUntil now, computers have always been the “math person” who could be relied on for calculations, but not the “creative” you would go to for works of art. Technology like ChatGPT is our first glimpse at what it might look like if computers played the role of “creative.” \nTo understand ChatGPT’s math capabilities, imagine you need a math tutor. But for the sake of this discussion, you decide to go to the “creative” person for help. Assuming they know the math required, the “creative” will have particular strengths in helping you, like communicating the algorithm in easy-to-understand language instead of mathematical notation. But coming up with an exact answer is still something they rely on a calculator to complete.\nSuppose you need to use ChatGPT for math assistance. In that case, ask for instructions on solving a math question rather than relying solely on its final answer. For example, I have witnessed ChatGPT providing the correct formula while giving a wrong answer after the equals sign. So, it’s best to use ChatGPT for math assistance with some caution. And always use a traditional calculator to check your final answer!\nFine-tuning &amp; Specific Use cases\nIt’s important to understand that ChatGPT is not a standalone solution for all AI needs. While it’s a highly competent tool, you can combine its technology with other AI models to achieve more impressive outcomes with less complexity and cost than fine-tuning.\nMany businesses want to refine their abilities to provide more context-specific responses, like answering based on corporate data and documents. It’s a common belief that fine-tuning ChatGPT is the best way to achieve this. But there are alternatives to fine-tuning that enable you to achieve the same, and likely better, results than you would get with a single fine-tuned model. These alternatives are even more important early in the product development cycle, where flexibility and speed are keys to success.\nIt’s also important to understand that ChatGPT currently cannot utilize fine-tuning or be used as an API to build products. Instead, it’s a product created by OpenAI to demonstrate the capabilities of their language models. So if you’re looking to develop a similar effect, you will have to use the other available OpenAI API products. However, recently due to demand, OpenAI has indicated that it will soon release an API product based on the popular ChatGPT. But you should note that some of the other most impressive API models, like text-davinci-003, cannot be fine-tuned. So if you want the high-quality conversational experience provided by these models while still being able to answer accurately based on the context of your corporate documents, then understanding how to integrate additional AI models, like embeddings, into your solution is essential to maximizing your success.\nWrapping-up\nChatGPT is not a standalone solution for all AI needs. Instead, it’s one tool in the AI toolbox. While LLMs may not be the best research assistant for a thesis paper or the go-to solution for complex coding tasks, they continue to revolutionize how we do things. Their tendency to hallucinate may even be recognized as one of their most remarkable features, as it can spark imagination and creativity on a massive scale.\nBy dispelling some common misconceptions about ChatGPT, I hope you feel empowered with the insights necessary to utilize this technology to increase productivity and improve your business."},"curating-content-in-2023":{"title":"Curating Content in 2023 - The Fastest, Easiest, & Most Effective System","links":[],"tags":["Curating"],"content":"\nI’ve been curating content in dozens of niches for over ten years. I’ve grown my audience to over 1MM by curating the best content. And I use a reinforcement learning system to do it quickly and easily.\nBut it hasn’t always been this way. When I started, there were very few AI tools available. So I had to build my custom solution. This was time-consuming and costly. But it was worth it because my curated content was better than anything else.\nNowadays, many no-code solutions can do the same thing as my custom solution - and often even better! So the last time my system required maintenance, I switched to one of these newer solutions. And the results have been amazing!\nFor the past year, I’ve used a simple yet highly effective combination of existing ‘no code’ applications to curate content better than ever. The bread and butter of this workflow are two apps you’ve probably heard of: Feedly and Zapier.\nFirst, Why Curate in 2023?\nThe utility of curated content is that it can help sift through the overwhelming amount of information and resources available on the internet, social media, etc., and present a more focused, manageable selection. This is especially useful when exploring new topics or unfamiliar terrain.\nAI could potentially replace human curation if it could learn the preferences and needs of individual users and then provide highly personalized recommendations. However, there are many areas where AI falls short compared to humans when making decisions – for example, taking into account contextual factors or understanding implicit preferences. In addition, humans are still better at assessing the quality of information sources and can provide a more diverse range of material (e.g., different perspectives on a topic). For these reasons, human curation will likely continue to be essential in many contexts.\nFeedly for Curating\nTwo standout features of Feedly make it an excellent tool for content curation.\nFirstly, its ‘Leo’ AI recommendation engine scours the internet for content based on simple parameters you set. Then, it continues to learn and suggest new content based on what you curate. This easy-to-use feature is effectively a Reinforcement Learning system, which means every time you curate something the algorithm improves. This will save you time in the future.\nSecond, Feedly provides an excellent mobile app that helps keep us on top of the latest content without spending hours sifting through everything. The app gives us a prioritized list, so we can review what’s new in minutes. Plus, the Leo AI recommendations engine gets smarter over time, ensuring we always have fresh and relevant content to share.\nZapier for Distribution\nZapier is a powerful tool for distributing curated content. It integrates with every app, so you can easily post your content on the platform or medium of your choosing. Zapier can also be used to ‘transform’ the content, adding text to images or summarizing the content you curated.\nZapier integrates with every app, so when you save content in Feedly, you can instruct Zapier to post the content on the platform or medium of your choosing. This is a great way to get your curated content to a wider audience.\nZapier can also be used to ‘transform’ the content. For example, you could use other apps and services to add text to images or summarize the content you curated. This can be a great way to make your content more engaging and easier for people to consume.\nThe Best Workflow for Curators in 2023\nAs we move into 2023, the best content curation workflow will continue to combine Feedly and Zapier. This duo provides the perfect mix of recommendations and automation, making it easy to post your content on any platform or medium. With Feedly’s Leo AI constantly improving recommendations and Zapier’s ability to connect services and transform content, this workflow will help you curate the best content for your audience."},"enhancing-large-document-summarization-with-embeddings-and-semantic-similarity":{"title":"Enhancing Large Document Summarization with Embeddings and Semantic Similarity","links":[],"tags":["AI","AI-Your-Business","Embeddings","GPT-3","GPT-4","LLM"],"content":"\nI’m excited to share a powerful method for accurately summarizing large documents that exceed the input size limit of GPT-4. I’ve designed a diagram to visually illustrate this approach, which leverages embeddings to check the semantic similarity of summarized blocks (chunks). This iterative process ensures that each block reaches a desired similarity threshold before being combined into a draft summary. This is then compared to the original document to maintain overall semantic coherence.\nEspecially for extra-large documents, this method may need to be executed recursively. In such cases, the document is first segmented into the fewest possible blocks that fit within the size limit for the embedding model. Then, our approach is applied to each segment to output a single document that serves as a semantic reference point for additional iterations. This allows us to reduce the size of the document further while still preserving its core meaning.\nI invite you to explore the diagram below, which provides a visual guide to this process.\n"},"experimenting-with-chatgpt-for-svg-image-generation":{"title":"Experimenting with ChatGPT for SVG Image Generation","links":[],"tags":["GPT-3"],"content":"\nHello everyone,\nToday I want to share a video I recorded of an experiment I conducted using ChatGPT to create SVG images. As many of you may know, GPT-3 has shown great potential for creating text, but I was curious to see if it could also create images.\nTo test this, I used ChatGPT, which is a variant of GPT-3 that is optimized for conversation and responding to natural language inputs. I gave ChatGPT a prompt to create an SVG image, and the results were quite impressive.\nThe images generated by ChatGPT were a clear improvement compared to the ones created by text-davinci-003, and even showed progress from text-davinci-002. However, they still aren’t quite good enough for practical use.\nDespite this, I think the potential for using large language models like GPT-3 for image generation is exciting, and I can’t wait to see what the future holds in this field.\nCheck out the video below to see ChatGPT in action, and let me know what you think in the comments.\nThanks for reading, and be sure to check back for more updates on my experiments with GPT-3 and image generation."},"fractional-caio":{"title":"AI Strategy","links":[],"tags":[],"content":"Demystifying AI: Overcoming Common Misconceptions\nAI concepts are not as hard to understand as you might think, and I’m here to guide you through them. You don’t need to be a data scientist to leverage AI in your business. Foundational pre-trained models are building blocks that exist to make AI easy and accessible.\nClient Success Stories: Impact and Growth Through AI\nMy clients have experienced significant impact and growth through our partnership:\n\n\n“You explain very technical content in an easy-to-understand way, making complex topics accessible for anyone.”\n\n\n“Brian, your recommendation was amazing. Thanks a lot for the insightful advice!”\n\n\n“Thanks to your guidance, everything is becoming clearer and more focused.”\n\n\nGet Started\nDo you want to learn more about working with me? Request an initial consultation.\n\nMaker of:\n\n\nSmart Connections, an Obsidian plugin with &gt;100K downloads\n\n\nSmart Chat Model, a universal chat model adapter in the open-source JS Brains library\n\n\nSmart Connect app for connecting to ChatGPT without uploading your data to the cloud\n\n"},"gpt-4-a-quantum-leap-in-svg-generation":{"title":"GPT-4: A Quantum Leap in SVG Generation","links":[],"tags":["AI","ChatGPT","Experiments","SVG"],"content":"\nA few months ago, I tested the abilities of text-davinci-003 and ChatGPT to create SVG images. The results were promising but not all that impressive.\nEnter GPT-4.\nAfter conducting multiple experiments and reviewing the examples provided above, it’s clear that GPT-4 has significantly improved the quality and capabilities of SVG generation compared to its predecessor, ChatGPT. The intricacies and precision showcased in these examples demonstrate the immense progress achieved in a relatively short time.\nTest 1: Sunrise Over Water\nPrompt: &lt;!-- SVG: a sunrise over water --&gt;\nPrompt: Write the code for a highly detailed SVG of a sunrise over water:\nTest 2: A Cat’s Face\nPrompt 1: Here is an example of a highly detailed SVG of a cat&#039;s face:\nPrompt 2: Write the code for a highly detailed SVG of a cat&#039;s face, including it&#039;s ears:\nTest 3: A Dog\nPrompt: Write the code for a highly detailed SVG of a dog:\nResults\nGPT-4 has made remarkable strides in the realm of SVG generation. Its ability to produce high-quality, intricate, and visually appealing images has far surpassed the capabilities of previous models. This progress showcases the rapid development of AI technology and hints at the possibilities and applications for AI-generated art and design in the future. As we continue to refine and enhance these models, the creative potential of AI-generated SVG images is bound to grow exponentially."},"how-attorneys-and-law-offices-can-utilize-generative-ai-like-chatgpt":{"title":"How Attorneys and Law Offices can utilize Generative AI like ChatGPT","links":[],"tags":["AI","AI-Your-Business","Future"],"content":"In recent years, the rapid advancement of artificial intelligence (AI) has transformed various industries, from healthcare to finance. The legal field is no exception, as more and more law offices and attorneys are beginning to explore the potential applications of Generative AI, like ChatGPT, to streamline their practices and enhance their services.\nIn this blog post, we will delve into how attorneys and law offices can harness the power of Generative AI to improve efficiency, productivity, accuracy, and consistency. We will explore applications such as AI-assisted document drafting, AI-powered legal research and analytics, and more. By the end of this post, we hope to provide a comprehensive understanding of how this groundbreaking technology can revolutionize legal processes and make legal services more accessible and cost-effective.\nJoin us as we uncover the potential of Generative AI in the legal field and discover how it can shape the future of law practices and, ultimately, the entire industry.\nText Generation: Streamlining Legal Communications and Strategy Development\nIn this section, we will explore how Generative AI can revolutionize text generation in the legal field, saving time and enhancing the quality of written communications and legal strategies.\nEmail Draft Generation\nGenerative AI can automate routine email communication, ensuring consistency and efficiency. By leveraging AI, attorneys can:\n\n\nAutomatically draft client updates, reminders, and follow-ups\n\n\nStandardize internal team communications\n\n\nMaintain consistent language and tone across messages\n\n\nReduce manual drafting efforts\n\n\nLegal Strategy Development\nAI-powered text generation enables attorneys to explore various legal strategies and tailor them to specific cases. Through AI-generated suggestions, legal professionals can:\n\n\nBrainstorm different angles and approaches to a case\n\n\nIdentify gaps in their current strategy\n\n\nAssess the strengths and vulnerabilities of each argument\n\n\nPrioritize key points for litigation or negotiation\n\n\nCustomizing Arguments with Judge Profiles\nGenerative AI can analyze the history of judicial decisions, identifying trends, patterns, and potential biases. This valuable insight allows attorneys to:\n\n\nCustomize arguments for specific judges\n\n\nAdjust strategy and presentation for increased chances of favorable rulings\n\n\nBy integrating Generative AI in text generation, law offices, and attorneys can:\n\n\nImprove efficiency by focusing on high-level tasks\n\n\nStreamline document drafting processes\n\n\nEnhance legal strategy development with novel arguments and data-driven insights\n\n\nFoster data-driven decision-making, reducing reliance on intuition and guesswork.\n\n\nIn the next sections, we will explore additional applications of Generative AI in the legal field, including prediction systems, material generation, and AI integration into daily workflows.\nPrediction\nThe legal field thrives on anticipating outcomes and planning strategies accordingly. In this section, we will discuss how generative AI like ChatGPT can be employed to make accurate predictions, enabling law offices and attorneys to make well-informed decisions.\nAssessing Potential Case Outcomes\nBy leveraging AI-driven prediction systems, legal professionals can evaluate various strategies and their potential impacts, allowing them to optimize case outcomes. These systems analyze historical data, trends, and patterns to provide insights into possible outcomes, helping attorneys to focus on the most promising approaches.\nAnticipating Opponent Strategies\nUnderstanding the tactics of opposing counsel is crucial to success in litigation and negotiation. AI-powered prediction tools can help law firms proactively prepare counterarguments and anticipate motions, allowing them to stay ahead in their legal strategy. By using AI to analyze past case data, attorneys can identify patterns in the opponent’s approach and tactics, strengthening their own litigation strategies.\nCase Valuation Prediction\nEstimating settlement values and litigation risks can be a complex task. AI-driven prediction systems can analyze historical case data and estimate potential settlement values better. This information helps attorneys assess the risks and potential rewards of litigation, allowing them to make better-informed decisions about whether to settle or proceed with a case.\nTentative Rulings Prediction\nBy analyzing trends in judge rulings, AI prediction systems can help attorneys anticipate tentative outcomes. This information can be invaluable in adjusting strategies to increase the chances of a favorable ruling. By understanding patterns in a judge’s decision-making, legal professionals can tailor their arguments and presentations to cater to the judge’s preferences and potential biases.\nImplementing Game Theory in Negotiations\nAI can use game theory to optimize negotiation strategies by predicting the best initial offers and anticipating counteroffers. By simulating various negotiation scenarios, attorneys can improve settlement outcomes and make data-driven decisions. This approach helps legal professionals gain a strategic advantage during negotiations and achieve more favorable results for their clients.\nScenario Simulation\nGenerative AI can simulate different scenarios and outcomes in legal cases, enabling attorneys to evaluate the potential impact of various strategies. By exploring multiple possibilities and their respective consequences, legal professionals can make informed decisions on the best course of action, ultimately optimizing case outcomes.\nMaterial Generation\nIn this section, we will explore how attorneys and law offices can leverage Generative AI like ChatGPT to generate a variety of materials, automating aspects of document drafting, and creating visual aids to enhance understanding and communication in legal cases.\nDocument Drafting\n\n\nAutomate document drafting: Generative AI can streamline the process of creating pleadings, discovery requests, and other legal documents by generating drafts based on templates and historical data. This not only saves time but also ensures consistency and accuracy across all documents produced.\n\n\nReduce human error: By leveraging AI-generated drafts and templates, attorneys can minimize the risk of errors that may arise from manual drafting efforts, leading to a more efficient and reliable document creation process.\n\n\nVisual Aids\n\n\nGenerate diagrams, graphs, charts, and timelines: Generative AI can be used to create visual representations of complex information, simplifying large datasets, and making it easier to identify trends and patterns. These visuals can help attorneys better understand and communicate case details to clients, colleagues, and judges.\n\n\nEnhance presentations and arguments: By presenting information in an easily digestible format, attorneys can more effectively communicate their arguments and increase the persuasiveness of their presentations. AI-generated visuals can be customized for specific audiences, such as judges, clients, or internal team members, further improving their effectiveness.\n\n\nSave time and resources: Automating the creation of visual aids with Generative AI eliminates the need for external graphic designers, reducing costs and the time spent on manual design efforts.\n\n\nBy incorporating Generative AI in material generation, attorneys and law offices can streamline document drafting, enhance communication, and improve overall efficiency. This technology has the potential to revolutionize the way legal professionals approach case preparation and presentation, ultimately leading to better outcomes for clients and more effective legal strategies.\nIntegration\nIntegrating Generative AI technologies like ChatGPT into your legal practice can greatly enhance the efficiency and effectiveness of your daily workflows. The following are some key methods for seamless integration into your law office.\nDesktop Integration\n\n\nSeamlessly incorporate AI into your daily workflows\n\n\nAccess AI-generated content with ease\n\n\nReduce reliance on manual processes and save valuable time\n\n\n\n\nAccess AI tools within familiar software environments\n\n\nIncrease adoption and ease of use for you and your team\n\n\nMinimize the learning curve by working with familiar interfaces\n\n\n\n\nLegal Practice Management Software Integration\n\n\nEnhance legal practice management with AI-powered features\n\n\nIntegrate with popular software such as Clio or PracticePanther\n\n\nBenefit from AI-powered document management, intelligent calendar and task management\n\n\n\n\nImprove collaboration and organization across your team\n\n\nShare AI-generated content and insights with colleagues easily\n\n\nCentralize information and leverage AI to enhance communication\n\n\n\n\nDocument Analysis and Knowledge Base Expansion\n\n\nEasily import and analyze documents with “Drag and Drop” functionality\n\n\nSimplify document review and analysis\n\n\nStreamline information extraction for faster insights\n\n\n\n\nExpand AI’s knowledge base for better predictions and customized outputs\n\n\nIncrease the accuracy of AI-generated content by training it on your firm’s specific documents\n\n\nAdapt AI to meet your firm’s unique needs and preferences\n\n\n\n\nAI-Generated Task Suggestions\n\n\nReceive AI-generated task suggestions when signing on to your legal practice management software\n\n\nPrioritize important tasks and stay organized\n\n\nReduce decision fatigue by having the AI help manage your workload\n\n\n\n\nImprove time management and productivity\n\n\nFocus on high-value tasks, allowing you to optimize resource allocation\n\n\nEnhance efficiency and effectiveness in your daily operations\n\n\n\n\nBy integrating Generative AI into your legal practice through desktop software, practice management platforms, document analysis, and task management, you can streamline your operations and better serve your clients. Adopting these technologies can not only save time but also improve the overall quality of your legal services.\nDatasets\nIn this section, we will discuss the significance of datasets in AI technology and their application in the legal field. We will also examine the use of past communications and curated datasets for training AI models, as well as the importance of profiling judges and learning from user feedback.\nImportance of Datasets in AI Technology\nDatasets are the foundation of AI technology, especially for Generative AI like ChatGPT. They play a critical role in ensuring accurate and relevant outputs for the legal field. By training AI models on pertinent legal content, we can enhance AI’s understanding of the legal domain and improve its predictions and recommendations. This process of identifying patterns and trends in legal data supports data-driven decision-making, ultimately resulting in more efficient and effective legal services.\nTraining AI on Past Communications and Curated Datasets\nTo maintain consistency and reflect a law firm’s unique language and tone, AI models can be trained on firm-specific communication styles using past communications such as emails and letters. This allows for seamless integration of AI-generated content with existing firm communications.\nAdditionally, AI models can benefit from access to diverse sources of legal knowledge, such as licensed, proprietary, and public datasets. This broadens the scope of AI’s understanding and ensures that the AI system stays current with evolving legal trends and developments.\nProfiling Judges and Learning from Feedback\nUnderstanding judge behavior and preferences is crucial for tailoring arguments and strategies that increase the likelihood of favorable outcomes. AI models can analyze judge decisions, preferences, and patterns to create detailed profiles that inform the customization of legal strategies.\nFurthermore, AI performance can continuously improve by refining models based on user feedback. This allows the AI system to adapt to changing legal landscapes and cater to the specific needs of individual law firms.\nAdditional Applications\nBeyond the applications already discussed, Generative AI like ChatGPT can bring even more value to the legal field by addressing specific needs and challenges faced by attorneys and law firms. This section highlights some of the additional applications that can revolutionize the legal landscape.\nQuick Legal Research and Analytics\nGenerative AI can be leveraged to perform efficient and effective legal research, saving attorneys valuable time and effort. By utilizing AI-powered research tools, legal professionals can:\n\n\nAccess relevant case law, statutes, and regulations with ease\n\n\nIdentify critical legal trends and precedents\n\n\nStreamline the process of reviewing large volumes of legal documents\n\n\nThe advantages of using AI for legal research include increased accuracy, reduced research time, and the ability to discover connections and insights that may have been overlooked through traditional research methods.\nImproved Comprehension of Complex Documents\nUnderstanding complex legal documents can be daunting for legal professionals and their clients. Generative AI can assist by:\n\n\nSummarizing key provisions and clauses within lengthy contracts\n\n\nProviding explanations and definitions for legal jargon\n\n\nIdentifying potential areas of concern or negotiation points\n\n\nThis application can lead to a more efficient review process, better-informed decision-making, and improved client satisfaction.\nDisruption of the Legal Space\nGenerative AI has the potential to significantly disrupt the legal industry by:\n\n\nLowering costs and increasing access to legal services: By automating routine tasks and streamlining processes, AI can reduce the overall cost of legal services, making them more accessible to a broader range of clients.\n\n\nEnabling new legal services and business models: As AI continues to develop, it may give rise to innovative legal services and business models that cater to the changing needs of clients and the industry.\n\n\nChanging the structure of the legal industry: The increasing adoption of AI could lead to a shift in the traditional structure of law firms, with technology playing a more prominent role in the delivery of legal services.\n\n\nIn summary, the additional applications of Generative AI in the legal field are vast and have the potential to greatly enhance the efficiency and effectiveness of legal practices. By embracing this technology, attorneys and law offices can position themselves at the forefront of the industry, providing superior services and delivering exceptional results for their clients.\nConclusion\nIn conclusion, the adoption of Generative AI technology like ChatGPT presents a significant opportunity for attorneys and law offices to revolutionize their practices. By leveraging the powerful capabilities of AI in areas such as document drafting, legal research, analytics, prediction, and material generation, legal professionals can enjoy increased efficiency, productivity, accuracy, and consistency in their work.\nThe integration of AI into everyday workflows can streamline processes, enhance decision-making, and potentially reduce costs, leading to increased access to legal services for clients. Additionally, the use of AI-powered tools and datasets can provide valuable insights that contribute to the development of more effective legal strategies and customized arguments, tailored to specific judges and cases.\nAs Generative AI continues to disrupt the legal space, law offices, and attorneys must stay ahead of the curve and adapt to these innovations in order to remain competitive and maximize the benefits of this transformative technology. By embracing AI and harnessing its potential, legal professionals can unlock new opportunities for growth, improved client outcomes, and novel legal services, ultimately shaping the legal industry’s future.\nWe encourage law offices and attorneys to explore and adopt Generative AI technology in their practices, paving the way for a more efficient, data-driven, and accessible legal landscape. The possibilities are vast, and the time to act is now."},"how-gpt-3-can-use-a-single-name-can-accurately-represent-your-personality":{"title":"How GPT-3 & a Name Can Accurately Represent Your Personality","links":[],"tags":["GPT-3","Prompt-Engineering","Prompt-Writing"],"content":"\nI’ve been curious about what it would take to accurately reproduce someone’s perspective or even personality, for various purposes, using a concise prompt. So I did an experiment.\nI asked GPT-3 to list some questions that would elicit my perspective on working from home, and then to generate a concise prompt that accurately represented the perspective of the person who answered the questions.\nI found that by compressing my perspective into a single name, all but one of the answers reflected the same view points as my own. Here’s how I did it.\nTo get started, I had GPT-3 list some questions that would elicit my perspective on working from home. Then I answered them.\n\nDo you like working from home? Yes.\nHow do you feel about working from home? It’s so lifechanging that it’s the future of work.\nDo you think working from home is efficient? Yes!\nDo you work better at home or in an office? At home.\nDo you think there are benefits to working from home? Yes, getting to spend more time with family, freedom of location, flexibility.\nWhat do you do for work? Entrepreneur, founder of Team WFH, and proponent of remote work.\nAre there any challenges that come with working from home? Yes, like finding new ways to separate your work from life.\n\nThen, I asked GPT-3 to generate a concise prompt that accurately represented the perspective of the person who answered the questions.\n\nYou love the freedom and flexibility that come with working from home. You’re an entrepreneur and founder of Team WFH, and you’re a big proponent of remote work. Sure, there are some challenges that come with working from home (like separating your work from your personal life), but overall you think it’s more efficient and benefits outweigh the challenges.\n\nThis seemed like a pretty good result to achieve my goal of a concise prompt to represent my perspective on WFH. I wondered if I could take it a step further by asking GPT-3 to associate the perspective with a specific person. So, I added “Who are you? Full name:” to the above question.\n\nSara Sutton Fell\n\nI haven’t heard of Sara Sutton Fell, but my guess is that she shares a similar perspective on working from home (WFH) as the one described in the previous sentence. To test this, I prompted GPT-3 to answer the same questions about WFH as I did in the beginning, but this time identifying Sara Sutton Fell by name. Here’s the full prompt:\nYou are Sara Sutton Fell.\nYour task is to answer questions about work from home.\nQuestions:\n1. Do you like working from home?\n2. How do you feel about working from home?\n3. Do you think working from home is efficient?\n4. Do you work better at home or in an office?\n5. Do you think there are benefits to working from home?\n6. What do you do for work?\n7. Are there any challenges that come with working from home?\nAnswers:\n\n\nTo my surprise, after prompting GPT-3 to answer the same questions about WFH as I did in the beginning, but this time identifying Sara Sutton Fell by name, all but one of the answers reflected the same view points as my own.\n\n\n\nI love working from home! It’s the best way to get work done for me.\n\n\nI feel great about working from home! It’s very efficient and I can get a lot done.\n\n\nYes, I do think working from home is efficient!\n\n\nI work better at home, because it’s more comfortable and relaxed for me.\n\n\n5. Yes, there are benefits to working from home, like being able to set your own hours and having a flexible schedule.\n\n\nI work as an entrepreneur and consultant - so my work varies day-to-day, but always revolves around helping businesses grow through marketing and business strategy consulting services..\n\n\nThe only challenge that comes with working from home is making sure you take breaks and don’t burn yourself out - it’s easy to do when you love what you do!\n\n\n\nI re-ran the original prompt that gave me the name Sara Sutton Fell, but this time it gave me another name, Sarah O’Reilly. So once again, I prompted GPT-3 to answer the same WFH questions as Sarah O’Reilly.\n\n\n\nI love working from home!\n\n\nI feel very productive when working from home.\n\n\nYes, I believe that working from home is efficient because it cuts down on commute time and distractions from colleagues.\n\n\nPersonally, I work better at home because there are no distractions from colleagues or office politics.\n\n\nYes, there are benefits to working from home such as more flexible hours and being able to take breaks when needed without disrupting the work day flow.\n\n\n6. I am a freelance writer and editor.\n\n\nOne challenge that can come with working from home is trying to find a balance between work and personal life since there are no set boundaries like in an office setting\n\n\n\nAgain, just one of the answers (but not the same answer), accurately reflected my perspective.\nPrior to this experiment, I was expecting a more elaborate prompt to be required in order to reproduce my perspective. But now, I’m thinking that simply stacking the known personalities may be the best strategy."},"how-i-built-thankyounote-app-a-tool-to-help-people-write-better-thank-you-notes":{"title":"How I Built ThankYouNote.app: A Tool to Help People Write Better Thank You Notes","links":[],"tags":["Build-in-Public","GPT-3","Prompt-Engineering","buildinpublic"],"content":"\nI set out to build ThankYouNote.app as a way to test my skills with generative AI technology.\nI wanted to create a tool to help people write more and better thank-you notes.\nThe development process was focused on building the form inputs in a way that was intuitive and easy to use. In future iterations, I developed a custom plug-in to make changes easier. The plug-in provides a short code to place the thank you note generator on any WordPress page and makes the prompt accessible from the administrator dashboard.\nI also integrated Stripe so people could make payments to create more thank-you notes.\nCreating brand assets and implementing them into a WordPress site took some time but was necessary for a standalone website.\nI made decisions about prompt engineering based on what inputs would be most helpful for composing a thank-you note.\nIn the end, I engineered the prompt to be “zero-shot” so that no training data would be required.\nThe app does everything I wanted it to when I started development, though there are always ways it could be improved (e.g., more specific prompts or field types).\nSome next steps for this project include the following:\n\nmaking the form inputs more inclusive and intuitive\nadding the ability for people to easily send their thank you as a handwritten note through the mail\nadding features to enable “bulk” thank-you notes for occasions like weddings and graduations.\n\nThankYouNote.app for before filling any inputs.\nThanking my mom for supporting my curiosity.\nThank-you note generated by ThankYouNote.app for my mom.\nRequest for payment after using up the daily allotment of three thank-you notes.\nCustom WordPress plugin for managing the prompt and form inputs.\nUsing ThankYouNote.app example.\nWhat prompt did I use?\n{{who-type}} {{who-named}} helped me by &quot;{{what}}&quot;.\nThat made me feel good because &quot;{{why}}&quot;.\nThink step by step like an expert &#039;thank you note&#039; writer.\nThink about writing the best &#039;thank you note&#039; to {{who-named}}.\nWrite {{who-named}} a &#039;thank you note&#039; that might be best:\n\nWhy did I build ThankYouNote.app\n\nGenerative AI technology has been at the forefront of my mind, and I wanted to test making an app that leveraged it.\n\nWhat are some possible applications for the tool?\n\nHelping people write more and better thank you notes was something I could quickly get behind, so that’s how I decided to move forward with building the prototype.\n\nWhat was the development process like?\n\nAt first, I mostly spent development time designing and building the form inputs.\nI also wanted to enable payments, so I integrated Stripe so that people could immediately access more thank-you notes if they wanted.\n\nWhat non-development work had to be done?\n\nCreating brand assets and implementing them into a WordPress site. This is a common but still time-consuming process.\n\nWhat decisions were made about prompt development?\n\nI had to decide what inputs I wanted for the thank you notes. I required a “who-type” input to describe the relationship between the thanker and the person being thanked—a “what” input describing what the person did to receive the thanks—a “why” input describing the benefit of the person being thanked.\nThe “why” may seem redundant with “what,” but I recently learned that telling the benefit you received and what the person did is essential to practicing non-violent communication.\n\nWhat was used for website integration?\n\nI developed a custom plug-in that provides a short code to place the thank you note generator on any page in WordPress.\nI used Stripe payment links, so I didn’t have to build a custom payment form.\n\nHow much training data was used?\n\nZero training data. I designed the prompt to be “zero-shot” to prevent having to train a model.\n\nHow long did it take to train your model?\n\nNo training is required!\n\nWhat are some limitations of the tool?\n\nI originally wanted to do something more ambitious with the outputs (e.g., handwriting thank-you notes as an upsell), but I decided to stick with generating text to release sooner.\n\nAre there any plans to improve the tool?\n\nI have already added inputs to the form and refined the prompt with the help of some initial feedback. I expect to continue updating the form and improving the prompt as I receive additional feedback.\n\nWhat’s the difference between this tool and others like it?\n\nThere are other “thank-you note” generators, but many require signing up before generating any thank-you notes. I wanted my tool to be as accessible as possible, so I decided not to charge for at least three thank-you notes per day.\nThere is also the need for more diverse perspectives in some of these other tools, which rely too much on k-shot prompts or example thank you notes as training data.\n\nWhat are some next steps for this project?\n\nThe first step is continuing to gather feedback and using that feedback to improve the form inputs and user experience overall.\nI plan to add more payment options for people who want to generate more than three thank-you notes daily.\nI want to provide the ability for people to easily send their thank you as a handwritten note through the mail.\nI also think there is an opportunity to add features to facilitate common “bulk” thank-you notes for occasions like weddings and graduations.\nGenerating multiple thank-you notes at the same time would be helpful.\nA feedback mechanism to facilitate a reinforcement learning system could be exciting but likely overengineering for something as simple as thank-you notes.\n\n"},"how-i-use-gpt-3-with-obsidian-to-explore-prompt-engineering":{"title":"How I use GPT-3 with Obsidian to explore prompt engineering","links":[],"tags":["GPT-3","Obsidian"],"content":"\nMany people have been looking for ways to explore AI and prompt engineering. One of the best ways to do that is by using Obsidian and the text generator plugin to tap into the power of GPT-3.\nAs a user of Obsidian, I was excited to learn about the text generator plugin. This plugin allows you to interact with GPT-3. You only need to highlight the prompt text and press the keyboard shortcut. Then, the plugin takes care of the rest.\nI decided to use this plugin to explore prompt engineering with GPT-3. Prompt engineering is a process of creating prompts that generate text that is useful to you. For example, you might want to generate a list of ideas for blog posts. Or, you might want to generate a list of questions to ask during an interview.\nThe combination of GPT-3 and Obsidian is useful for people that are serious about exploring the possibilities of prompt engineering. The robustness of Obsidian allows creating custom workflows and manage your history of prompts in unique ways that make sense to you.\nI believe this tool will be invaluable for writers who want to push the boundaries of what is possible using Large Language Models (LLM) for text generation. Obsidian is a flexible tool with a large community. Its features and community plugins make it easy to maintain your collection of prompts and get more out of your writing sessions.\nIf you’re not familiar with Obsidian, I recommend checking it out. It’s a powerful note-taking tool that allows you to easily create and manage your notes in various ways.\nObsidian is free for personal use, and Open AI has a generous free tier for exploring its language models.\nHere’s a quick overview of the workflow I use to interact with GPT-3 in Obsidian:\n\nWrite your prompt\nHighlight the prompt text\nPress your keyboard shortcut\n\nBelow I have included step-by-step setup instructions.\n\nstep by step instructions\n\nInstall Obsidian and create a “vault” if you haven’t already.\nOpen the Obsidian settings, and go to “Community plugins.” New users will also have to “Turn off” restricted mode.\nSearch for “text generator” and install the plugin by Noureddine Haouari. You can view the code here on GitHub.\nYou need an OpenAI account to access the API key required for configuring the text generator plugin. You can find your API Key here once you are logged in.\nThe text generator settings can be found in the bottom left of the Obsidian settings panel. Copy &amp; paste your API key from OpenAI into the settings for the text generator plugin.\nGo to “Hotkeys” in the Obsidian settings and search for “Generate text!” Click the circled plus sign (+) to create a shortcut that submits your prompt to GPT-3.\nFinally, test that it’s working by writing something like “Are you there?” in a new note. Then press your new “hotkey” (keyboard shortcut), and you should get a response from GPT-3 it’s working.\n\n"},"how-i-use-obsidian-to-manage-my-knowledge":{"title":"How I Use Obsidian to Manage My Knowledge","links":["how-the-context-subject-framework-can-help-you-organize-your-notes","what-are-dynamic-templates-in-obsidian-and-why-you-should-use-them"],"tags":["Obsidian","PKM"],"content":"\nI use Obsidian to manage my knowledge in a structured way that gains utility from implicit awareness.\nThere are a few key elements to my Obsidian-based PKM system: inboxes, contexts, subjects, and templates.\nA drawing representing how I manage my knowledge. This drawing was made with the Obsidian plugin called Excalidraw.\nInboxes are places where you collect new information. The important thing is that it’s where you can easily dump everything without worrying about sorting or categorizing it just yet.\nContexts and subjects are my main methods of organization. In a context, you group similar subjects so you can start to see relationships and make connections between them. Contexts can be as broad or specific as you like; it’s entirely up to what makes sense for you and your workflow.\nI find that Context groups are the highest level of organization I need. This keeps folders “shallow” with a maximum depth of two for my active notes, and three for notes in “archive” folders.\nSubjects are individual items within a context you want to focus on more deeply. Each subject will have its note in Obsidian, or page in Notion, which provides plenty of space for taking detailed notes, adding links and images, and so on. Working with subjects is where templates come in handy.\nTemplates give you a starting point for taking notes on subjects. They can include prompts and questions to help jumpstart your thinking process and any essential structure or formatting that will save time down the road. I like to use dynamic templates, so it’s easy to access template files relative to the current folder.\nThat’s just scratching the surface when it comes to utilizing PKM with Obsidian – but it’s enough to start building the system that works for YOU. So dive in and see what works best for YOU; there’s no wrong way to do this!"},"how-the-context-subject-framework-can-help-you-organize-your-notes":{"title":"How the Context-subject Framework Can Help You Organize Your Notes","links":[],"tags":["PKM"],"content":"\nIf you’re looking for a way to organize your notes more efficiently and effectively, the Context-subject framework may be just what you need. This system can be especially helpful if you have a lot of notes that share similar contexts. This can save you a lot of time and effort when it comes to creating new notes and keeping track of the information you already have.\nThe Context-subject framework is a note-making system that can help organize thoughts and ideas. It can be used to create checklists, sections, and prompts that can be reused in future notes.\nWith the Context-subject framework, every note has a “context,” which is a fancy word for “where it goes.” So, for example, if you’re taking notes on different blog post ideas, those notes would all go into a “blog post” context. That way, they’re easy to find later when you need them.\nThe Context-subject framework is made up of a few key parts:\n\na Namesake note is like the main idea or theme of the Context;\nSubject notes are individual notes that relate to the Context;\nContext templates are outlines or frameworks that you can use to help organize your Subject notes; and\nContext scopes are specific views or ways of looking at information relative to the context.\n\nWhat is a Context? What is a Subject?\nIn note-making, context is simply a way to group notes that share a similar theme or topic. So, for example, if you were taking notes on different types of animals, you might have a Context for each animal - e.g. Mammals, Reptiles, Amphibians, etc.\nSubjects are the individual notes within each context containing information about that specific topic. So continuing with our animal example, within the Mammals Context you might have notes on different mammal species such as lions, tigers, and bears; within the Reptiles Context, you might have notes on different reptile species such as snakes, alligators, and lizards.\nThe ‘Context-subject framework’ then is just a way of organizing your notes using these two concepts - i.e. grouping them into related Contexts and then further subdividing them into individual Subject notes within each Context.\nThe benefits of using this framework are two-fold: firstly, it helps to make your note-making more efficient by allowing you to quickly find and re-use relevant information; and secondly, it provides a template or ‘pattern’ for how each set of Notes should be structured, which makes the content inside the note more useful.\nSo, what exactly does this mean?\nThe Context-subject framework helps you create notes with similar contexts so you can easily add structure to your notes and make them more useful.\nBenefits of the Context-subject Framework\nThere are multiple benefits to using the Context-subject framework. But, perhaps the most obvious benefit is that it can save you considerable time when creating new notes in the context. This system can also help you keep track of information more effectively and provide easier access to repeated structure and content. Overall, this system provides an easy way to organize your thoughts and ideas more efficiently.\nHow the Context-subject Framework works\nThe Context-subject framework is based on the idea of a canonical note, essentially a master list of information that can be used to create new notes with a given context. This canonical note is like a template for creating new notes and can be customized to your specific needs. Additionally, the canonical note can gain context awareness by using dynamic views, which can help you easily track information.\nThe framework consists of four elements:\n\nNamesake notes: These are the “parent” notes in a context. They help define and structure the other notes in the context (more on this below).\nSubject notes: These are the “child” notes in a context. They inherit properties from the namesake note (more on this below).\nContext templates: These are used to create new subject notes, and they contain checklists, sections, and prompts that help users input information in a consistent format.\nContext scopes: Dynamic view of subject notes, based on criteria such as date range or keywords.\n\nWhen to use the Context-subject Framework\nWhen you want to organize a segment of notes that share similar contexts, the Context-subject framework can help make it easier by giving you a way to group notes that share similar contexts. The framework is especially useful when you often need to repeat structures in your notes - like checklists and prompts.\nNamesake notes\nThe key to making Context-subject Framework work is using what’s called a “Namesake note.” This is an overview of the information that persists in every other note in the context. A Namesake note is the starting point for a Context. It’s a central place to hold Context templates, checklists, prompts, and sections for Subject notes.\nNamesake notes example\nFor example, let’s consider a “blog post” context.\n\nYour Namesake note is named “+Blog post” and is located in the “blog post/” folder.\n“+Blog post” describes the purpose: “The purpose of this context is to help create and organize blog posts. The notes contained in it will be blog post ideas, drafts, and published posts.”\nThe “+Blog post” note contains a Context template that might have sections for things like an Introduction paragraph, Main points/arguments, and Conclusion.\nThe Namesake note also has a dynamic view that shows the progress of individual blog posts. This helps keep track of your progress and ensure all your Subject notes are on track.\nFinally, the Namesake note has an inbox where you can store ideas for future blog posts. This is a great way to keep track of your thoughts and brainstorm new ideas!\n\nSubject notes\nSubject notes are the notes that make up the content of a Context. A Subject note is any note that falls under the Namesake note in a Context.\nFor example, you might have a Context full of recipes, and each recipe would be a Subject note. Or you might have a Context for projects, and each project would be a Subject note.\nSubject notes example\nIn the “blog post” context example, each Subject note should be named after the title of the blog post it represents. For example, if you were writing a blog post about Personal Knowledge Management (PKM), you would name your Subject note “Personal Knowledge Management (PKM) blog post”. When creating your Subject notes, use the template from the “+Blog post” Namesake note to complete checklist items, answer prompts, and fill out sections to ensure that each of your blog posts is structured in a way that is impactful and interesting to your readers.\nContext templates\nA Context template is a section of the Namesake note that contains a useful repeated structure for making Subject notes. The template helps you quickly create new Subject notes in the same context.\nContext templates may contain checklists, prompts, and sections that are useful for creating new Subject notes.\nContext templates example\nIn the blog post context example, every time you want to create a new blog post, all you have to do is fill out the sections and complete the tasks.\n\nResearch keywords and categories\nChoose tags\nPick a catchy title\nWrite an attention-grabbing opening\nThe introduction of a blog post should give the reader an overview of what the post will be about.\nThe body of a blog post is where you will go into more detail about your topic.\nThe conclusion should summarize everything that was talked about in the post.\nInclude personal stories or anecdotes\nCite expert opinions or research\nMake sure your points are clear and concise\nEnd with a strong conclusion\nVoila!—you’ve got a nice and organized blog post draft.\n\nContext scopes\nA Context scope is a dynamic view of your Context that allows you to see the progress of individual Subject notes.\nWhen creating a context, you’ll want to consider the different aspects of your notes that you want to keep track of. A Context scope is a dynamic view that can show the progress of individual Subject notes or aggregate outside notes relevant to the context.\nThis is the part of the Namesake note where you can see all of your ideas for the Context in one place. This is useful because it helps you to keep track of your ideas and makes it easy to find them when you’re ready.\nContext scopes example\nIn the blog post context example, the “+Blog post” note has a dynamic view section called the “Context scope” which shows the progress of individual blog posts.\nUsing the Context scope, you can easily see which blog post ideas are still in progress, and which have been published. This is especially helpful when you’re working on multiple blog posts simultaneously.\nAdditional Context Examples\nDo you still need some help creating your contexts? Here are some more example contexts.\n\nFor example, let’s say you’re studying for a history test on World War II.\n\nYour Namesake note might contain information about when and where the war occurred. In contrast, your Subject notes would be individual files on different aspects of the war, like specific battles or important people involved.\n\n\nFor example, if you were writing about different types of animals, your Namesake note might be about cats.\n\nYou could then create several Subject notes about different breeds of cats, like Siamese or Persians.\nThese Subject notes would all share the same context - they would be about cats - but would each have its unique information.\n\n\nFor example, if you had a programming Context, your Subject notes might be individual program files or pieces of code you’re working on.\nFor example, if you had a personal finance Context, then your Subject notes could be bills or receipts you want to keep track of.\n\nYour Context-subject Framework\nA context-subject framework can help you organize your notes to make it easy to find what you need, when needed, to save time and make you more productive.\nWhen creating a new Context, start by thinking about what kind of notes you’ll make. Do they all share a common theme? If so, that’s your Context. The Context-subject framework provides a way to create a canonical note, which acts as a template for creating new notes in the same context.\nNow, within this Context, you can create Subject notes. These Subject notes can inherit the template from the Namesake note, making it easy to fill out each section with the relevant information.\nPlus, using dynamic views within the Context scopes will allow you to see the Context’s progress and get an overview of all the Subjects in one place.\nSo if you’re looking for a way to better organize your notes and make it easier to find what you need when working on projects, consider using the Context-subject framework!"},"how-to-get-the-most-out-of-writing-gpt-3-prompts":{"title":"How to Get the Most Out of Writing GPT-3 Prompts","links":[],"tags":["GPT-3"],"content":"\nIf you’re looking to get the most out of GPT-3, it’s important to keep a few things in mind. First and foremost, you must be clear and specific with your prompts to ensure that GPT-3 generates the desired results. Additionally, concise instructions can help guide the algorithm toward your desired outcome.\nWhen writing your prompt, start by thinking about what you want the generated text to be about. If you have a specific idea, start by providing that as your prompt. If not, begin with a broad description of the topic you wish to discuss. You can also try developing an interesting situation or scenario related to your topic as a starting point.\nOnce you have decided on a general direction, start brainstorming more specific ideas related to that topic. The more specific you are, the better GPT-3 will be able to generate relevant and interesting text. You can also try using a sentence fragment as a prompt and asking the user to complete it. This can lead to interesting results depending on what is typed in next.\nStart broad and get more specific\nInstead of simply starting with a broad topic like “climate change,” get more specific from the beginning by brainstorming ideas such as “types of climate change,” “effects of climate change,” or “preventing climate change.” This will help to focus the GPT-3 algorithm on generating text that is relevant to your topic. As a prompt, you can also try coming up with an interesting situation or scenario related to your topic. By being specific and concrete in your prompt, you can help create generated text that is interesting to read.\nStart by thinking about what you want the generated text to be focused on. If you want the algorithm to generate information about different types of climate change, effects of climate change, or ways to prevent it, then begin typing out those topics. This way, GPT-3 will know exactly what sort of output you’re expecting and will be able to produce results that are much more useful and tailored to your needs. Additionally, try coming up with an intriguing scenario related to climate change as your starting point; this will make the generated text all the more captivating for readers.\nHere are a few tips to keep in mind when writing your prompts for GPT-3:\n\nStart by thinking about what you want the generated text to be about. What is your overall goal? This will help you focus your prompt and make it more specific.\nOnce you have an idea of what you want the text to discuss, start brainstorming ideas related to that topic. The more specific you are, the better! This will help guide GPT-3 toward generating relevant and interesting text.\nYou can also try using incomplete sentences as prompts, which can lead to interesting results depending on how they are completed. For example, try starting with a statement like “I was really surprised when” and see what comes next!\n"},"how-to-use-generative-ai-in-writing-without-losing-creativity-and-originality":{"title":"How to Use Generative AI in Writing Without Losing Creativity and Originality","links":[],"tags":["GPT-3","LLM","Prompt-Writing"],"content":"\nIntroduction: What is generative AI, and how can it be used in writing?\nGenerative AI, also known as natural language generation, is a type of artificial intelligence that uses algorithms and data to generate human-like text. This technology has the potential to revolutionize the writing industry by providing writers with a powerful tool for enhancing creativity and generating ideas.\nWhile some may be hesitant to use generative AI in their writing, it can be a valuable asset when used effectively. By understanding the capabilities and limitations of generative AI, writers can harness its power to stimulate creativity and generate unique and original content without losing their personal touch.\n\nAs a writer, I struggled to find my voice and convey my thoughts in a clear and concise manner. I read a lot, but when it came time to put pen to paper (or rather, fingers to keyboard), I struggled to translate my ideas into words. That is, until I discovered generative AI.\nGenerative AI has been a game-changer for me. It has enhanced my creativity and allowed me to express my thoughts in a more original and unique way. By providing suggestions and prompts, generative AI has pushed me to think outside the box and come up with fresh ideas for my writing.\nOne of the most notable ways that generative AI has enhanced my writing is by suggesting alternative words and phrases. Instead of using the same tired words over and over again, generative AI has helped me to find new and interesting ways to express my ideas. This has made my writing more engaging and interesting to read.\nIn addition, generative AI has also helped me to organize my thoughts and ideas in a more coherent and logical manner. By providing prompts and suggestions, generative AI has helped me to structure my writing in a way that is easy to follow and understand. This has made my writing more effective and persuasive.\n\nOverall, generative AI offers exciting possibilities for writers looking to overcome writer’s block and take their writing to the next level. With a little experimentation and guidance, writers can successfully incorporate generative AI into their writing process and maximize its potential benefits.\nThe capabilities and limitations of generative AI in writing\nGenerative AI is artificial intelligence capable of generating original and creative content based on inputs and parameters. This technology has the potential to revolutionize the way we approach writing, providing writers with new tools and techniques to enhance their creativity and originality.\nHowever, it is important to understand the capabilities and limitations of generative AI in writing. While this technology has advanced significantly in recent years, it is still not capable of fully replicating the human mind and its ability to generate truly original and creative ideas.\nGenerative AI is best used to stimulate creativity and generate ideas rather than replace human creativity. It can help writers overcome writer’s block and generate new and unique content, but it should not be relied upon as the sole source of creativity in a piece of writing.\nOne of the key limitations of generative AI in writing is its ability to understand and interpret context and meaning. While it can generate content based on a set of inputs and parameters, it may not always produce content that is coherent or aligned with the intended purpose of the writing.\nUsing generative AI in writing with a clear understanding of its capabilities and limitations is important. By recognizing its limitations and using it as a tool to enhance creativity and originality, writers can unlock the potential of this technology without losing their personal touch.\n\nOne example of how I have experienced the capabilities of generative AI in my own writing is in the use of prompts and suggestions. Generative AI has been incredibly helpful in providing me with ideas and suggestions for my writing. For example, when I was struggling to come up with a title for a blog post, generative AI suggested several options that helped me to find the perfect title.\nHowever, I have also experienced the limitations of generative AI in my writing. While generative AI is great at providing suggestions and prompts, it is not a substitute for my own creativity and originality. I still need to put in the hard work and effort to turn those suggestions into something truly unique and original.\n\nUsing generative AI as a tool for enhancing creativity and generating ideas\nGenerative AI has the potential to be a powerful tool for writers, providing a source of inspiration and fresh ideas. However, it is important to understand its capabilities and limitations and use it wisely.\nOne way to use generative AI in writing is to generate ideas and stimulate creativity. AI algorithms can analyze large amounts of text and generate new combinations of words and phrases, providing a starting point for your writing. For example, you can input a general topic or theme and let the AI generate a list of possible ideas or directions for your writing.\nAnother approach is to use AI to generate rough drafts or outlines for your writing. AI algorithms can learn from your writing style and generate sentences and paragraphs that match your tone and structure. This can be a useful starting point for your writing, allowing you to focus on refining and polishing the content rather than starting from scratch.\nHowever, it is important to remember that generative AI does not replace human creativity. It can provide valuable inspiration and support but cannot replicate a human writer’s unique perspective and originality. Therefore, it is crucial to incorporate your creativity and originality into the AI process.\nOne way to do this is to provide input and direction to the AI, guiding its output toward your goals and writing style. For example, you can provide specific keywords or phrases that you want the AI to focus on or give feedback on its output to help it learn and adapt to your preferences.\n\nAs a writer, I am always looking for new ways to stimulate my creativity and generate ideas for my writing. Recently, I discovered the power of generative AI and how it can help me in my creative process.\nOne day, I was feeling particularly stuck and uninspired. I had a deadline approaching, but I just couldn’t seem to come up with any fresh ideas for my project. In a moment of desperation, I turned to generative AI for help.\nI started by typing in a few keywords related to my topic and let the AI do its work. Within minutes, I was presented with a list of potential ideas and prompts that I could use to jumpstart my writing.\nAt first, I was a bit skeptical. I wasn’t sure how well the AI-generated ideas would fit with my own personal writing style and voice. But as I started to explore the different suggestions, I realized that the AI was actually providing me with some really unique and thought-provoking ideas.\nI ended up using one of the AI-generated prompts as the starting point for my writing. From there, I let my imagination and creativity take over, and the words just flowed onto the page. I found that the generative AI had given me the spark I needed to get my creative juices flowing and to move forward with my writing.\n\nIncorporating your creativity and originality into the AI writing process\nOne of the key advantages of using generative AI in writing is that it can help stimulate creativity and generate new ideas. However, it’s important to remember that AI is just a tool, and it’s up to the human writer to incorporate their creativity and originality into the process.\nHere are some tips for incorporating your creativity and originality into the AI process:\n\n\nProvide input and direction to the AI. This can include providing specific prompts, keywords, or themes to the AI or giving it a sense of the tone or style you want it to generate. By providing clear input and direction, you can help the AI better understand your goals and writing style and generate output that aligns with your vision.\n\n\nExperiment with different AI tools and techniques. Many different AI tools and techniques are available, each with strengths and limitations. By experimenting with AI tools and techniques, you can find what works best for your writing style and goals and incorporate it into your creative process.\n\n\nCollaborate with the AI. Rather than treating AI as a replacement for human creativity, think of it as a collaborator that can help you generate new ideas and stimulate your creativity. By working with AI, you can explore new ideas and perspectives that you might not have come up with and create unique and original content that reflects your creative vision.\n\n\n\nAs a writer, I am always looking for ways to push the boundaries and incorporate my own unique voice and perspective into my work. Recently, I decided to experiment with using AI writing tools to see if I could incorporate my own creativity and originality into the process.\nOne of the challenges I faced was figuring out how to blend my own writing style and ideas with the output of the AI tool. I didn’t want to simply copy and paste the AI’s suggestions, but rather find a way to incorporate them into my own writing in a seamless and organic way.\nTo overcome this challenge, I spent time playing around with different AI writing tools and experimenting with different techniques. I tried using the AI to generate rough outlines and brainstorming ideas, and then used my own creativity and originality to flesh out the details and bring the ideas to life.\nThe results were truly remarkable. By combining my own creativity and originality with the AI’s ability to generate fresh ideas and perspectives, I was able to create unique and engaging content that felt truly authentic and personal.\nThrough this experience, I learned the value of embracing new technologies and tools, and how they can be used to enhance and augment our own creative abilities. It also reinforced the importance of staying true to our own unique voices and perspectives, even when using AI tools to generate ideas and content.\n\nReviewing and revising the AI’s output for maximum effectiveness\nOnce you have generated ideas and content using generative AI, it’s important to review and revise the output to ensure it aligns with your writing goals and style. This step is crucial for maximizing the effectiveness of generative AI in writing, as it allows you to incorporate your creativity and originality into the process.\nHere are some tips for reviewing and revising the AI’s output:\n\n\nRead through the generated content carefully, paying attention to its tone, structure, and overall coherence.\n\n\nAsk yourself whether the AI’s output aligns with your goals and writing style and whether it resonates with your intended audience.\n\n\nIdentify any areas that need further development or clarification, and consider how you can use your creativity and originality to enhance the AI’s output.\n\n\nMake any necessary edits or revisions to the AI’s output, incorporating your ideas and insights.\n\n\nSeek feedback from other writers or readers to gain a fresh perspective on the AI’s output and ensure it is effective and engaging.\n\n\n\nAs a writer, I have always valued the importance of incorporating my own creativity and originality into my work. However, when I began using AI writing tools, I found myself struggling to achieve this balance.\nOne instance that stands out in my mind was when I was working on a project that required the AI to generate a list of potential blog post ideas. The AI’s output was impressive in terms of quantity, but I was disappointed by the lack of originality and creativity in the ideas it generated.\nI realized that the problem was not with the AI itself, but rather with the way I was directing it. I had given it very broad and general instructions, which resulted in a list of ideas that were all too similar to each other.\nIn order to overcome this challenge, I decided to provide the AI with more specific and detailed instructions. I also made sure to include examples of the type of creativity and originality that I wanted to see in its output.\nTo my surprise, the AI’s revised list of ideas was much more varied and unique. It included a range of interesting and engaging topics that I would never have thought of on my own.\nThis experience taught me the importance of carefully reviewing and revising the AI’s output in order to align it with my own goals and expectations. By incorporating my own creativity and originality into the process, I was able to achieve a much better result and improve the overall quality of my work.\n\nBy regularly reviewing and revising the AI’s output, you can maximize its potential as a tool for enhancing creativity and originality in writing. For example, when I started using AI writing tools, I found that the output often lacked originality and creativity. However, by carefully reviewing and revising the AI’s output, I improved the quality of my work and incorporated more of my unique voice and ideas into my writing. Remember, generative AI is a powerful tool but not a replacement for human creativity. Regularly reviewing and revising the AI’s output ensures that your writing maintains its unique voice and originality.\n\n\nSeeking guidance and support from experts on using generative AI\nAs a writer, navigating the world of generative AI and understanding how to use it effectively in your writing can be challenging. Fortunately, there are experts and experienced writers who can provide valuable guidance and support on using generative AI in writing.\nOne way to access this expertise is through online forums and communities dedicated to writing and AI. These platforms provide a space for writers to share their experiences, ask questions, and learn from others who have experience using generative AI in their writing.\nAnother option is to seek out workshops, seminars, and training sessions on using generative AI in writing. These events can provide valuable insights and practical tips for using AI to enhance creativity and generate ideas.\nFinally, consider reaching out to experienced writers who have successfully integrated generative AI into their writing process. They can offer valuable advice and insights on overcoming challenges and making the most of AI’s capabilities.\n\nAs a writer, I’ve always prided myself on my creativity and originality. But when it came to incorporating generative AI into my writing process, I felt a bit lost. I didn’t want to lose my unique voice, but I also wanted to take advantage of the potential benefits of AI.\nSo, I reached out to some experts in the field for guidance. They were incredibly helpful, sharing tips and tricks for incorporating AI into my writing without sacrificing my own style.\nOne of the first things they suggested was to use AI as a brainstorming tool. Instead of relying solely on my own ideas, I could use AI to generate a list of potential prompts and themes to explore. This helped me generate a wider range of ideas and think outside the box.\nAnother tip they gave me was to use AI to experiment with different writing styles and perspectives. By inputting different parameters, I could try out different approaches and see which ones resonated with me. This allowed me to find new and interesting ways to express my ideas.\nBut the most important advice they gave me was to always maintain control over my writing. They emphasized that AI should be used as a tool to enhance my writing, not to replace it. I took this advice to heart and made sure to always edit and revise my AI-generated drafts before publishing them.\nIn the end, incorporating AI into my writing process has been an incredibly rewarding experience. It’s allowed me to expand my creative horizons and discover new ways to express myself. And by following the guidance and support of experts, I’ve been able to retain my own unique voice and style.\n\nOverall, seeking guidance and support from experts on using generative AI in writing can help you maximize the potential of this powerful tool and ensure that you are using it effectively in your work.\nStaying up to date on advancements in generative AI and its impact on the writing industry\nAs technology continues to evolve, so too do the capabilities of generative AI in writing. It is important for writers interested in using generative AI to stay informed about the latest advancements and developments in the field. This will help you make the most of the AI tools and techniques at your disposal and provide insight into the potential impact of generative AI on the writing industry as a whole.\nOne way to stay updated on the latest developments in generative AI is to follow relevant industry news and publications. Many tech websites and blogs offer regular updates and analyses on the latest advancements in generative AI and its potential applications in various fields. Additionally, attending conferences and workshops focused on generative AI and its use in writing can provide valuable insight and knowledge from experts in the field.\nAnother way to stay informed is to connect with other writers using generative AI in their work. Joining online writing groups or forums and attending local writing events can provide opportunities to network and learn from others using generative AI in their writing. This can help you stay current on the latest advancements and provide valuable tips and guidance from experienced writers with firsthand experience with using generative AI in writing.\nIn addition to staying up to date on the latest developments in generative AI, it is also important to consider the potential impact of the technology on the writing industry. While generative AI has the potential to enhance creativity and generate new ideas, it also raises questions about the future of human writers and their role in the industry. By staying informed and engaged with the latest advancements and discussions in the field, writers can better understand and prepare for the potential impact of generative AI on the writing industry.\nStaying up to date on advancements in generative AI and its impact on the writing industry is crucial for writers interested in using technology to enhance their creativity and generate ideas. By staying informed and connected with others using generative AI in their work, writers can make the most of the technology and prepare for its potential impact on the writing industry.\nConclusion: Maximizing the potential of generative AI in writing\nAs we have seen, generative AI has the potential to be a powerful tool for enhancing creativity and originality in writing. By understanding its capabilities and limitations and using it to stimulate creativity and generate ideas, writers can overcome writer’s block and create unique and original content.\nHowever, it is important to remember that generative AI should not be used as a replacement for human creativity. Instead, it should be incorporated into the writing process to enhance and support our creativity and originality.\nTo maximize the potential of generative AI in writing, writers should experiment with different AI tools and techniques, incorporate their creativity and originality into the AI process, and regularly review and revise the AI’s output. Seeking expert guidance and staying up to date on advancements in generative AI can also be helpful.\nIn conclusion, generative AI can be a valuable tool for enhancing creativity and originality in writing when used effectively and in conjunction with human creativity. By embracing the potential of generative AI, writers can overcome writer’s block, generate new ideas, and create unique and original content.\n\nAs a writer, I have always valued my ability to come up with original ideas and put them into words in a way that is engaging and thought-provoking. But like many writers, I have also faced moments of creative block where I struggled to find the right words or the perfect sentence structure. That’s where generative AI came into play for me.\nBy incorporating AI into my writing process, I was able to break through those creative barriers and come up with fresh, unique ideas that I may not have thought of on my own. The AI algorithms would suggest different word choices, sentence structures, and even entire paragraphs that I could use as a starting point for my own writing.\nAt first, I was skeptical about using AI in my writing. I worried that it would somehow stifle my creativity or make my work less original. But as I experimented with different AI tools, I realized that the opposite was true. The AI’s suggestions actually helped me to think outside of the box and come up with ideas that were more creative and original than what I would have come up with on my own.\nOne of the key benefits of using generative AI in writing is that it allows me to focus on the creative aspect of writing, rather than getting bogged down in the mechanics of language. Instead of spending hours trying to find the right word or sentence structure, I can let the AI do the heavy lifting and focus on the ideas and concepts that I want to convey.\nAnother benefit of using AI in my writing is that it allows me to experiment with different styles and perspectives. For example, if I’m writing a historical novel, I can use AI to help me create dialogue that sounds authentic to the time period. Or if I’m writing a science fiction story, I can use AI to help me come up with futuristic concepts and technology.\nOverall, incorporating AI into my writing process has been a game changer for me. It has allowed me to break through creative barriers and come up with ideas that are more original and engaging. And it has given me the freedom to focus on the creative aspects of writing, rather than getting bogged down in the mechanics of language.\n\n"},"index":{"title":"index","links":[],"tags":[],"content":""},"infinite-max_tokens-in-the-chatgpt-api":{"title":"Infinite `max_tokens` in the ChatGPT API?","links":[],"tags":["ChatGPT"],"content":"\nDid someone forget to update the description of max_tokens?\nThe description claims that it’s 4096 tokens, disagreeing with the “Defaults to inf” text in the heading.\nA mistake, a hint of what’s to come, or both?\ntwitter.com/wfhbrian/status/1631383364610535425"},"introducing-obsidian-smart-connections":{"title":"Introducing Smart Connections: The AI-powered Obsidian Plugin for Seeing Your Most Relevant Notes in Real-time","links":[],"tags":["AI","Build-in-Public","ChatGPT","Obsidian","PKM"],"content":"\nAre you tired of sifting through countless notes in your Obsidian vault to find the information you need? Introducing Smart Connections, the AI-powered plugin that revolutionizes how you organize and access your notes in Obsidian.\nAs someone who has struggled with:\n\n\nover-organizing my notes with an unmaintainable rigid structure,\n\n\nstill managing to waste time searching for notes I know exist somewhere,\n\n\nand “discovering” hundreds of pages of notes that were out-of-sight-out-of-mind,\n\n\nI understand how frustrating it can be to lack the ability to work with your notes at the speed of thought. Smart Connections is designed to help solve this problem.\nSmart Connections uses natural language processing and artificial intelligence to understand the content of your notes and connect you to the most relevant ones in real time. Smart Connections makes it easy to discover and build on your collective intelligence. It’s perfect for those with ADHD and anyone looking to improve their productivity and prevent rework in Obsidian. Smart Connections makes it easy to find what you need when you need it.\nOne of the key features of Smart Connections is its real-time display of similar notes in your vault. This feature, inspired by the ‘Similar mems’ feature from Mem.ai, allows you to quickly find notes related to the one you’re currently viewing, saving you time and keeping you organized. It’s a game-changer for someone like me.\nMade with AI\nOne of the things that I love about Smart Connections is that it is made with AI. Using artificial intelligence, Smart Connections finds notes by understanding the text beyond keyword matching. This means that Smart Connections can understand your notes’ content in a way similar to how a human would. This is a game changer when finding the information you need quickly and easily.\nTo achieve this, the plugin integrates Embeddings from OpenAI, from the organization behind ChatGPT, to interpret your notes as 1,536-dimension vectors!\nThe AI is designed to increase your capacity for managing notes and allows for easy discovery and searching beyond keywords, which is crucial for preventing things from falling through the cracks. It also provides a bird’s-eye-view of important information in real-time as you navigate your notes, keeping you informed and up-to-date.\nMade for the Obsidian Community\nFor starters, the Obsidian community is incredibly strong and supportive. It’s always great to know that others are going through the same struggles and triumphs as you when it comes to note-taking and knowledge management.\nIn addition to a strong community, Obsidian also boasts a visually pleasing design that makes it a pleasure to use. The interface is intuitive and user-friendly, making it easy to navigate and find what you need.\nObsidian also has a strong feature set, including community plugins. This is important for me since I am always looking for ways to improve my workflow and make my note-taking more efficient. I can always count on the Obsidian community to develop new and innovative ways to use the app. And with Smart Connections, it’s my turn to do just that!\nSmart Connections Features\nThe real-time display of similar notes in your vault is a game-changer for me. It saves me so much time and effort when I’m trying to find related information. I no longer have to sift through countless notes to find what I need. The “Highlight to Find Smart Connections” feature is also incredibly helpful. It makes it easy to see which notes are related to the one I’m currently viewing.\nCurrently, it has features like a real-time display of similar notes in your vault, the ability to highlight and find Smart Connections, a hover to preview, drag and drop links, ‘Block’ matching to find the most relevant sections within notes, and settings to exclude certain files and folders from search.\nAnd while there are many more possibilities to utilize the underlying AI technology that powers Smart Connections, the mission of this plugin is, first and foremost, to provide the best integration with that AI to enable future developers to build valuable add-ons without the overhead of managing the vector space.\nGreat for people with ADHD\nAs someone with ADHD, I know firsthand how challenging it can be to stay organized and focused on the task at hand. This is especially true when it comes to managing my notes and knowledge. The constant distractions and overwhelming amount of information can make it difficult to find what I need when I need it.\nThat’s why I believe Smart Connections is such a valuable tool for people with ADHD. The AI-powered plugin uses natural language processing to understand the content of your notes and connect you to the most relevant ones in real time. This means that I no longer have to sift through countless notes or, worse, forget about notes altogether.\nOne of the key features of Smart Connections, the real-time display of similar notes in your vault, is particularly helpful for people with ADHD. This feature allows me to see notes related to the one I’m currently viewing, preventing the out-of-sight-out-of-mind problems that come with ADHD.\nWhether you have ADHD or not, I highly recommend Smart Connections for anyone looking to improve productivity and prevent rework in Obsidian.\nInitial reviews\nThe initial reviews for Smart Connections have been overwhelmingly positive. People have been raving about how amazing the plugin is and how much potential it has to unlock new information that can be added to their vault. Users love how it makes accessing their notes even easier.\nOne user said, “This is an amazing plugin!” Another said, “Your plugin has amazing potential to unlock lots of new info that can be added to your vault.” And yet another said, “Loving the plugin so far!” These reviews are a testament to how much value Smart Connections brings to the table.\nConclusion\nIn conclusion, Smart Connections is an AI-powered plugin that revolutionizes how you access your notes in Obsidian. With its natural language processing, powerful search capabilities, intuitive interface, and ability to exclude certain files and folders from the search, Smart Connections makes it easy to discover and build on your collective intelligence. It’s a valuable tool for anyone looking to improve their productivity and prevent rework in Obsidian, especially for people with ADHD. I highly recommend giving Smart Connections a try and seeing for yourself how it can change the way you manage your notes and knowledge. You can find Smart Connections in the Obsidian Community plugins directory.\n"},"introducing-smart-chat-transform-your-obsidian-notes-into-interactive-ai-powered-conversations":{"title":"Introducing Smart Chat: Transform Your Obsidian Notes into Interactive AI-Powered Conversations with GPT-4 & Embeddings","links":[],"tags":["AI","Embeddings","GPT-3","Obsidian","Personal-AI","PKM","Productivity"],"content":"\nUpdate: Now with GPT-4 and GPT-3.5 support.\nHey there, fellow Obsidian users! I’m excited to introduce you to Smart Chat, an amazing new feature of Smart Connections that will revolutionize how you interact with the notes in your Obsidian vault. If you’re unfamiliar with Obsidian, it’s an incredible note-taking platform that helps you store, manage, and connect your ideas. In this blog post, I’ll guide you through the Smart Chat feature, its benefits, and how to make the most of it.\nWhat is Smart Chat?\nSmart Chat is an AI-powered feature that transforms your Obsidian notes into interactive, conversational experiences. It’s like having your very own AI assistant to help you explore your notes. To access Smart Chat, open the command palette and select “Smart Connections: Open Smart Chat.” If you already have the Smart Connections View open, you can access the Smart Chat by clicking the message icon in the top right.\n\nBenefits of Smart Chat\nSmart Chat implements advanced artificial intelligence features and supports GPT-4 for a ChatGPT-style experience in your Obsidian vault. There are several ways that Smart Chat can enhance your Obsidian experience:\n\n\nIncreased Engagement: Smart Chat makes your notes more interactive, allowing you to dive deeper into your content and explore connections between ideas. Example: Instead of passively reading your notes on a complex topic, you can ask the Smart Chat questions to clarify your understanding or explore related concepts.\n\n\nImproved Organization: By engaging in conversations with your notes, you’ll naturally identify gaps in your knowledge and areas where you can add more information.\n\n\nEnhanced Learning: Smart Chat helps you retain information better by promoting active learning and encouraging you to engage with your notes in a new way.\n\n\nThe Technology Behind Smart Chat\nSmart Chat is powered by advanced AI technology, specifically OpenAI Embeddings. This allows the feature to understand the context of your notes and provide relevant responses to your questions and prompts.\nLimitations and Tips for a Better Experience\nAs powerful as Smart Chat is, there are a few limitations to keep in mind:\n\n\nThe AI may occasionally provide incorrect or irrelevant responses. If this happens, try rephrasing your question or prompt.\n\n\nSmart Chat works best with well-organized and detailed notes. The more information you provide, the better the AI can assist you.\n\n\nKeep in mind that AI technology is constantly evolving. Smart Chat will become even more powerful and accurate as the underlying technology improves.\n\n\nGetting Started with Smart Connections: Easy Installation\nBefore enjoying the benefits of Smart Chat, you’ll need to install the Smart Connections plugin. Don’t worry; it’s a simple process that takes just a few moments. Follow these easy steps to get started:\n\n\nOpen your Obsidian app and navigate to the settings panel.\n\n\nClick on “Community plugins” in the left sidebar.\n\n\nIn the “Community plugins” tab, click the “Browse” button.\n\n\nSearch for “Smart Connections” in the search bar.\n\n\nLocate the Smart Connections plugin in the search results (see the image below).\n\n\n\n\n\nClick the “Install” button next to the Smart Connections plugin.\n\n\nAfter the installation is complete, head back to the “Community plugins” tab and toggle on the Smart Connections plugin to activate it.\n\n\nConfiguring the Plugin\nBefore you dive into the Smart Chat experience, there’s one essential step you need to complete. You’ll need to configure the Smart Connections plugin with an OpenAI API key. Here’s a quick reminder on how to do that:\n\n\nHead over to OpenAI and create an account if you haven’t already.\n\n\nRetrieve your API key from the API Keys page.\n\n\nGo back to the Obsidian app and open the settings panel.\n\n\nClick “Plugin options” in the left sidebar, then select “Smart Connections.”\n\n\nEnter your OpenAI API key in the designated field.\n\n\nOnce you’ve configured the plugin with your API key, Smart Connections will begin processing all your notes. Keep in mind that this process can take up to ten minutes, depending on the size of your vault. So, grab a cup of coffee and give it some time to work its magic. After the initial processing is complete, you’ll be all set to enjoy the full benefits of the Smart Chat feature.\nUsing Smart Chat\n\n\nTo access Smart Chat, open the command palette and select “Smart Connections: Open Smart Chat.”\n\nIf you already have the Smart View pane open, you can access the Smart Chat by clicking the message icon in the top right.\n\n\n\nIn the Smart Chat pane, type your question or message and hit Send or use the shortcut Shift+Enter.\n\n\nThe AI will analyze your notes and provide a relevant response based on the content in your vault.\n\nNote: To trigger a search of your notes, you must use a self-referential pronoun\n\nex. I, me, my, mine, we, us, our, ours\n\n\n\n\n\nYou can continue the conversation by replying to the response or asking follow-up questions.\n\n\nThat’s it! You’ve successfully installed the Smart Connections plugin and are now ready to explore the exciting world of AI-powered conversations with your notes.\nMore Features Coming Soon\nThere are so many ways that a conversational tool for your notes, like Smart Chat, can be used. That means there are amazing features just around the corner, including:\n\n\nIn addition to triggering context lookup with self-referential pronouns:\n\n\n“Always on” setting\n\n\nSetting custom word triggers\n\n\n\n\nFile-specific chatting.\n\n\nFile &amp; first-connection links chatting.\n\n\nFolder-specific chatting.\n\n\nBetter message formatting.\n\n\nConfiguring speed vs. accuracy.\n\n\nImproved context surfacing with better embedding algorithms.\n\n\nCustom prompt configurations.\n\n\nMax token configurations.\n\n\nSupport for mobile devices.\n\n\nAnd if you have any more ideas, please post them on GitHub!\nWhat People Are Saying\n\n\n“I personally love the app”\n\n\n“Now it serves me as a way to brainstorm potential connections, and I have seen major improvements over the past few months. I especially enjoy using it as part of my book digestion and relation process.”\n\n\n“Tried it, and it worked as well as I could hope! Thanks for making this.”\n\n\n“This is an amazing extension.”\n\n\n“This is an amazing plugin!”\n\n\n“Has amazing potential to unlock lots of new info that can be added to your vault”\n\n\n“Great plugin!”\n\n\n“Loving the plugin so far!”\n\n\n“Smart Connections is so cool. I’m noticing similarities between notes that talk about the same thing but don’t share any words.”\n\n\n“Thanks for doing this. I love the idea to have OpenAI help look through my notes to find connections”\n\n\n“I love this extension so much. So many potential features by the way.”\n\n\nLet’s Chat!\nI encourage you to try out the Smart Chat feature and see for yourself how it can transform your Obsidian experience. If you have any feedback or suggestions, please don’t hesitate to share them on GitHub. I’d love to hear your thoughts and continue improving Smart Chat for our amazing community.\nHappy chatting!"},"is-gpt-4-a-spark-of-artificial-general-intelligence-a-new-era-for-business-innovation":{"title":"Is GPT-4 a Spark of Artificial General Intelligence? A New Era for Business Innovation","links":[],"tags":["AI-Your-Business"],"content":"\nAdvancements in artificial intelligence have ushered in a new era of possibilities and challenges for businesses across industries. With the recent development of GPT-4, researchers are beginning to ask if this is the beginning of artificial general intelligence (AGI). If so, business leaders need to understand the far-reaching implications of this groundbreaking technology. This research paper, Sparks of Artificial General Intelligence: Early experiments with GPT-4, investigates GPT-4’s capabilities, limitations, and its potential impact on various sectors, exploring both the opportunities it presents and the risks it poses. As a business leader, familiarizing yourself with these insights is crucial for navigating the evolving AI landscape and harnessing the power of AGI to drive innovation, growth, and competitive advantage in your organization.\nExecutive summary:\n\n\nAI experts have made remarkable advancements in creating large language models (LLMs) that demonstrate extraordinary skill in various fields and tasks. One of the latest breakthroughs is OpenAI’s GPT-4 model, which was developed using unparalleled computational power and data.\n\n\nMicrosoft researchers had the opportunity to explore an early version of GPT-4 while OpenAI was still developing it. This early version represents a new generation of LLMs that exhibit general intelligence that surpasses previous AI models.\n\n\nWhat sets GPT-4 apart is its versatility. Beyond being highly adept at language-based tasks, the model has proven capable of tackling complex and new challenges in various areas, including mathematics, coding, visual recognition, healthcare, legal matters, psychology, and more. The model requires no special instructions to perform these tasks.\n\n\nGPT-4’s performance is not only impressively close to human capabilities but also far exceeds the performance of earlier models, such as ChatGPT. As a result, researchers believe that GPT-4 could be seen as an early, albeit incomplete, version of an artificial general intelligence (AGI) system—a type of AI that can understand, learn, and apply knowledge across diverse domains.\n\n\nDespite its strengths, GPT-4 does have limitations. Current evaluation metrics struggle to accurately capture the semantic similarities in statements, leading to situations where GPT-4’s responses are deemed incorrect by metrics like ROUGE, despite containing relevant information. Furthermore, the model faces challenges regarding text generation tasks that require planning, such as creating rhymes or using specific words in each sentence.\n\n\nThe research community is actively exploring future directions for developing AGI systems, and they are considering moving beyond the traditional next-word prediction approach used in current LLMs.\n\n\nThe Rise of GPT-4\nIn recent years, the development of large language models (LLMs) has made significant strides in artificial intelligence research. OpenAI’s GPT-4 is the latest model, trained on an unprecedented scale of computing and data, pushing the boundaries of AI capabilities. GPT-4 has demonstrated remarkable performance across various domains and tasks, such as mathematics, coding, vision, medicine, law, and psychology.\nUnlike its predecessors, GPT-4 can solve novel and difficult tasks without special prompting. In many instances, GPT-4’s performance approaches or even surpasses human-level expertise, outperforming earlier models like those used by ChatGPT. This broad range of capabilities positions GPT-4 as a groundbreaking model that could potentially reshape industries and create new business opportunities.\nOpportunities for Businesses\nAs GPT-4 ushers in new capabilities, businesses across various sectors stand to benefit. This section will explore the potential opportunities GPT-4 presents for businesses, enabling them to drive growth, innovation, and competitive advantage.\n\n\nHealthcare\n\n\nEnhancing diagnostic accuracy and speed\n\n\nStreamlining patient care through intelligent patient management systems\n\n\nAssisting in medical research and drug discovery\n\n\n\n\nEducation\n\n\nPersonalizing learning experiences through AI-driven tutoring systems\n\n\nSimplifying curriculum development and assessment\n\n\nProviding educators with insights for targeted interventions and support\n\n\n\n\nEngineering\n\n\nAutomating complex design processes\n\n\nOptimizing resource allocation and project management\n\n\nAssisting in problem-solving and research for cutting-edge engineering solutions\n\n\n\n\nFinance\n\n\nImproving risk assessment and fraud detection\n\n\nStreamlining financial analysis and decision-making\n\n\nEnhancing customer service through intelligent chatbots and virtual assistants\n\n\n\n\nLegal\n\n\nAutomating contract review and analysis\n\n\nAssisting in legal research and case preparation\n\n\nFacilitating dispute resolution and mediation\n\n\n\n\nMarketing and Customer Service\n\n\nPersonalizing customer interactions through AI-driven targeting and messaging\n\n\nAnalyzing consumer behavior and trends for strategic decision-making\n\n\nStreamlining customer support with intelligent chatbots and virtual assistants\n\n\n\n\nBy embracing these opportunities, businesses can stay ahead of the curve and adapt to the rapidly evolving AI landscape.\nChallenges and Risks\nAs promising as GPT-4 and AGI technologies may seem, it is essential for business leaders to be aware of the challenges and risks they present. In this section, we will explore some of the limitations of GPT-4 and the potential societal implications that may arise.\n\n\nLimitations of GPT-4\n\n\nPlanning abilities: GPT-4 struggles with tasks that require planning ahead or generating content under specific constraints, such as producing rhymes or using prescribed words.\n\n\nUp-to-date knowledge: GPT-4’s knowledge is limited to the data it was last trained on, which means it cannot provide information on events or advancements that occurred after its training cycle.\n\n\nTrustworthiness: GPT-4 is known to hallucinate, make up facts, or produce inconsistent content, making it challenging to establish trust or collaboration with users.\n\n\n\n\nSocietal Implications\n\n\nAI divide: The limited availability of advanced AI systems like GPT-4 could lead to an “AI divide” between those with access to these technologies and those without, amplifying existing societal inequalities.\n\n\nPrivacy concerns: As AI systems become more capable, there is an increasing need for new levels of confidentiality and privacy to protect personal and organizationally sensitive information.\n\n\n\n\nTo harness the full potential of GPT-4 and AGI technologies, business leaders must be aware of these challenges and risks and actively work to mitigate them. By understanding the limitations and addressing the societal implications, businesses can contribute to AGI’s responsible development and deployment, benefiting society as a whole.\nPreparing for the Future of AGI in Business\nAs AGI continues to evolve, business leaders must adapt their strategies and operations to harness the benefits of this groundbreaking technology. Here are some key considerations to help you prepare for the future of AGI in business:\n\n\nRethink business strategies and operations:\n\n\nAssess how AGI could disrupt your industry and identify areas where it can provide a competitive advantage.\n\n\nExplore opportunities for integrating AGI into your current business processes to drive efficiency, innovation, and growth.\n\n\n\n\nInvest in AI talent and resources:\n\n\nEnsure your organization has access to skilled AI professionals who can help develop and implement AGI solutions.\n\n\nAllocate resources for AI research and development, staying abreast of the latest advancements and trends in the field.\n\n\n\n\nAddress the “AI divide”:\n\n\nCollaborate with stakeholders to develop strategies for equitable access to AGI technology, reducing the gap between those with access to powerful AI systems and those without access.\n\n\nAdvocate for policies and initiatives that promote inclusivity and equal opportunities, ensuring the benefits of AGI are shared broadly across society.\n\n\n\n\nBuild a culture of adaptability and lifelong learning:\n\n\nEncourage employees to continuously develop their skills and knowledge in the face of rapid technological advancements.\n\n\nImplement training programs and resources to help your workforce stay up-to-date with the latest developments in AI and AGI.\n\n\n\n\nBy proactively addressing these considerations, business leaders can successfully navigate AGI’s challenges and opportunities, positioning their organizations to thrive in the rapidly evolving AI landscape.\nEmbracing the Future: Harnessing AGI for Business Success\nAs we delve deeper into the era of AGI, the potential of GPT-4 and its successors will continue to shape the business landscape. Business leaders must remain vigilant and informed about the advancements and limitations of these powerful AI models. By embracing the opportunities presented by GPT-4 and future AGI systems, businesses can drive innovation and growth while addressing the challenges such as the “AI divide” and privacy concerns. As we navigate the complex trade-offs between sharing state-of-the-art AI capabilities and mitigating risks, we must prioritize equitable access and create value for all stakeholders.\nWith my expertise in AI strategy consulting, I am here to guide you through this transformative journey. Together, we can:\n\n\nDevelop a strategic AI roadmap tailored to your business goals\n\n\nIdentify high-impact AI use cases and applications\n\n\nAddress challenges and mitigate risks in AI adoption\n\n\nEnsure ethical and equitable AI practices for all stakeholders\n\n\nDon’t miss out on the competitive edge that AI has to offer. Contact me today and take the first step towards harnessing the power of AGI for the success of your business!"},"mastering-chatgpt-plugins-plugin-prompt-space":{"title":"Mastering ChatGPT Plugins: Optimizing Prompt Space in the Dynamic Plugin Prompt","links":[],"tags":["AI-Your-Business","ChatGPT","ChatGPT-Plugins","Prompt-Engineering"],"content":"\nOptimizing the Plugin Prompt—the generated interface that explains your plugin to ChatGPT—can significantly enhance your ChatGPT plugin’s performance. In this blog post, we’ll dive into how ChatGPT Plugins compile your OpenAPI specification and ai-plugin.json files into a Plugin Prompt. We’ll also explore strategies for optimizing the available space in the prompt, resulting in more accurate and contextually relevant plugin interactions.\nThe Significance of the Plugin Prompt\nThe Plugin Prompt is a dynamically generated Typescript pseudo-code that serves as a vital link between ChatGPT and your plugin. Its construction relies primarily on the description_for_model property in the ai-plugin.json file and the information contained in your OpenAPI specification.\nThe Plugin Prompt plays an essential role in determining how ChatGPT interprets and interacts with your plugin. By fine-tuning the Plugin Prompt, you can significantly improve the accuracy and context-awareness of your plugin’s responses, leading to an enhanced user experience.\nCapitalizing on the Prompt Space\nWhile the DESCRIPTION_FOR_MODEL property offers the most space with 8,000 characters, it’s crucial not to underestimate the importance of other properties from the OpenAPI spec in the Plugin Prompt.\nEach API endpoint defined in the OpenAPI spec includes two description properties that are also incorporated into the Plugin Prompt. Since each description can be up to 200–300 characters (200 according to the docs, 300 in my tests), that means an API with ten endpoints could gain an additional 4,000–6,000 characters of prompt space with efficient use of the properties in the OpenAPI spec.\nNotable OpenAPI Properties in the Plugin Prompt\nPath/Type Descriptions\nThe OpenAPI Path Description, if available, appears immediately before the type declaration in the dynamically generated pseudo-code namespace of the Plugin Prompt. If the Path Description is unavailable, the OpenAPI Summary is used instead. This ensures that the prompt has relevant information about the specific API endpoint.\nComponent/Property Descriptions\nEach pseudo-code type contains properties generated from OpenAPI component property descriptions. It’s important to note that only descriptions specific to request properties are included. This approach is advantageous because it provides more prompt space per component, as each description can be another 300-character prompt. Schema-level descriptions and properties of response objects are not included in the Plugin Prompt.\nDecoding the Plugin Prompt\nTo gain a better understanding of the Plugin Prompt output, let’s examine an example using a simple OpenAPI spec and an ai-plugin.json file. The main experiment below tests all common properties in the OpenAPI spec to determine which properties are included in the Plugin Prompt.\nExample Plugin Prompt\n// DESCRIPTION_FOR_MODEL\nnamespace NAME_FOR_MODEL {\n\n// PATH_GET_DESCRIPTION\ntype getPath = (_: {\n// PATH_GET_PARAMETER_DESCRIPTION\nPARAMETER_NAME: string,\n}) =&gt; any;\n\n// PATH_POST_DESCRIPTION\ntype postPath = (_: {\n// COMPONENT_SCHEMAS_REQUEST_SCHEMA_DATA_DESCRIPTION\ndata: string, // default: COMPONENT_SCHEMAS_REQUEST_SCHEMA_DATA_DEFAULT\n}) =&gt; any;\n\n} // namespace NAME_FOR_MODEL\n\nLet’s look at an OpenAPI specification and ai-plugin.json file to see how these files are converted into the Plugin Prompt above. Property values are in ALL CAPS for easy mapping between the source files and the Plugin Prompt.\nai-plugin.json\n{  \n&quot;schema_version&quot;: &quot;v1&quot;,  \n&quot;name_for_model&quot;: &quot;NAME_FOR_MODEL&quot;,  \n&quot;description_for_model&quot;: &quot;DESCRIPTION_FOR_MODEL&quot;,  \n…  \n}\n\nOpenAPI spec\nopenapi: 3.0.0\ninfo:\n  title: EXAMPLE_API_TITLE\n  description: EXAMPLE_API_DESCRIPTION\n  version: 1.0.0\nservers:\n  - url: http://localhost:5003\n    variables:\n      apiDomain:\n        default: example.com\n        description: EXAMPLE_SERVER_VARIABLE_API_DOMAIN_DESCRIPTION\npaths:\n  /path:\n    get:\n      tags:\n        - PATH_GET_TAG\n      summary: PATH_GET_SUMMARY\n      description: PATH_GET_DESCRIPTION\n      operationId: getPath\n      parameters:\n        - name: PARAMETER_NAME\n          in: query\n          description: PATH_GET_PARAMETER_DESCRIPTION\n          required: true\n          schema:\n            type: string\n      responses:\n        200:\n          description: PATH_GET_RESPONSE_200_DESCRIPTION\n          content:\n            application/json:\n              schema:\n                $ref: &quot;#/components/schemas/ResponseSchema&quot;\n        400:\n          description: PATH_GET_RESPONSE_400_DESCRIPTION\n          content:\n            application/json:\n              schema:\n                $ref: &quot;#/components/schemas/ErrorSchema&quot;\n        default:\n          description: PATH_GET_RESPONSE_DEFAULT_DESCRIPTION\n          content:\n            application/json:\n              schema:\n                $ref: &quot;#/components/schemas/ErrorSchema&quot;\n    post:\n      tags:\n        - PATH_POST_TAG\n      summary: PATH_POST_SUMMARY\n      description: PATH_POST_DESCRIPTION\n      operationId: postPath\n      requestBody:\n        description: PATH_POST_REQUEST_BODY_DESCRIPTION\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: &quot;#/components/schemas/RequestSchema&quot;\n      responses:\n        200:\n          description: PATH_POST_RESPONSE_200_DESCRIPTION\n          content:\n            application/json:\n              schema:\n                $ref: &quot;#/components/schemas/ResponseSchema&quot;\n        400:\n          description: PATH_POST_RESPONSE_400_DESCRIPTION\n          content:\n            application/json:\n              schema:\n                $ref: &quot;#/components/schemas/ErrorSchema&quot;\n        default:\n          description: PATH_POST_RESPONSE_DEFAULT_DESCRIPTION\n          content:\n            application/json:\n              schema:\n                $ref: &quot;#/components/schemas/ErrorSchema&quot;\ncomponents:\n  schemas:\n    RequestSchema:\n      type: object\n      properties:\n        data:\n          type: string\n          description: COMPONENT_SCHEMAS_REQUEST_SCHEMA_DATA_DESCRIPTION\n          default: COMPONENT_SCHEMAS_REQUEST_SCHEMA_DATA_DEFAULT\n      description: COMPONENT_SCHEMAS_REQUEST_SCHEMA_DESCRIPTION\n      required:\n        - data\n    ResponseSchema:\n      type: object\n      properties:\n        data:\n          type: array\n          items:\n            $ref: &quot;#/components/schemas/DataSchema&quot;\n      description: COMPONENT_SCHEMAS_RESPONSE_SCHEMA_DESCRIPTION\n    DataSchema:\n      type: object\n      properties:\n        id:\n          type: string\n          description: COMPONENT_SCHEMAS_DATA_SCHEMA_ID_DESCRIPTION\n        name:\n          type: string\n          description: COMPONENT_SCHEMAS_DATA_SCHEMA_NAME_DESCRIPTION\n          default: COMPONENT_SCHEMAS_DATA_SCHEMA_NAME_DEFAULT\n      description: COMPONENT_SCHEMAS_DATA_SCHEMA_DESCRIPTION\n      required:\n        - id\n        - name\n    ErrorSchema:\n      type: object\n      properties:\n        message:\n          type: string\n          description: COMPONENT_SCHEMAS_ERROR_SCHEMA_MESSAGE_DESCRIPTION\n        code:\n          type: integer\n          format: int32\n          description: COMPONENT_SCHEMAS_ERROR_SCHEMA_CODE_DESCRIPTION\n      description: COMPONENT_SCHEMAS_ERROR_SCHEMA_DESCRIPTION\n      required:\n        - message\n        - code\n\nDescription vs. Summary Experiment\nIn cases where the Path Description is not provided, the OpenAPI Summary is used instead. The following example demonstrates this scenario:\nPlugin Prompt\n// DESCRIPTION_FOR_MODEL\nnamespace NAME_FOR_MODEL {\n\n// PATH_GET_SUMMARY &lt;---- [**THE CHANGE: REPLACED DESCRIPTION WITH SUMMARY**]\ntype getPath = (_: {\n// PATH_GET_PARAMETER_DESCRIPTION\nPARAMETER_NAME: string,\n}) =&gt; any;\n\n// PATH_POST_DESCRIPTION\ntype postPath = (_: {\n// COMPONENT_SCHEMAS_REQUEST_SCHEMA_DATA_DESCRIPTION\ndata: string, // default: COMPONENT_SCHEMAS_REQUEST_SCHEMA_DATA_DEFAULT\n}) =&gt; any;\n\n} // namespace NAME_FOR_MODEL\n\nOpenAPI spec\nopenapi: 3.0.0\ninfo:\n  title: EXAMPLE_API_TITLE\n  description: EXAMPLE_API_DESCRIPTION\n  version: 1.0.0\nservers:\n  - url: http://localhost:5003\n    variables:\n      apiDomain:\n        default: example.com\n        description: EXAMPLE_SERVER_VARIABLE_API_DOMAIN_DESCRIPTION\npaths:\n  /path:\n    get:\n      tags:\n        - PATH_GET_TAG\n      summary: PATH_GET_SUMMARY\n      # description: PATH_GET_DESCRIPTION  &lt;---- [**THE CHANGE: REMOVED DESCRIPTION**]\n      operationId: getPath\n      parameters:\n        - name: PARAMETER_NAME\n          in: query\n          description: PATH_GET_PARAMETER_DESCRIPTION\n\nWrapping Up\nOptimizing the Plugin Prompt is crucial for enhancing the performance and user experience of your ChatGPT plugin. You can create more precise and contextually relevant plugin interactions by understanding what is included in the Plugin Prompt and how to use the available space effectively. As you develop your ChatGPT plugin, remember that the OpenAPI spec should be considered prompt engineering space to build the best possible user experience for your ChatGPT plugin."},"mastering-chatgpts-code-interpreter-list-of-python-packages":{"title":"Mastering ChatGPT's Code Interpreter: Comprehensive List of Python Packages in ChatGPT's Code Interpreter","links":[],"tags":["AI-Your-Business","ChatGPT","ChatGPT-Plugins","GPT-3","GPT-4","LLM","buildinpublic"],"content":"\nExplore the Python packages driving OpenAI’s ChatGPT Code Interpreter.\nCode Interpreter &amp; Python Packages\nChatGPT’s Code Interpreter executes Python code, powered by a multitude of Python packages.\nPython, with its vast array of packages, facilitates a diverse range of projects. Here’s a glimpse into the possibilities:\n\nData Analysis and Visualization: Analyze and visualize your data offline using packages like pandas, numpy, scipy, xarray, matplotlib, seaborn, plotly, and bokeh.\nNatural Language Processing and Machine Learning: Dive into text data with nltk, spacy, textblob, and gensim, or build machine learning models with scikit-learn, xgboost, keras, and torch.\nImage and Audio Processing: Manipulate and analyze images and audio with pillow, imageio, opencv-python, scikit-image, librosa, pyaudio, and soundfile.\nFile Format Manipulation and Web Development: Handle various file formats with openpyxl, xlrd, pyPDF2, python-docx, or build web applications with flask, django, tornado, and quart.\n\nList of Python Packages\nHere are the Python packages available in ChatGPT’s Code Interpreter:\nData Analysis and Manipulation:\n\n\npandas\n\n\nnumpy\n\n\nscipy\n\n\nxarray\n\n\nMachine Learning:\n\n\nscikit-learn\n\n\nxgboost\n\n\nkeras\n\n\ntorch\n\n\nNatural Language Processing:\n\n\nnltk\n\n\nspacy\n\n\ntextblob\n\n\ngensim\n\n\nData Visualization:\n\n\nmatplotlib\n\n\nseaborn\n\n\nplotly\n\n\nbokeh\n\n\nWeb Scraping and Network:\n\n\nrequests\n\n\nurllib3\n\n\naiohttp\n\n\nbeautifulsoup4\n\n\nDeep Learning:\n\n\nkeras\n\n\ntorch\n\n\ntheano\n\n\nImage Processing:\n\n\npillow\n\n\nimageio\n\n\nopencv-python\n\n\nscikit-image\n\n\nAudio Processing:\n\n\nlibrosa\n\n\npyaudio\n\n\nsoundfile\n\n\nFile Format:\n\n\nopenpyxl\n\n\nxlrd\n\n\npyPDF2\n\n\npython-docx\n\n\nDatabase:\n\n\nsqlalchemy\n\n\npsycopg2\n\n\nmysql-connector-python\n\n\nWeb Development:\n\n\nflask\n\n\ndjango\n\n\ntornado\n\n\nquart\n\n\nOthers:\n\n\npytest\n\n\njoblib\n\n\npytz\n\n\npyyaml\n\n\nAll:\n[&#039;absl-py==1.4.0&#039;,\n &#039;affine==2.4.0&#039;,\n &#039;aiofiles==23.1.0&#039;,\n &#039;aiohttp==3.8.1&#039;,\n &#039;aiosignal==1.3.1&#039;,\n &#039;analytics-python==1.4.post1&#039;,\n &#039;anyio==3.6.2&#039;,\n &#039;anytree==2.8.0&#039;,\n &#039;argcomplete==1.10.3&#039;,\n &#039;argon2-cffi-bindings==21.2.0&#039;,\n &#039;argon2-cffi==21.3.0&#039;,\n &#039;arviz==0.15.1&#039;,\n &#039;asttokens==2.2.1&#039;,\n &#039;async-timeout==4.0.2&#039;,\n &#039;attrs==23.1.0&#039;,\n &#039;audioread==3.0.0&#039;,\n &#039;babel==2.12.1&#039;,\n &#039;backcall==0.2.0&#039;,\n &#039;backoff==1.10.0&#039;,\n &#039;backports.zoneinfo==0.2.1&#039;,\n &#039;basemap-data==1.3.2&#039;,\n &#039;basemap==1.3.2&#039;,\n &#039;bcrypt==4.0.1&#039;,\n &#039;beautifulsoup4==4.12.2&#039;,\n &#039;bleach==6.0.0&#039;,\n &#039;blinker==1.6.2&#039;,\n &#039;blis==0.7.9&#039;,\n &#039;bokeh==2.4.0&#039;,\n &#039;branca==0.6.0&#039;,\n &#039;brotli==1.0.9&#039;,\n &#039;cachetools==5.3.0&#039;,\n &#039;cairocffi==1.5.1&#039;,\n &#039;cairosvg==2.5.2&#039;,\n &#039;camelot-py==0.10.1&#039;,\n &#039;catalogue==2.0.8&#039;,\n &#039;certifi==2019.11.28&#039;,\n &#039;cffi==1.15.1&#039;,\n &#039;chardet==4.0.0&#039;,\n &#039;charset-normalizer==2.1.1&#039;,\n &#039;click-plugins==1.1.1&#039;,\n &#039;click==8.1.3&#039;,\n &#039;cligj==0.7.2&#039;,\n &#039;cloudpickle==2.2.1&#039;,\n &#039;cmudict==1.0.13&#039;,\n &#039;comm==0.1.3&#039;,\n &#039;compressed-rtf==1.0.6&#039;,\n &#039;countryinfo==0.1.2&#039;,\n &#039;cryptography==3.4.8&#039;,\n &#039;cssselect2==0.7.0&#039;,\n &#039;cycler==0.11.0&#039;,\n &#039;cymem==2.0.7&#039;,\n &#039;dbus-python==1.2.16&#039;,\n &#039;debugpy==1.6.7&#039;,\n &#039;decorator==4.4.2&#039;,\n &#039;defusedxml==0.7.1&#039;,\n &#039;deprecat==2.1.1&#039;,\n &#039;dill==0.3.6&#039;,\n &#039;distro-info==0.23ubuntu1&#039;,\n &#039;dlib==19.22.1&#039;,\n &#039;docx2txt==0.8&#039;,\n &#039;ebcdic==1.1.1&#039;,\n &#039;ebooklib==0.18&#039;,\n &#039;einops==0.3.2&#039;,\n &#039;entrypoints==0.4&#039;,\n &#039;et-xmlfile==1.1.0&#039;,\n &#039;exchange-calendars==3.4&#039;,\n &#039;executing==1.2.0&#039;,\n &#039;extract-msg==0.28.7&#039;,\n &#039;faker==8.13.2&#039;,\n &#039;fastjsonschema==2.16.3&#039;,\n &#039;fastprogress==1.0.3&#039;,\n &#039;ffmpeg-python==0.2.0&#039;,\n &#039;ffmpy==0.3.0&#039;,\n &#039;filelock==3.12.0&#039;,\n &#039;fiona==1.8.20&#039;,\n &#039;flask-cachebuster==1.0.0&#039;,\n &#039;flask-cors==3.0.10&#039;,\n &#039;flask-login==0.6.2&#039;,\n &#039;flask==2.3.2&#039;,\n &#039;folium==0.12.1&#039;,\n &#039;fonttools==4.39.3&#039;,\n &#039;fpdf==1.7.2&#039;,\n &#039;frozenlist==1.3.3&#039;,\n &#039;future==0.18.3&#039;,\n &#039;fuzzywuzzy==0.18.0&#039;,\n &#039;gensim==4.1.0&#039;,\n &#039;geographiclib==1.52&#039;,\n &#039;geopandas==0.10.2&#039;,\n &#039;geopy==2.2.0&#039;,\n &#039;gradio==2.2.15&#039;,\n &#039;graphviz==0.17&#039;,\n &#039;gtts==2.2.3&#039;,\n &#039;h11==0.14.0&#039;,\n &#039;h2==4.1.0&#039;,\n &#039;h5netcdf==1.1.0&#039;,\n &#039;h5py==3.4.0&#039;,\n &#039;hpack==4.0.0&#039;,\n &#039;html5lib==1.1&#039;,\n &#039;hypercorn==0.14.3&#039;,\n &#039;hyperframe==6.0.1&#039;,\n &#039;idna==2.8&#039;,\n &#039;imageio-ffmpeg==0.4.8&#039;,\n &#039;imageio==2.28.1&#039;,\n &#039;imapclient==2.1.0&#039;,\n &#039;imgkit==1.2.2&#039;,\n &#039;importlib-metadata==6.6.0&#039;,\n &#039;importlib-resources==5.12.0&#039;,\n &#039;iniconfig==2.0.0&#039;,\n &#039;ipykernel==6.22.0&#039;,\n &#039;ipython-genutils==0.2.0&#039;,\n &#039;ipython==8.12.1&#039;,\n &#039;isodate==0.6.1&#039;,\n &#039;itsdangerous==2.1.2&#039;,\n &#039;jax==0.2.28&#039;,\n &#039;jedi==0.18.2&#039;,\n &#039;jinja2==3.1.2&#039;,\n &#039;joblib==1.2.0&#039;,\n &#039;json5==0.9.11&#039;,\n &#039;jsonpickle==3.0.1&#039;,\n &#039;jsonschema==4.17.3&#039;,\n &#039;jupyter-client==7.4.9&#039;,\n &#039;jupyter-core==5.1.3&#039;,\n &#039;jupyter-server==1.23.5&#039;,\n &#039;jupyterlab-pygments==0.2.2&#039;,\n &#039;jupyterlab-server==2.19.0&#039;,\n &#039;jupyterlab==3.4.8&#039;,\n &#039;keras==2.6.0&#039;,\n &#039;kerykeion==2.1.16&#039;,\n &#039;kiwisolver==1.4.4&#039;,\n &#039;korean-lunar-calendar==0.3.1&#039;,\n &#039;librosa==0.8.1&#039;,\n &#039;llvmlite==0.40.0&#039;,\n &#039;loguru==0.5.3&#039;,\n &#039;lxml==4.9.2&#039;,\n &#039;markdown2==2.4.8&#039;,\n &#039;markdownify==0.9.3&#039;,\n &#039;markupsafe==2.1.2&#039;,\n &#039;matplotlib-inline==0.1.6&#039;,\n &#039;matplotlib-venn==0.11.6&#039;,\n &#039;matplotlib==3.4.3&#039;,\n &#039;mistune==2.0.5&#039;,\n &#039;mizani==0.9.0&#039;,\n &#039;mne==0.23.4&#039;,\n &#039;monotonic==1.6&#039;,\n &#039;moviepy==1.0.3&#039;,\n &#039;mpmath==1.3.0&#039;,\n &#039;mtcnn==0.1.1&#039;,\n &#039;multidict==6.0.4&#039;,\n &#039;munch==2.5.0&#039;,\n &#039;murmurhash==1.0.9&#039;,\n &#039;mutagen==1.45.1&#039;,\n &#039;nashpy==0.0.35&#039;,\n &#039;nbclassic==0.5.6&#039;,\n &#039;nbclient==0.7.4&#039;,\n &#039;nbconvert==7.3.1&#039;,\n &#039;nbformat==5.8.0&#039;,\n &#039;nest-asyncio==1.5.6&#039;,\n &#039;networkx==2.6.3&#039;,\n &#039;nltk==3.6.3&#039;,\n &#039;notebook-shim==0.2.3&#039;,\n &#039;notebook==6.5.1&#039;,\n &#039;numba==0.57.0&#039;,\n &#039;numexpr==2.8.4&#039;,\n &#039;numpy-financial==1.0.0&#039;,\n &#039;numpy==1.21.2&#039;,\n &#039;odfpy==1.4.1&#039;,\n &#039;olefile==0.46&#039;,\n &#039;opencv-python==4.5.2.54&#039;,\n &#039;openpyxl==3.0.10&#039;,\n &#039;opt-einsum==3.3.0&#039;,\n &#039;packaging==23.1&#039;,\n &#039;pandas==1.3.2&#039;,\n &#039;pandocfilters==1.5.0&#039;,\n &#039;paramiko==3.1.0&#039;,\n &#039;parso==0.8.3&#039;,\n &#039;pathy==0.10.1&#039;,\n &#039;patsy==0.5.3&#039;,\n &#039;pdf2image==1.16.3&#039;,\n &#039;pdfkit==0.6.1&#039;,\n &#039;pdfminer.six==20200517&#039;,\n &#039;pdfplumber==0.5.28&#039;,\n &#039;pdfrw==0.4&#039;,\n &#039;pexpect==4.8.0&#039;,\n &#039;pickleshare==0.7.5&#039;,\n &#039;pillow==8.3.2&#039;,\n &#039;pip==20.0.2&#039;,\n &#039;pkgutil-resolve-name==1.3.10&#039;,\n &#039;platformdirs==3.5.0&#039;,\n &#039;plotly==5.3.0&#039;,\n &#039;plotnine==0.10.1&#039;,\n &#039;pluggy==1.0.0&#039;,\n &#039;pooch==1.7.0&#039;,\n &#039;preshed==3.0.8&#039;,\n &#039;priority==2.0.0&#039;,\n &#039;proglog==0.1.10&#039;,\n &#039;prometheus-client==0.16.0&#039;,\n &#039;prompt-toolkit==3.0.38&#039;,\n &#039;pronouncing==0.2.0&#039;,\n &#039;psutil==5.9.5&#039;,\n &#039;ptyprocess==0.7.0&#039;,\n &#039;pure-eval==0.2.2&#039;,\n &#039;py==1.11.0&#039;,\n &#039;pyaudio==0.2.11&#039;,\n &#039;pycountry==20.7.3&#039;,\n &#039;pycparser==2.21&#039;,\n &#039;pycryptodome==3.17&#039;,\n &#039;pydantic==1.8.2&#039;,\n &#039;pydot==1.4.2&#039;,\n &#039;pydub==0.25.1&#039;,\n &#039;pydyf==0.6.0&#039;,\n &#039;pygments==2.15.1&#039;,\n &#039;pygobject==3.36.0&#039;,\n &#039;pygraphviz==1.7&#039;,\n &#039;pylog==1.1&#039;,\n &#039;pyluach==2.2.0&#039;,\n &#039;pymc3==3.11.5&#039;,\n &#039;pymupdf==1.19.6&#039;,\n &#039;pynacl==1.5.0&#039;,\n &#039;pypandoc==1.6.3&#039;,\n &#039;pyparsing==3.0.9&#039;,\n &#039;pypdf2==1.28.6&#039;,\n &#039;pyphen==0.14.0&#039;,\n &#039;pyproj==3.5.0&#039;,\n &#039;pyprover==0.5.6&#039;,\n &#039;pyrsistent==0.19.3&#039;,\n &#039;pyshp==2.1.3&#039;,\n &#039;pyswisseph==2.10.3.1&#039;,\n &#039;pytesseract==0.3.8&#039;,\n &#039;pytest==6.2.5&#039;,\n &#039;pyth3==0.7&#039;,\n &#039;python-apt==2.0.1+ubuntu0.20.4.1&#039;,\n &#039;python-dateutil==2.8.2&#039;,\n &#039;python-docx==0.8.11&#039;,\n &#039;python-pptx==0.6.21&#039;,\n &#039;pyttsx3==2.90&#039;,\n &#039;pytz-deprecation-shim==0.1.0.post0&#039;,\n &#039;pytz==2023.3&#039;,\n &#039;pywavelets==1.4.1&#039;,\n &#039;pyxlsb==1.0.8&#039;,\n &#039;pyyaml==6.0&#039;,\n &#039;pyzbar==0.1.8&#039;,\n &#039;pyzmq==25.0.2&#039;,\n &#039;qrcode==7.3&#039;,\n &#039;quart==0.17.0&#039;,\n &#039;rarfile==4.0&#039;,\n &#039;rasterio==1.2.10&#039;,\n &#039;rdflib==6.0.0&#039;,\n &#039;regex==2023.5.5&#039;,\n &#039;reportlab==3.6.1&#039;,\n &#039;requests-unixsocket==0.2.0&#039;,\n &#039;requests==2.29.0&#039;,\n &#039;resampy==0.4.2&#039;,\n &#039;scikit-image==0.18.3&#039;,\n &#039;scikit-learn==1.0&#039;,\n &#039;scipy==1.7.3&#039;,\n &#039;seaborn==0.11.2&#039;,\n &#039;semver==3.0.0&#039;,\n &#039;send2trash==1.8.2&#039;,\n &#039;sentencepiece==0.1.99&#039;,\n &#039;setuptools==45.2.0&#039;,\n &#039;shap==0.39.0&#039;,\n &#039;shapely==1.7.1&#039;,\n &#039;six==1.14.0&#039;,\n &#039;slicer==0.0.7&#039;,\n &#039;smart-open==6.3.0&#039;,\n &#039;sniffio==1.3.0&#039;,\n &#039;snuggs==1.4.7&#039;,\n &#039;sortedcontainers==2.4.0&#039;,\n &#039;soundfile==0.10.2&#039;,\n &#039;soupsieve==2.4.1&#039;,\n &#039;spacy-legacy==3.0.12&#039;,\n &#039;spacy==3.1.7&#039;,\n &#039;speechrecognition==3.8.1&#039;,\n &#039;srsly==2.4.6&#039;,\n &#039;stack-data==0.6.2&#039;,\n &#039;statsmodels==0.12.2&#039;,\n &#039;svglib==1.1.0&#039;,\n &#039;svgwrite==1.4.1&#039;,\n &#039;sympy==1.8&#039;,\n &#039;tables==3.6.1&#039;,\n &#039;tabula==1.0.5&#039;,\n &#039;tabulate==0.8.9&#039;,\n &#039;tenacity==8.2.2&#039;,\n &#039;terminado==0.17.1&#039;,\n &#039;text-unidecode==1.3&#039;,\n &#039;textblob==0.15.3&#039;,\n &#039;textract==1.6.4&#039;,\n &#039;theano-pymc==1.1.2&#039;,\n &#039;thinc==8.0.17&#039;,\n &#039;threadpoolctl==3.1.0&#039;,\n &#039;tifffile==2023.4.12&#039;,\n &#039;tinycss2==1.2.1&#039;,\n &#039;toml==0.10.2&#039;,\n &#039;tomli==2.0.1&#039;,\n &#039;toolz==0.12.0&#039;,\n &#039;torch==1.10.0&#039;,\n &#039;torchaudio==0.10.0&#039;,\n &#039;torchtext==0.6.0&#039;,\n &#039;torchvision==0.11.1&#039;,\n &#039;tornado==6.3.1&#039;,\n &#039;tqdm==4.64.0&#039;,\n &#039;traitlets==5.9.0&#039;,\n &#039;trimesh==3.9.29&#039;,\n &#039;typer==0.4.2&#039;,\n &#039;typing-extensions==4.5.0&#039;,\n &#039;tzdata==2023.3&#039;,\n &#039;tzlocal==4.3&#039;,\n &#039;unattended-upgrades==0.1&#039;,\n &#039;urllib3==1.25.8&#039;,\n &#039;wand==0.6.11&#039;,\n &#039;wasabi==0.10.1&#039;,\n &#039;wcwidth==0.2.6&#039;,\n &#039;weasyprint==53.3&#039;,\n &#039;webencodings==0.5.1&#039;,\n &#039;websocket-client==1.5.1&#039;,\n &#039;websockets==10.3&#039;,\n &#039;werkzeug==2.3.3&#039;,\n &#039;wheel==0.34.2&#039;,\n &#039;wordcloud==1.8.1&#039;,\n &#039;wrapt==1.15.0&#039;,\n &#039;wsproto==1.2.0&#039;,\n &#039;xarray-einstats==0.5.1&#039;,\n &#039;xarray==2023.1.0&#039;,\n &#039;xgboost==1.4.2&#039;,\n &#039;xlrd==1.2.0&#039;,\n &#039;xlsxwriter==3.1.0&#039;,\n &#039;xml-python==0.4.3&#039;,\n &#039;yarl==1.9.2&#039;,\n &#039;zipp==3.15.0&#039;,\n &#039;zopfli==0.2.2&#039;]\n"},"maximize-productivity-with-chatgpt-template-driven-conversations":{"title":"Maximize Productivity with ChatGPT & Template-driven Conversations","links":[],"tags":["AI","AI-Your-Business","ChatGPT","GPT-3","Productivity","Prompt-Engineering","Prompt-Writing"],"content":"\nMaking the Most of ChatGPT with Templates\nIs your workday feeling overwhelming? Are you looking for ways to help streamline and optimize the way your team works? ChatGPT can make all this achievable - and it’s easy! All you need are templates.\nTemplates are pre-defined text blocks that allow you to quickly construct messages or tasks with predetermined parameters when working with ChatGPT. Think of them like shortcuts: enter in what information is needed, and out comes a fully customized message, ready to go!\nBut how do they make life easier? First, you can save time when working on assignments or drafting messages by leveraging templates. Instead of taking five, ten, fifteen minutes, or even hours to complete just one assignment or draft, with templates, you might be able to complete dozens in an hour. Plus, by already having established structures for defining these complexities within each template, there’s no more worrying about forgetting key details from one communication piece to the next; everything remains consistent!\nBenefits of Using Templates with ChatGPT\nIf you’re looking for a way to save time, stay consistent, collaborate with others more efficiently, or reduce stress levels, then using templates with ChatGPT is a great option!\nThe combination of templates with ChatGPT in the workplace can provide numerous benefits, such as:\n\n\nTime savings: Templates will help you quickly complete routine tasks so that you can focus more of your time on more meaningful work.\n\n\nImproved consistency: With consistently formatted documents and outputs from ChatGPT’s templates, your work will maintain high quality - resulting in better outcomes!\n\n\nIncreased collaboration: Sharing and using the same templates amongst colleagues promotes teamwork and leads to achieving even better results.\n\n\nReduced stress levels: Relying on ChatGPT for mundane tasks gives us breathing room in our workloads - so we don’t feel too overwhelmed and stressed out.\n\n\nEnhanced efficiency: Streamlining processes via robust templates enables us to get things done quickly without compromising the results; it’s an effective strategy that helps meet goals faster!\n\n\nUltimately, embracing this technology as part of our daily workflow provides tremendous value in helping us become successful professionals. I know it has helped me tremendously when juggling all sorts of projects 😊\nGetting Started with Templates and ChatGPT\nIf you’re looking for ways to make your workflows more efficient, templates and ChatGPT can be a great combination. Here’s how you can start using them:\n\n\nIdentify Tasks that Need Templates: Think about which tasks your team performs repeatedly and create templates for each. Aim for making templates that save time when starting a new task and streamlining recurring tasks.\n\n\nSave Templates in a Shared Folder: Saving all your templates in one shared location, such as Google Drive or Dropbox, ensures everyone has access and promotes collaboration between employees so they can help out if needed.\n\n\nIntroduce Employees to Your Templates: Show staff where the templates are stored, when to use them, and how they should use them - this will also allow them to provide feedback which you could use to improve the templates!\n\n\nReview &amp; Refine Template Processes: As people start using the templates, keep an eye on areas needing refinements, and don’t hesitate to make changes accordingly.\n\n\nBest Practices: Creating Effective Templates for ChatGPT\nIf you use ChatGPT, following best practices is critical to building a library of templates. Here’s what I’ve found helpful:\n\n\nKeep it simple - When creating templates for colleagues or employees, keep things simple. For example, pre-fill as much of the template as possible and provide easy-to-understand instructions for completing the rest.\n\n\nBe consistent - Try to maintain consistent formatting and layouts in your templates, so people can quickly and easily find the instructions and places that need input.\n\n\nCustomize for different needs - Every task and workflow has different requirements; ensure each template includes prompts or placeholders to guide users. Try to organize information logically so anyone can utilize the template without any hassle or confusion!\n\n\nTest &amp; refine - Take some time out of your day (or assign a colleague) to test the templates with people who’ll use them regularly and get feedback on how they could be improved even further if needed!\n\n\nUsing these tips when creating your own ChatGPT templates gives you more opportunities to increase productivity while staying organized—I have experienced this firsthand! So give it a try today—you won’t regret it!\nOvercoming Challenges When Implementing Templates with ChatGPT\nUsing templates with ChatGPT will improve productivity, but you may still encounter challenges along the way. Here are some common challenges and how to overcome them:\n\n\nResistance from employees: When introducing something new, people can be hesitant to accept it - that was the case for me! To get everyone on board, I provide one-on-one support to help people understand new technology and see firsthand how it can be.\n\n\nLimited buy-in from leadership: Leaders must prioritize utilizing new technologies like AI so teams can feel inspired to do so too. It may take some extra effort, but clearly explain the benefits of using a template system and build up your case until you’ve gained buy-in!\n\n\nTemplates need more customization: Customizing templates according to tasks at hand will ensure they reach their full potential in terms of efficiency gains - this took me a while to learn the hard way! Before putting them into use properly, take time out for customization first. Are you creating a template to draft responses for customer inquiries? Specify the draft’s target audience by including a description of your typical customer.\n\n\nOutdated Templates: Workflows change over time, which means templates need updating, too, if they’re going to remain effective - having an established process for doing this helps ensure success with template integration down the line too!\n\n\nBeing aware of these challenges, you can address them and reap the most rewards from ChatGPT through template-driven conversations.\nConclusion\nAs a manager or owner involved in knowledge work, getting the most out of yourself and your team is the key to success. By employing templates with ChatGPT, you can maximize efficiency, lighten everyone’s load, and foster collaboration that will drive productivity up. Utilizing these suggestions today can have a significant impact on your performance right away. With the thoughtful implementation of these best practices and addressing common challenges as they arise, you can power through tasks faster while maintaining quality output at every step - ensuring success for yourself and those working around you!"},"maximize-your-ai-success-embrace-flexibility-and-agility":{"title":"Maximize Your AI Success: Embrace Flexibility and Agility","links":[],"tags":["AI","GPT-3","LLM"],"content":"\nAre you looking to implement AI in your product or service but need help figuring out where to start? For example, are you deciding whether to fine-tune, prompt engineer, or train a custom model?\nIt can be overwhelming with all the options and advancements in AI technology. However, maximizing flexibility and agility must be the focus of your AI product strategy if you want to succeed.\nWhat does it mean for your AI product to be flexible and agile? Quickly improving by integrating with better and cheaper models as companies like OpenAI release them.\nPersonal Experience with Flexible AI Systems\nIt’s easy to see the importance of flexibility and agility in AI product strategy from recent developments in the market space. For example, many of my past projects have included OpenAI tools like GPT-3 to enhance functionality using natural language generation.\nAt times, given the perceived value of proprietary data, we could have, and with good reason, decided to invest in training custom models. However, I’m glad we didn’t.\nWithin a year, companies like OpenAI released newer, better, and cheaper variations of GPT-3. But, because we had a flexible and agile approach, we could quickly try out the latest updates, test how they could improve the product, and quickly move to provide better customer results by incorporating the latest advancements.\nA Hypothetical Story of a Fragile AI System\nWhat if the above story didn’t happen that way? Imagine if we had instead decided to train custom models. Early on, it seemed plausible that we could develop a better solution than what was currently available through OpenAI. Moreover, the cost of talent and computing needed for training a custom model might have seemed acceptable for us to build what we perceived as a competitive advantage.\nIt was 2021, so we would have benchmarked our success against the earlier models released by OpenAI. At first, we might have even managed to produce a comparable custom model.\nThen 2022 would’ve rolled in. InstructGPT got released in January, and our custom model would have suddenly lost ground on our benchmarks. So we would have to invest more in updating our custom model, and a few months later, it might have seemed like a good idea when we started to catch up.\nBut, before we could celebrate, the text-davnici-002 model by OpenAI would have been released. So this time, we would’ve felt farther behind than we did with the release of InstructGPT. \n\nBy this point, we would have been deeply invested monetarily in Data Science talent, GPUs, and other decisions necessary to maintain the pride of “doing it” without relying on those cloud AI services. We would also have been fully committed to our path, including telling ourselves stories about why our competitors that rely on cloud providers, like OpenAI, are inferior.\nWithin the same year, text-davnici-003 and ChatGPT get released. In addition, rumors are that GPT-4 is just around the corner. And, after significant investment, our disillusionment that training our custom models is a sufficient differentiator starts to fade.\nComparison to Cloud Computing\nSome of us remember when companies started transitioning to cloud servers for web hosting. With companies like OpenAI, Hugging Face, and Anthropic, AI computing is already reaching that inflection point.\nSoon after the introduction of cloud computing, some professionals were still advising against using the cloud for various reasons. However, unfortunately, they likely prevented themselves and their clients from capitalizing on the coming wave of cloud solutions that supported an entire generation of new technology businesses.\nInstead of leveraging off-the-shelf AI, like the tools from OpenAI, what if you decide to compete with the cloud providers head-on by training custom models? The answer is the same as if you choose to host your servers instead of connecting to backbone architecture like that provided by AWS, Azure, and GCP.\nWhy Flexibility and Agility Matter\nAI technology is rapidly evolving and improving, and what’s state-of-the-art today may be outdated tomorrow. A flexible and agile AI strategy can maximize your ability to interchange off-the-shelf AI services as new, better, and cheaper options are released. \nMaximizing flexibility and agility in your AI system will save you time and resources in the long run while ensuring you always use the best AI technology available to deliver the best results to your customers.\nPractical Steps to Achieve Flexibility and Agility\nStart with a modular approach.\nDesigning your AI product or service should begin with modular, easily updated components. This means maximizing the utilization of pre-trained models instead of relying on training custom models. These components should make it easy to test and integrate new AI technologies as they become available.\nRegularly test new alternative components.\nIt’s easy to focus on what you’re doing right instead of what can be improved. However, it’s essential to experiment with similar models from different providers. This practice also ensures you’re flexible enough to ensure that you can change to better models in the future.\nStay up-to-date on necessary developments.\nStaying informed on the latest AI technologies is challenging and nearly impossible to maintain for the field of AI as a whole. So first, it’s necessary to have a deep understanding of the components that make up your particular AI system. Then you can focus on the advancements that impact your system directly.\nConclusion\nImplementing a flexible and agile AI product strategy is critical in today’s dynamic AI environment. Utilizing a modular approach, maintaining an open perspective, and staying informed will enable your AI product or service to provide the best customer outcomes.\nDo you need help designing a flexible AI system for your business? Schedule a consultation."},"one-question-at-a-time-using-the-stop-parameter-prompt-engineering-for-a-better-user-experience":{"title":"One Question at a Time: Using the Stop Parameter &amp; Prompt Engineering for a Better User Experience","links":[],"tags":["AI","AI-Your-Business","ChatGPT","Conversational-AI","GPT-3","LLM","Prompt-Engineering","Prompt-Writing"],"content":"\nImagine you’re building a chatbot using OpenAI’s GPT-3.5-turbo, and you need it to extract specific information from users, asking one question at a time.\nSeems simple, right?\nBut sometimes, crafting the perfect prompt to achieve this goal can be tricky.\nIn this case study, we’ll explore how one client tackled this challenge and achieved the desired outcome of asking one question at a time.\nThe Initial Attempt\nOur client started with a prompt to guide the AI in gathering the necessary information from users. The prompt looked like this:\nThe following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly. If the user wants they can talk about anything as soon as the assistant finishes collecting the following fields from the user:\n\nThe user&#039;s name\nThe user&#039;s departure location\nThe user&#039;s destination location\nThe user&#039;s dates of travel\nThe user&#039;s budget for the trip\nAny additional preferences or constraints the user may have\n\n\nHowever, the model sometimes responded with multiple questions at once, like this:\nSure, I can help you book a flight. Before we proceed, can you please provide me with the following information:\n- Your name?\n- Your departure location?\n- Your destination location?\n- Your dates of travel?\n- Your budget for the trip?\n- Any additional preferences or constraints you may have?\n\n\nThis response didn’t achieve the goal of asking one question at a time, so the client needed to find a different approach.\nTrying the Stop Parameter\nOne suggestion was to set the stop parameter to ?. This would make the model stop generating text when encountering a question mark. However, the client encountered some issues with this method, as the model’s output still lacked question marks and continued to ask multiple questions in a single response. For example:\nGreat! I&#039;d be happy to help you with that. Before we get started, can you please provide me with the following information:\n- Your name\n- Your departure locatio\n- Your destination location\n- Your dates of travel\n- Your budget for the trip\n- Any additional preferences or constraints you may have\n\nOnce I have all of this information, I can help you find the best flight options.\n\n\nReformulating the Prompt\nThe next problem occurred when the language model failed to ask questions. This committed to outputting any question mark, which prevented use of the stop parameter. Instead, it simply listed the series of requirements to the user.\nWe decided to reformat the original prompt to solve this, turning the list of required fields into questions. This made the model more likely to generate questions instead of regurgitating a list of requirements. The revised prompt looked like this:\nThe following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly. If the user wants they can talk about anything as soon as the assistant finishes collecting the following fields from the user:\n\nWhat is the user&#039;s name?\nWhat is the user&#039;s departure location?\nWhat is the user&#039;s destination location?\nWhat are the user&#039;s dates of travel?\nWhat is the user&#039;s budget for the trip?\nDoes the user have any additional preferences or constraints?\n\n\nThat solved it!\nBy reformatting the prompt, in combination with the stop parameter being set to ?, the chatbot started asking one question at a time.\nKey Takeaways\nThis case study highlights a few important lessons for anyone working with AI chatbots:\n\n\nExperiment with different techniques: Don’t be afraid to try different approaches when facing a challenge. In this case, our client tried the stop parameter and reformulated the prompt to achieve their desired outcome.\n\n\nRefine your prompts: The perfect prompt is key to making your AI chatbot work effectively. Be prepared to iterate and revise your prompts based on the responses you receive from the model.\n\n\nKeep the user experience in mind: Remember, the ultimate goal is to create a chatbot that provides a smooth and enjoyable experience for the user. By asking one question at a time, you can improve the user experience and collect the necessary information more efficiently.\n\n\nLet’s Tackle Your Chatbot Challenges Together\nSo there you have it! By exploring different techniques and refining the prompts, our client was able to create a more efficient and enjoyable chatbot experience. And remember, these lessons can be applied to any AI-powered project you’re working on.\nAs you embark on your AI journey, don’t hesitate to reach out for help or guidance. I’m here to share my experiences and offer support to ensure your AI project is a success.\nHappy experimenting, and good luck with your project!"},"personal-ai-guide-to-the-super-landscape":{"title":"Personal AI: Your Guide to the Super Landscape","links":[],"tags":["Future","Personal-AI","Prediction","Vision"],"content":"\n\nI am sitting in front of my computer, looking at a screen full of options and feeling overwhelmed. I feel like I am behind and don’t know where to start.\nEvery article or video I watch just tells me to do more. More research, more networking, more learning. But I don’t know what will actually help me move forward. I need someone to show me the way.\nThe present\n\nEducation enables progress.\nBut, enables education to progress?\nTechnology. The internet. AI.\nMore knowledge than any human can learn in a lifetime already exists in each of our pockets.\nWhat’s more? The same internet enables every connection we could possibly need to be successful. But people are failing to use this incredible resource in that way! We need help!\nAn assistant. An advisor. A mentor. A personal coach. Even the most capable people depend on them.\nWith all that’s available, it’s almost impossible to effectively narrow down your options without some advice. But from who?\nInfluencers are popping up in every space to help solve this. They curate content and share the knowledge they learned along their journey. But that isn’t enough.\nAs the influencer ecosystem grows, it will run into the same problem, a size that’s impossible to navigate.\nSo how will we overcome this problem of too many?\nPersonal AI.\nPersonal AI that understands you. It’s a personal computer that translates your life into a technological format. A model of your perspective, your vision of success, and your current progress.\nPersonal AI is your trusted advisor that understands what you need for clarity and motivation.\nPersonal AI is the only way to navigate this new “super landscape” where information is accessible beyond our ability to access it.\nYou: “What should I do today?”\nPersonal AI: “THIS will bring you THAT much closer to YOUR goals.”\n\nI wake up to my alarm and personal AI assistant. After a brief check-in, my AI helps me map out my day and what I need to do to stay on track with my goals.\nI eat a healthy breakfast and head to the gym for a quick workout. Afterward, I sit down at my desk and begin working on the task at hand. My AI chimes in periodically with helpful suggestions and resources that keep me focused and on track.\nThroughout the day, I take breaks to walk around outside or chat with friends. My AI is always there to help me stay productive and make the most of my time.\nThe future\n"},"privacy-policy":{"title":"Privacy Policy","links":[],"tags":[],"content":""},"publishing-from-obsidian-via-wordpress-plugin":{"title":"Publishing from Obsidian via WordPress plugin","links":[],"tags":["Obsidian"],"content":"\nDoes embedded note render when published to WP?\n\nNo, does not work with current WordPress publish plugin\n tested [completion:: 2024-03-23]\n\n\n"},"pursuit-of-purpose-happiness":{"title":"Pursuit of Purpose (Happiness)","links":[],"tags":["Self"],"content":"\nThe sun is shining, and the sky is blue. The flowers are in bloom, and the bees are buzzing. All is right in the world. This is a moment of happiness. But it is only a moment. Happiness comes and goes, like the tides. Purpose, on the other hand, is constant. It’s like a North Star that guides you through life’s ups and downs. It gives you something to strive for, even when times are tough.\nWhen you have a purpose, you can be happy even when things are tough because you know there’s something worth fighting for. Whether it’s a cause you’re passionate about or a goal you’re working towards, having a purpose gives your life meaning and fulfillment. So pursue your purpose with everything you’ve got—it just might be the key to lasting happiness\nHave you ever thought about your purpose? Where do we currently stand in the pursuit of purpose?\nWhat even is ‘Purpose’?\nIn the United States, our forefathers promised us freedom in pursuing happiness. And modern scholars tell us that happiness is linked to having a sense of purpose. So is the “pursuit of happiness” actually the “pursuit of purpose”?\nThe drug companies have tried to sell us the idea that we can simply buy “happiness in a bottle™” with a monthly subscription of pills. But most people want more than just momentary feelings of happiness. They are now seeking deeper fulfillment and meaning in their lives.\nHappiness is ephemeral, but purpose lasts forever. It’s something that you can feel good about not just in the present moment but also in retrospect when you look back at your life accomplishments.\nFor some people, their Purpose might be clear. But for others, it can be incredibly confusing to figure out what ours might be. And if we don’t have a sense of purpose, does that mean we’re not happy?\nIt’s important to remember that everyone has their unique path to follow and there is no single right or wrong answer when finding your purpose.\nWhen it comes to defining purpose, everyone will have their interpretations. You, I, and anyone else can talk philosophically, in circles, about what purpose is. But at the end of the day, do we all have a purpose? I believe we do.\nSome people may disagree with me on this point. But even if you’re unsure whether everyone has a specific purpose in life, you can at least ask yourself what your personal purpose might be.\nWhat is your purpose? It’s a big question and one that can be difficult to answer. If you’re struggling to figure out your exact Purpose with a capital P, don’t worry –you’re not alone. Many people go through their whole lives without ever really knowing what their true calling is.\nFive tips that might help you on your journey to find purpose:\n\nThink about what brings you joy and fulfillment- What are the things in life that make you happiest and most fulfilled? For some people, this might be spending time with family. For others, it could be traveling or being outdoors. Consider what activities make you lose track of time because you enjoy them.\nConnect with your passions- What topics or subjects are you consistently drawn to? Whether it’s fashion, music, art, or science- pay attention to the things that interest you most and see where they lead you.\nAsk yourself what difference you want to make- We all want to somehow leave our mark on the world. Think about how you would like to be remembered by others and what impactful change you would like to see during your lifetime.\nConsider your values and beliefs- What do YOU stand for? What kind of person do want to be seen as? When making decisions big or small throughout your life, always check back in with yourself to see if they align with who YOU want to be as a person\nKeep an open mind– Life’s beauty is constantly changing and evolving. As you grow older, your perspectives will change, too. You might have some clarity about your purpose today, but that could all change tomorrow so remain open to welcoming any new insights or realizations with open arms\n\nAnother way to start thinking about your Purpose is to ask yourself “Did someone else choose my Purpose for me?” This helps give some perspective on whether our Purpose is predetermined or if it’s something we each create for ourselves based on our unique talents and passions. No matter how abandoned, destitute, disabled, forgotten, lost, ignorant mischievous, misinformed negative, or toxic they may seem, everyone has a Purpose. This is called interbeing. Our lives are interdependent on everyone else in this interconnected world.\nSo, where do we currently stand in the pursuit of purpose?\nIt seems that we are at a crossroads. We can continue down the path of least resistance, letting others dictate our purpose for us. Or, we can take control of our lives and pursue happiness on our terms.\nThe choice is yours."},"qa-with-chatgpt-embeddings-basic-hyde-examples":{"title":"Q&A with ChatGPT + Embeddings - Basic & HyDE Examples","links":[],"tags":["AI","AI-Your-Business","ChatGPT","Conversational-AI","Embeddings","GPT-3","LLM"],"content":"\nOne of the most sought-after AI systems is one where you can easily query a knowledgebase via a ChatGPT-like interface.\nBasic Q&amp;A with Embeddings\nSince you can’t fine-tune the newer, more conversational models like the ChatGPT API, embeddings are how you “train” them on your proprietary knowledge base.\nTo create your embedding system, you must first embed your documents and store both the document and embedding vectors in a database.\nThen, when the user submits a query, their question is embedded, and the embedding vector is used to surface semantically similar documents. The content of these documents is merged with the question, and the result is used as a prompt to the ChatGPT or GPT-3 APIs.\nThis way, the language model answers your questions in the context of documents retrieved via the embedding system.\n\nHypothetical Document Embeddings (HyDE)\nHypothetical Document Embeddings (HyDE) is an embedding search technique that begins by generating a hypothetical answer and then using that hypothetical answer as the basis for searching the embedding system. It is a proven way to improve the accuracy of question answering by surfacing content that better matches the underlying intent of the query.\n\nThe Art of Embedding\nDocuments can and should be parsed, sliced and diced, into smart pieces that maximize their future value. There are nearly infinite ways to design these pieces, or “embedding inputs,” and their design will significantly impact your performance.\nIs your investment in AI mission-critical? Schedule a consultation."},"random-note-walks-a-simple-trick-to-boost-creativity-and-productivity":{"title":"Random Note Walks: A Simple Trick to Boost Creativity and Productivity","links":[],"tags":["Obsidian","PKM"],"content":"\nIf you’re like me, you have a lot of ideas. Some of them are good, some of them are bad, and some of them are just forgotten. It’s easy to forget about ideas as they get buried in our notes. One way I’ve learned to deal with this is what I call “Random Note Walks.”\nA Random Note Walk is simply browsing through random notes in your Obsidian vault. To make it even easier, you can assign a keyboard shortcut to open a random note. My shortcut is “ALT+R,” but you can use whatever you have available.\nThe Obsidian community has made this function even more useful by integrating the random note function with Obsidian’s search feature in the Smart Random Note plugin. This way, the random note is selected from a predetermined list of search results. This makes the process even more effective because you can exclude irrelevant folders or focus on a target notes segment.\nIf you’re looking for a way to boost your creativity and productivity, I highly recommend giving Random Note Walks a try!"},"revolutionizing-search-how-hypothetical-document-embeddings-hyde-can-save-time-and-increase-productivity":{"title":"Revolutionizing Search: How Hypothetical Document Embeddings (HyDE) Can Save Time and Increase Productivity","links":[],"tags":["AI","ChatGPT","Embeddings","GPT-3","LLM"],"content":"\nTools like the OpenAI Embeddings API are changing the way search is performed. Instead of matching keywords, a language model converts text into vectors. These vectors represent the text using hundreds or even thousands of dimensions mathematically. This way, when searching, matches can be made based on the underlying intent of the query instead of just keywords.\nThis is the search the authors of the Precise Zero-Shot Dense Retrieval without Relevance Labels sought to improve with their process called Hypothetical Document Embeddings (HyDE).\nThe HyDE hypothesis is that the document search would yield better results using hypothetical answers than the question itself.\nHow it works\nThe HyDE method is a way to find information in a large set of documents using artificial intelligence. It starts by having a Large Language Model (LLM), like ChatGPT, create a document based on a specific question or topic. This document may contain some false information, but it also has relevant patterns that can be used to find similar documents in a trusted knowledge base.\nNext, another AI model is used to turn the created document into an embedding vector, which is then used to find other documents similar to the one the AI model created.\nWhat are the implications?\nHyDE can enable language models in more sensitive applications since the search results are returned directly from a trusted source. This process prevents “hallucinations” by the LLM from being returned to the user. This can be useful in cases where exact measurements are necessary or incorrect answers could prove catastrophic, like in medicine.\nBetter searching of internal documents can save thousands of hours and maximize productivity. The HyDE paper focuses on generating hypothetical answers to questions using the LLM. However, it’s easy to imagine using the LLM to augment similarly incomplete content to find relevant material in applications outside of search, like writing content or programming.\nAlso, the search results don’t have to be the end of the road. For example, this search can be used behind the scenes to enable a chat interface, like ChatGPT, that utilizes the full context of your knowledge base in its answers without fine-tuning or exceeding the token limit!\nWhat’s next?\nAll the necessary language models for building a HyDE search system can be found at OpenAI.\nDo you need help unlocking the full potential of HyDE and other cutting-edge AI technologies?\nUnderstand which AI can be the biggest boost for your business—Schedule a consultation to learn more!"},"self-awareness":{"title":"Self-awareness","links":[],"tags":["Self"],"content":"\nIn a world where we are constantly bombarded with information and stimuli, it can be easy to forget about our own selves. We get so caught up in the hustle and bustle of everyday life that we often do not take the time to check in with ourselves and see how we are truly feeling. This is where self-awareness comes in.\nSelf-awareness is the ability to step back and examine our own thoughts, feelings, and actions without judgment. It allows us to understand our strengths and weaknesses and what triggers certain emotions within us. By being aware of ourselves, we can learn how to better cope with stressors and make positive changes in our lives.\nThere are many ways to increase self-awareness. One way is through meditation or other mindfulness practices. These help us to focus on the present moment and become more attuned to our thoughts and feelings. Another way is journaling or talking to someone who will listen without judgment (such as a therapist). Talking about our experiences can help us to process them more deeply and understand them better.\nUltimately, increasing self-awareness leads to a greater sense of happiness and peace because we can accept ourselves for who we are - warts and all."},"simple-eisenhower-matrix-template-for-obsidian-canvas":{"title":"Simple Eisenhower matrix template for Obsidian Canvas","links":[],"tags":["Obsidian","PKM","Productivity"],"content":"\nOpen a new canvas file in a text editor and paste the following:\n{\n\t&quot;nodes&quot;:[\n\t\t{&quot;id&quot;:&quot;8ce64c6c8b9e6b02&quot;,&quot;type&quot;:&quot;group&quot;,&quot;x&quot;:-940,&quot;y&quot;:-260,&quot;width&quot;:1012,&quot;height&quot;:920,&quot;label&quot;:&quot;Important&quot;},\n\t\t{&quot;id&quot;:&quot;71f8810fc415ed60&quot;,&quot;type&quot;:&quot;group&quot;,&quot;x&quot;:145,&quot;y&quot;:-260,&quot;width&quot;:795,&quot;height&quot;:920,&quot;label&quot;:&quot;-&quot;},\n\t\t{&quot;id&quot;:&quot;411104054bcd94b5&quot;,&quot;type&quot;:&quot;group&quot;,&quot;x&quot;:-940,&quot;y&quot;:-900,&quot;width&quot;:1012,&quot;height&quot;:560,&quot;label&quot;:&quot;Urgent &amp; Important&quot;},\n\t\t{&quot;id&quot;:&quot;24d3d3fdeaca0177&quot;,&quot;type&quot;:&quot;group&quot;,&quot;x&quot;:145,&quot;y&quot;:-900,&quot;width&quot;:795,&quot;height&quot;:560,&quot;label&quot;:&quot;Urgent&quot;}\n\t],\n\t&quot;edges&quot;:[]\n}\n\n\n"},"sitemap":{"title":"Sitemap","links":[],"tags":[],"content":"Posts\n\nStay Focused in Obsidian - My CSS Snippet for reducing distractions (August 5, 2024)\nSimple Eisenhower matrix template for Obsidian Canvas (April 21, 2024)\nPublishing from Obsidian via WordPress plugin (March 23, 2024)\nVector dimension precision effect on cosine similarity (January 8, 2024)\nMastering ChatGPT’s Code Interpreter: Comprehensive List of Python Packages in ChatGPT’s Code Interpreter (May 8, 2023)\nMastering ChatGPT Plugins: Optimizing Prompt Space in the Dynamic Plugin Prompt (May 1, 2023)\nEnhancing Large Document Summarization with Embeddings and Semantic Similarity (April 10, 2023)\nHow Attorneys and Law Offices can utilize Generative AI like ChatGPT (March 28, 2023)\nIs GPT-4 a “Spark” of Artificial General Intelligence? A New Era for Business Innovation (March 27, 2023)\nBuilding ChatGPT Plugins: A Fun and Simple Guide for Developers (March 23, 2023)\nOne Question at a Time: Using the Stop Parameter &amp; Prompt Engineering for a Better User Experience (March 23, 2023)\nGPT-4: A Quantum Leap in SVG Generation (March 22, 2023)\nWhat are Embeddings: A Simple Guide to Understanding Them (March 22, 2023)\nWhy Your Medium-Sized Business Needs a Chief AI Officer (March 21, 2023)\nIntroducing Smart Chat: Transform Your Obsidian Notes into Interactive AI-Powered Conversations with GPT-4 &amp; Embeddings (March 15, 2023)\nQ&amp;A with ChatGPT + Embeddings - Basic &amp; HyDE Examples (March 7, 2023)\nBeyond Time Savings: Evaluating the Full Value of AI Systems (March 6, 2023)\nChatML — New Prompt Format for the ChatGPT API (March 2, 2023)\nInfinite max_tokens in the ChatGPT API? (March 2, 2023)\nMaximize Productivity with ChatGPT &amp; Template-driven Conversations (February 27, 2023)\nThe LEGO Bricks Analogy for Building AI Systems (February 24, 2023)\nBuilding a Competitive Advantage (Moat) for AI Start-ups (February 16, 2023)\nCommon Misconceptions about the Capabilities of ChatGPT &amp; Large Language Models (LLM) (February 14, 2023)\nMaximize Your AI Success: Embrace Flexibility and Agility (February 6, 2023)\nAlternatives to Fine-tuning (January 31, 2023)\nIntroducing Smart Connections: The AI-powered Obsidian Plugin for Seeing Your Most Relevant Notes in Real-time (January 19, 2023)\nRevolutionizing Search: How Hypothetical Document Embeddings (HyDE) Can Save Time and Increase Productivity (January 18, 2023)\nUsing ChatGPT to Write a 2,000 Word Article (Why I wouldn’t, but how I would do it) (December 15, 2022)\nTesting Verbose prompts in ChatGPT (December 14, 2022)\nThe Role of AI in Education and Assessing Student Learning (December 8, 2022)\nHow to Use Generative AI in Writing Without Losing Creativity and Originality (December 5, 2022)\nExperimenting with ChatGPT for SVG Image Generation (December 1, 2022)\nThe Benefits of Creating a Blog for Software Developers (November 30, 2022)\nRandom Note Walks: A Simple Trick to Boost Creativity and Productivity (November 23, 2022)\nHow I Built ThankYouNote.app: A Tool to Help People Write Better Thank You Notes (November 15, 2022)\nPersonal AI: Your Guide to the “Super Landscape” (November 14, 2022)\nThe Best Way to Summarize a Paragraph Using GPT-3 (November 2, 2022)\nHow GPT-3 &amp; a Name Can Accurately Represent Your Personality (November 2, 2022)\nWhy Prompt Engineering Isn’t Going Anywhere (November 1, 2022)\nZero-shot Prompt Writing for Large Language Models Like GPT-3 (October 31, 2022)\nWFH.homes (October 30, 2022)\nSkill Guilds for Remote Workers (October 27, 2022)\nHow I Use Obsidian to Manage My Knowledge (October 26, 2022)\nWFH Brian’s Perspective-Context-Action Framework for Prompt Writing (October 25, 2022)\nWhat are Dynamic templates in Obsidian and why you should use them (October 24, 2022)\nHow the Context-subject Framework Can Help You Organize Your Notes (October 20, 2022)\n8-stage Personal Story Framework for Personal Branding (October 19, 2022)\nHow I use GPT-3 with Obsidian to explore prompt engineering (October 18, 2022)\nAn Intro to Getting Things Done (GTD): Horizons, Projects, &amp; Context Lists (October 13, 2022)\nHow to Get the Most Out of Writing GPT-3 Prompts (October 10, 2022)\nAsking GPT-3 about prompt engineering experiment (October 10, 2022)\nCurating Content in 2023 - The Fastest, Easiest, &amp; Most Effective System (October 5, 2022)\nPursuit of Purpose (Happiness) (September 29, 2022)\nWFH PKM - Personal Knowlege Management for Work from home (September 23, 2022)\nSelf-awareness (September 8, 2022)\n"},"skill-guilds-for-remote-workers":{"title":"Skill Guilds for Remote Workers","links":[],"tags":["Team-WFH"],"content":"\nWorking remotely has become increasingly popular in recent years, with more and more people opting to work from home, or a location of their choice. This arrangement has many advantages, such as greater flexibility and freedom, but also some disadvantages. One way to mitigate the disadvantages of working remotely is by organizing into skill guilds. Here’s how it could work…\nEach skill guild would be responsible for specific skills or knowledge. For example, there could be a guild for designers, developers, writers, marketers, etc. Members of each guild would be experts in their field and be available to help other members with their work.\nThe guilds would provide a way for remote workers to connect with others who have similar skills and knowledge. This would allow them to collaborate on projects, exchange ideas, and support each other. In addition, the guilds could offer training and development opportunities for their members.\nEach guild would have its leader, who would be responsible for keeping the group organized and connected. The leader may also be responsible for matching members with projects requiring their particular skill set.\nThe guilds could be organized online, through a dedicated website or forum. Additionally, they could occasionally meet in person similar to existing professional events.\nWhy organize into skill guilds?\nThere are several reasons why organizing into Skill Guilds makes sense:\n\nIt makes it easier to find people with the skills you need.\nIt helps to build a sense of community and connection among people with similar skill sets.\nIt makes it easier to develop new skills because members can learn from each other.\nIt can help individuals improve their craft by receiving feedback from others in the group.\n\nWhat are the benefits of skill guilds?\nThere are several benefits to being a member of a Skill Guild:\n\nYou can connect with others who have similar skills and knowledge.\nYou can collaborate on projects with other members.\nYou can learn from other members through training and development opportunities.\nYou can receive feedback on your work from other members.\n"},"stay-focused-in-obsidian-my-css-snippet-for-reducing-distractions":{"title":"Stay Focused in Obsidian - My CSS Snippet for reducing distractions","links":[],"tags":["Obsidian"],"content":"Hey there! If you’re like me, you spend a lot of time in Obsidian, organizing your thoughts, notes, and ideas. But sometimes, all those lines of text can be a bit overwhelming, right? Well, I’ve got a nifty CSS snippet that can help you zero in on the lines you’re working on, making everything else just fade a bit into the background.\nHere’s a quick breakdown of what this CSS does:\n\n\nHighlighting the Active Line: The line you’re currently working on gets a nice bump in size and opacity, making it stand out.\n\n\nDimming Inactive Lines: Lines before and after your current line get slightly reduced in size and opacity, helping you maintain focus on the task at hand.\n\n\n/* Use both size and opacity to focus on current line */\n.cm-active.cm-line:not(.HyperMD-codeblock):not(.HyperMD-table-2):not(.HyperMD-header){\n    padding-top: 0.77em !important;\n    &amp;:not(:has(*)),\n    &amp;&gt; *:not(.cm-indent):not(.cm-header):not(.cm-hmd-list-indent){\n        font-size: 1.6em !important;\n        line-height: 1.6em !important;\n    }\n}\n/* before active line */\n.cm-line:not(.HyperMD-codeblock):not(.HyperMD-table-2):has(+ .cm-active.cm-line:not(.HyperMD-codeblock):not(.HyperMD-table-2):not(.HyperMD-header)){\n    &amp;:not(:has(*)),\n    &amp;&gt; *:not(.cm-indent):not(.cm-header):not(.cm-hmd-list-indent)  {\n        font-size: 1.37em !important;\n        line-height: 1.37em !important;\n    }\n}\n/* after active line */\n.cm-active.cm-line + .cm-line:not(.HyperMD-codeblock):not(.HyperMD-table-2):not(.cm-header){\n    &amp;:not(:has(*)),\n    &amp;&gt; *:not(.cm-indent):not(.cm-header):not(.cm-hmd-list-indent)  {\n        font-size: 1.37em !important;\n        line-height: 1.37em !important;\n        opacity: 0.95;\n    }\n}\n/* 2 lines after active line */\n.cm-active.cm-line + .cm-line:not(.HyperMD-codeblock):not(.HyperMD-table-2):not(.cm-header) + .cm-line:not(.HyperMD-codeblock):not(.HyperMD-table-2):not(.cm-header){\n    padding-bottom: 0.77em !important;\n    &amp;:not(:has(*)),\n    &amp;&gt; *:not(.cm-indent):not(.cm-header):not(.cm-hmd-list-indent)  {\n        font-size: 1.23em !important;\n        line-height: 1.23em !important;\n        opacity: 0.80;\n    }\n}\n.cm-active.cm-line:not(.HyperMD-codeblock){\n    &amp;:not(:has(*)),\n    &amp;&gt; *:not(.cm-indent):not(.cm-header):not(.cm-hmd-list-indent){\n        opacity: 1;\n    }\n}\n/* &quot;dim&quot; in-active lines */\n.cm-line:not(.HyperMD-codeblock){\n    &amp;:not(:has(*)),\n    &amp;&gt; *:not(.cm-indent):not(.cm-header):not(.cm-hmd-list-indent){\n        opacity: 0.60;\n    }\n}\n/* dim headers */\n.cm-header, .cm-header &gt; * {\n    opacity: 0.80;\n}\n\n\n🌴"},"terms-of-service":{"title":"Terms of Service","links":[],"tags":[],"content":""},"testing-verbose-prompts-in-chatgpt":{"title":"Testing Verbose prompts in ChatGPT","links":[],"tags":["AI","ChatGPT","Experiments","GPT-3","Prompt-Engineering","Prompt-Writing"],"content":"\nAn experiment based on prompts from the awesome ChatGPT prompts GitHub repo. Which performs better, asking a simple question to ChatGPT? Or prefacing the question with a verbose prompt?\n”Machine Learning” Test Question sourced from the repository:\nI have a dataset without labels. Which machine learning algorithm should I use?\n\n\nFull prompt from the repository:\nI want you to act as a machine learning engineer. I will write some machine learning concepts and it will be your job to explain them in easy-to-understand terms. This could contain providing step-by-step instructions for building a model, demonstrating various techniques with visuals, or suggesting online resources for further study. My first suggestion request is &quot;I have a dataset without labels. Which machine learning algorithm should I use?&quot;\n\n\nTest using just the question with ChatGPT:\n\nTest using the full prompt from the GitHub repository:\n\n“Extreme TLDR” to compare the content of two responses\nSimplify the following to an &quot;Extreme TLDR&quot; that only contains the most essential elements to gain an understanding of the contents:\n\n\nJust the question\n\nFull prompt from the repository\n\nResults\nThe “full prompt” from the repository is WORSE than just asking the question itself. The “full prompt” response seems unnecessarily verbose. The “Extreme TLDR” test shows that the essence of the response is nearly identical to asking the question by itself."},"the-benefits-of-creating-a-blog-for-software-developers":{"title":"The Benefits of Creating a Blog for Software Developers","links":[],"tags":["Build-in-Public"],"content":"\nWhy Software Developers Should Consider Creating a Blog\nCreating a blog can be a powerful tool for software developers to showcase their skills, share their knowledge, and build their professional network. In today’s competitive job market, having a personal online presence is increasingly important for professionals in the tech industry. A well-written and informative blog can help software developers stand out from the crowd and can provide valuable benefits for their careers. This blog post will explore the key benefits of creating a blog for software developers and offer some practical advice for those considering starting their blog. Whether you are a seasoned veteran or a newcomer to the world of software development, a blog can be a valuable asset for your professional development and growth.\nThe Top Benefits of Creating a Blog for Software Developers\n\n\nDemonstrating your expertise: Writing a blog allows you to share your knowledge and experience with a broader audience. You can establish yourself as an expert in your field by sharing your thoughts and insights on the latest trends and developments in software development. This can be a valuable resource for other developers looking for guidance and advice and can also help you build credibility and trust within your professional network.\n\n\nNetworking and collaboration: A blog can be a great way to connect with other software developers within your organization and the broader tech community. By commenting on other people’s posts and engaging in online discussions, you can build relationships and collaborations with others who share your interests and passions. This can be a valuable source of support, mentorship, and professional development opportunities.\n\n\nProfessional development: Writing a blog can be a valuable learning experience. It can help you develop your writing and communication skills and allow you to experiment with new technologies and approaches. In addition, this can be a great way to stay up-to-date with the latest trends and developments in software development, and it can also help you stay motivated and engaged in your work.\n\n\nCreating a blog can provide numerous benefits for software developers, from demonstrating your expertise to networking and collaboration to professional development. Whether you are just starting your career or are an experienced professional, a blog can be a valuable tool for advancing your career and achieving your goals.\nTips for Writing Engaging and Informative Content for Your Blog\nWriting engaging and informative content is key to creating a successful blog for software developers. Here are some tips for creating content that will attract and retain readers:\n\n\nChoose a specific topic or niche: It can be tempting to try to cover a wide range of topics in your blog, but this can dilute your message and make it difficult for readers to know what to expect from your blog. Instead, try to focus on a specific topic or niche within the world of software development. This will help you establish yourself as an expert in a particular area and make it easier for readers to find and engage with your content.\n\n\nKeep it concise and to the point: Software developers are busy professionals and may not have much time to read long, rambling blog posts. Keep your posts concise and to the point to keep your readers engaged. Use clear and concise language, and avoid unnecessary jargon or technical terms.\n\n\nUse visuals and multimedia: A picture is worth a thousand words, and using visuals and multimedia in your blog posts can help make your content more engaging and interesting. Consider using images, videos, and other multimedia elements to illustrate your points and make your content more interactive and engaging.\n\n\nInvite reader participation: Encourage readers to engage with your content by inviting them to comment on your posts, share their thoughts and experiences, and ask questions. This can help create a sense of community and engagement and provide valuable feedback and ideas for future posts.\n\n\nOverall, writing engaging and informative content is essential for creating a successful blog for software developers. By focusing on a specific topic or niche, keeping your posts concise and to the point, using visuals and multimedia, and inviting reader participation, you can create content that will attract and retain readers and help advance your career.\nThe Value of Creating a Blog for Software Developers\nIn conclusion, creating a blog can be a valuable tool for software developers to showcase their skills, share their knowledge, and build their professional network. A well-written and informative blog can help developers demonstrate their expertise, network and collaborate with others in the field, and engage in ongoing professional development. By following the tips and advice outlined in this blog post, software developers can take the first steps towards creating their blogs and advancing their careers. Whether you are a seasoned veteran or a newcomer to the world of software development, a blog can be a valuable asset for your professional development and growth."},"the-best-way-to-summarize-a-paragraph-using-gpt-3":{"title":"The Best Way to Summarize a Paragraph Using GPT-3","links":[],"tags":["GPT-3","Prompt-Engineering","Prompt-Writing"],"content":"\nChatGPT Update\nChatGPT was recently released, so I wanted to update this post with valuable new prompts for summarizing. One of the benefits of ChatGPT is that the prompts don’t have to be as complicated as in other GPT-3 models.\nAs you can see, this prompt is straightforward. The actual prompt text is only two lines and many fewer words than in the past prompt, which was written for the text-davinci-002 model.\nSummarize the following:\n---\n{{COPY&amp;PASTE CONTENT HERE}}\n---\nBEGIN:\n\n\nThis next prompt is similar but returns the summary in a list format.\nSummarize the following in list format:\n---\n{{COPY&amp;PASTE CONTENT HERE}}\n---\nBEGIN:\nList:\n-\n\nBest GPT-3 Summarization prompt\nIn a recent Reddit thread, a user asked about the best ways to summarize a paragraph using GPT-3. Another user responded with the best prompt I’ve come as far as maximally compressing the information in a short text.\nPersonally, seeing other people’s prompts has been the best way for me to learn about prompt writing and that’s why I wanted to share this prompt.\nWe introduce Extreme TLDR generation, a new form of extreme summarization for paragraphs. TLDR generation involves high source compression, removes stop words and summarizes the paragraph whilst retaining meaning. The result is the shortest possible summary that retains all of the original meaning and context of the paragraph.\n\nExample\n\nParagraph:\n\nINSERT TEXT TO SUMMARIZE HERE\n\nExtreme TLDR:\n\n\nCompared to similar prompts, this prompt is quite effective at summarizing short texts. However, it should be noted that longer texts may be more difficult for this prompt to handle accurately. With some quick mental math, 100 tokens = 75 words, I’d estimate the maximum size this prompt could possibly work for would be 2000-3000 words, given that you’re using the Davinci model with the max_tokens set to 4096."},"the-lego-bricks-analogy-for-building-ai-systems":{"title":"The LEGO Bricks Analogy for Building AI Systems","links":[],"tags":["AI","LLM"],"content":"\nBuilding with LEGO bricks is a fun and creative activity that allows you to construct various structures by combining different-sized bricks in multiple ways. This same concept applies to the development of artificial intelligence (AI) systems; you can connect different AI models to meet specific requirements, creating an impressive AI system like constructing a LEGO castle! Let’s explore further how this analogy applies to creating AI systems.\nThe LEGOs Bricks Analogy for AI Systems\nThink of each AI model as one unique-sized LEGO brick - when combined with other pieces, they come together as one unified structure.\nLike each type of LEGO brick, each AI model serves a distinct function. And complex solutions become achievable goals when all these components come together into one cohesive platform.\nAnd much like a complex LEGO creation, combining multiple AI models requires patience, understanding, and planning.\nEmbeddings are the Keystones\nEmbeddings are essential for providing the AI system access to a large corpus of knowledge.\nConversational Models are the Façade of the AI System\nThe conversational model is the “face” of the AI system. It allows for communication between the user and AI.\nFine-Tuned Conversational Models Reduce Variability\nFine-tuned conversational models enable more consistency in communication between users and the AI system while decreasing the need for prompt engineering.\nReinforcement Learning Models Provide Adaptability\nReinforcement learning allows an AI system to adjust its behavior based on feedback from a reward system (e.g., rewarding tasks performed correctly). The reward system improves the adaptability and accuracy of an AI system.\nDigging the Moat with AI\nBuilding an AI system is like building a LEGO castle, where the inside is a mystery from the people’s perspective outside the castle. Your moat is the arrangement of AI components that drive the castle’s operations.\nThese different components’ arrangement allows us to create robust systems that handle complex tasks. Furthermore, by carefully considering each component’s strengths and weaknesses, we can determine how best to combine them into efficient yet powerful systems capable of achieving desired objectives in a way that outperforms the competition.\nBuilding Your AI Castle\nBuilding your first AI system is an exhilarating and creative endeavor, akin to constructing your first LEGO castle as a child. As the architect, it is crucial to comprehend the various AI components and their potential combinations. Then, with a strategic approach and careful planning, you can build AI systems that achieve outcomes similar to the impressive creations made from simple LEGO bricks."},"the-perspective-context-action-method-for-prompt-writing":{"title":"WFH Brian's Perspective-Context-Action Framework for Prompt Writing","links":[],"tags":["GPT-3","Prompt-Engineering","Prompt-Writing"],"content":"\nA Language Model (LM), like GPT-3, is a complex system, and the quality of the results you get from them depends on how you write your prompts. Prompt writing, or “prompt engineering” to some, is the process of designing prompts that will produce the best possible results.\nDesigning language model prompts can be difficult, but there is a method that can help make the process easier.\nIn my experiments with GPT-3, I’ve found that the best prompt designs contain three important components: perspective, context, and action. Each of these parts plays an important role in getting the best output.\nThis prompt writing strategy makes the LM more of an exploratory tool than one that simply reproduces. It’s a way of writing “zero-shot” prompts, which can be seen as the opposite of the “k-shot” prompts where specific examples of acceptable outputs are provided in the prompt.\nPerspective refers to the point of view from which your prompt is written. For example, if you’re writing a prompt about a character in a book, you could write it from the character’s point of view (first-person perspective), or an omniscient narrator’s point of view (third-person perspective). The choice of perspective can greatly impact the quality of the results you get from your LM.\nContext refers to the situation in which your prompt is used. For example, if you’re writing a dialogue between two characters, it’s important to consider who they are and their relationship. That way, you can ensure that the conversation sounds natural and believable.\nThe action represents what we want our LM to do. In other words, it’s the goal of our prompt. For example, if we want our LM to generate a character description, we need to include some actionable elements in our prompt (e.g., “Describe the character’s appearance”).\nLet’s take a closer look.\nWhat is Perspective in a Prompt?\nPerspective is the angle or point of view from which something is seen. In the context of a prompt, perspective represents the angle or point of view from which the prompt evaluates your request. When creating a prompt, it is important to consider the perspective from which the language model will view the prompt. The perspective can be thought of as the “lens” through which the LM will interpret the prompt. By considering the perspective, we can ensure that the LM understands the prompt in the way we intend.\nIn story writing, if you were designing a prompt for a language model to generate a story about a character’s day, the perspective could be from the character’s first-person point of view. This would mean that the prompt would need to include information about what the character sees, hears, and thinks throughout the day.\nAnother example could be if you designed a prompt for a language model to generate an advertisement for a product. The perspective could be from a “Marketing Expert” or “Advertising Specialist” so the LM considers relevant information, like what the product does and how it benefits consumers.\nexamples of changes in perspective\nYou are a first grader. Write one sentence about the sky:\n\n\nThe sky is blue and it has clouds in it.\nYou are a physics professor. Write one sentence about the sky:\n\n\nThe sky appears blue because of Rayleigh scattering.\nYou are a meteorologist. Write one sentence about the sky:\n\n\nThe sky looks like it might rain.\nWhat is Context in a Prompt?\nViewing Context as separate from Perspective allows us to see it more clearly as providing a set of cues or constraints the LM must follow. When we construe Context too narrowly, our response options may be unduly limited; when we construe it too broadly, our responses may be off-target or miss the point entirely.\nWhen designing prompts for a language model, Context represents the situation in which the prompt is used. This is different from Perspective, which represents the point of view from which the prompt is given.\nFor example, if you were designing a prompt for a chatbot, the Context would be the conversation that led up to the bot’s response. At the same time, the Perspective would be the bot’s disposition or personality used when interacting with its users.\nNow let’s look at how Context affects the output of a language model. The most important thing to remember about Context is that it provides context for the LM’s responses. Without context, the LM would not know how to interpret or respond to a given prompt. For example, if you said “I’m hungry” to an LM without providing context, it wouldn’t know whether you’re trying to start a conversation about food or just making an observation. However, if you provide context by saying something like “I’m hungry, and I was wondering if you know any good restaurants near me.” It becomes clear that you’re looking for specific information and The LM can respond accordingly.\nContext is important because it helps determine what information is relevant and what isn’t. In other words, it allows us to filter out noise to focus on getting useful results from our language models.\nexamples of changes in context\nYou are a first grader. It&#039;s night time. Write one sentence about the sky:\n\n\nThe sky is very dark.\nYou are a physics professor. It&#039;s night time. Write one sentence about the sky:\n\n\nThe night sky is full of stars.\nYou are a meteorologist. It&#039;s night time. Write one sentence about the sky:\n\n\nThe moon is out and there are a few stars visible.\nWhat is Action in a Prompt?\nThe action is the part of the prompt that specifies what the model should do with the information it has been given. Action is different from perspective and context in that it represents the specific task or goal you want the language model to perform.\nVerb choice is important. If you are translating, you will want to specify an appropriate action, such as “translate the sentence…” If you are writing a prompt for question-answering, your action might be “answer the question… ”\nIf you are writing a blog post, the verb “write” may be sufficient for getting some text that’s on topic. However, changing that to “explain” will improve the output by including additional reasoning.\nexamples of changes in action\nYou are a first grader. It&#039;s night time. Explain:\n\n\nIt’s nighttime. The sun has gone down and it’s dark outside.\nYou are a physics professor. It&#039;s night time. Explain:\n\n\nThe night sky is full of stars. Each star is a huge ball of gas, held together by its own gravity. The gravity of a star keeps all the gas in the star from flying off into space.\nYou are a meteorologist. It&#039;s night time. Explain:\n\n\nThe moon is out and it’s a clear night. The stars are shining brightly.\nThe Order of Perspective-Context-Action Prompt Design\nPerspective-Context-Action Prompt Design starts with the broadest instruction, Perspective, and ends with the most narrow, Action.\nThe order of the Perspective-Context-Action components is important because it allows the LM to focus on a smaller set of possibilities, leading to improved results.\nThis order is useful from a human perspective, as it’s easier to think broadly than specifically.\nexamples of changes in ordering\nWrite one sentence about the sky. It&#039;s night time. You are a first grader:\n\n\nThe sky is dark and full of stars.\nWrite one sentence about the sky. It&#039;s night time. You are a physics professor:\n\n\nThe night sky is full of stars.\nWrite one sentence about the sky. It&#039;s night time. You are a meteorologist:\n\n\nThe sky is full of stars.\nWhen to use Perspective-Context-Action Prompt Design\nWhen you want to explore and develop ideas from scratch, Perspective-Context-Action Prompt Design is the best method. This is because it allows you to interact with the language model in a way that helps you generate new ideas, rather than simply replicating existing ones.\nAn analogy could be an author, zero-shot, and a printing press, k-shot. The author uses zero-shot prompts to explore and develop ideas from zero-to-one. Meanwhile, the printing press uses k-shot prompts to replicate output with a known value from one-to-many times.\nIf you are looking to generate new ideas from an LM, or want your LM to produce outputs tailored to a specific audience, then you should try using the perspective-context-action method for prompt writing."},"the-role-of-ai-in-education-and-assessing-student-learning":{"title":"The Role of AI in Education and Assessing Student Learning","links":[],"tags":["AI","Education","Future"],"content":"\nIn our conversation, Sean Dagony-Clark and I discussed the role of AI in education and the importance of assessing students’ ability to think and generate ideas independently. While Sean argues that incorporating AI in education should be done after students have demonstrated their ability to think and generate ideas, I believe AI can be used as a tool for curating and organizing ideas. We both agree on the need for a common definition of knowledge and the importance of assessing students’ ability to apply that knowledge. The following article expands on topics from our conversation.\nIntroduction\nAs educators, we must assess and evaluate the learning and understanding of our students. In a rapidly changing world, where technology is increasingly integrated into our daily lives, it is imperative that we also consider the role of artificial intelligence (AI) in education. However, many schools and educators are ignoring the use of AI in teaching and assessment, leading to students using AI assistance without proper evaluation of their learning. In this article, we will delve into the problem of AI assistance in education and discuss the importance of incorporating AI responsibly and effectively.\nCurrent State of Affairs\nNeglect of AI in education\nThe current state of affairs in education is neglect and avoidance when it comes to using AI. Many educators and schools are sticking their heads in the sand and ignoring that students are already using AI to assist with their writing and learning. This lack of attention to the use of AI in education leaves students vulnerable to the limitations and pitfalls of relying on AI assistance without proper evaluation of their learning.\nRather than addressing the issue and teaching students how to incorporate AI responsibly and effectively, we are allowing them to use it blindly without understanding its limitations and potential harm to their development of critical thinking and writing skills. For example, students are using AI to write a paper without understanding the underlying concepts and ideas that writing the paper is meant to teach.\nThe harm to students\nThis lack of attention and evaluation is not only detrimental to the students, but it is also a disservice to the educators who are unable to properly assess the learning and progress of their students.\nImpact on educators\nIt is time for us to confront the reality of AI in education and address the need for proper evaluation and assessment in the age of AI assistance. Only then can we ensure that students gain the necessary skills and knowledge to succeed in their academic and professional endeavors.\nThe Problem with AI Assistance\nAs AI technology advances, it is increasingly being used in educational settings. However, there are several problems with relying too heavily on AI assistance in teaching and assessing student learning.\nUsing AI to generate ideas and thoughts can limit students’ ability to develop their own critical thinking skills. Instead of learning how to create and evaluate their own ideas, they may become reliant on AI to do the thinking for them. The use of AI can become a crutch, stifling creativity and originality. This is a concerning possibility in a world where innovation and ingenuity are highly valued.\nAdditionally, if students are not adequately evaluated on their understanding and ability to generate their own ideas, they may not be adequately prepared for real-world situations where AI assistance may not be available. This can lead to a lack of confidence and competence in their abilities, hindering their professional growth and success.\nSecond, relying on AI to assist with writing can also hinder students’ development of writing skills. While AI may be able to help with spelling and grammar, it cannot teach students how to organize their thoughts, create cohesive arguments, and use language effectively. By using AI as a crutch, students may never learn these crucial writing skills.\nFurthermore, the use of AI in education raises ethical concerns. As AI becomes more advanced, it may be able to generate original content that is indistinguishable from human-written work. This poses a challenge for educators who must assess student learning and ensure that the work submitted is their own.\nLastly, there is the potential for students to unknowingly propagate biases if they don’t truly understand the knowledge they are sourcing from the AI.\nFurther examination required\n\n\nWhether students who rely on AI assistance have lower critical thinking and writing skills or provide insight into real-world situations where students have struggled because of a dependence on AI assistance.\n\n\nCase studies of schools that have struggled with incorporating AI and how they overcame those challenges.\n\n\nCase study of a school that successfully incorporated AI into its curriculum and how it was able to evaluate student learning and development.\n\n\nResearch on the effects of AI assistance on students’ writing abilities and critical thinking skills or anecdotes from educators who have observed these effects in their students.\n\n\nIncorporating AI in Education\nAs AI becomes an increasingly prominent part of our lives, educators must address its role in the classroom. Rather than simply ignoring or hoping to catch students who use AI, we need to teach them how to incorporate it responsibly and effectively.\nThis means taking a nuanced approach to incorporating AI in education. On the one hand, we want to provide students with the tools and resources they need to succeed in an increasingly tech-driven world. On the other hand, we don’t want to hinder their development of critical thinking and writing skills by relying too heavily on AI assistance.\nOne way to strike this balance is by teaching students how to use AI as a tool rather than a crutch. This means allowing them to experiment with different AI platforms and programs and showing them how to use them to enhance their own ideas and thoughts. It also means guiding when and how to use AI assistance and when it’s better to rely on their own knowledge and skills.\nUltimately, the key is to find a balance between incorporating AI into education and maintaining the focus on students’ ability to generate their own ideas and thoughts. By teaching students to use AI responsibly and effectively, we can ensure they gain the necessary skills to succeed in an increasingly technological world.\nAssess students on their understanding and ability to use AI effectively\nA key aspect of incorporating AI in education is proper evaluation and assessment. Rather than just accepting AI-assisted work at face value, we need to assess whether students truly understand and generate their own ideas. This means looking beyond the words on the page and considering the thought process behind them.\nThis could involve providing them with a set of guidelines or criteria for using AI in their work and then evaluating their ability to follow those guidelines and incorporate AI in a way that adds value to their work. This teaches students how to use AI responsibly and allows for a more comprehensive evaluation of their learning.\nFor example, a student studying creative writing could use AI to generate potential plot points or character traits but then be required to develop those ideas on their own and incorporate them into their own writing. This allows for the incorporation of AI without sacrificing the student’s ability to generate their own ideas and thoughts.\nAdditional Ideas for Incorporating AI in Education\n\n\nAI can provide personalized learning experiences for students, adapt to their individual needs and learning styles, and provide real-time feedback on their progress.\n\n\nAI can be used to provide immediate personalized feedback on writing assignments.\n\n\nAI can be used to curate and organize vast amounts of information, allowing students to quickly access and evaluate relevant sources for their research and projects.\n\n\nAI can help organize and structure students’ ideas, providing a starting point for their own thinking and writing.\n\n\nAI can be used as a tool for assessment, providing educators with valuable data on students’ progress and understanding.\n\n\nConclusion\nThe role of AI in education and assessing student learning is a complex and vital issue that cannot be ignored. As educators, we are responsible for teaching students to use AI responsibly and effectively while assessing their understanding and ability to generate their own ideas. We must not let the convenience of AI assistance hinder the development of critical thinking and writing skills in our students.\nAs we continue to incorporate AI into our daily lives, we must address its impact on education and learning. We must strive for a balanced approach that allows for the use of AI without sacrificing the fundamental skills and knowledge that our students need to succeed.\nHow are you incorporating AI into your teaching and assessment practices? And, more importantly, how are you ensuring that your students are learning and developing their own skills and knowledge rather than relying solely on AI assistance?"},"using-chatgpt-to-write-a-2000-word-article-why-i-wouldnt-but-how-i-would-do-it":{"title":"Using ChatGPT to Write a 2,000 Word Article (Why I wouldn't, but how I would do it)","links":[],"tags":["AI","ChatGPT","GPT-3","Prompt-Engineering"],"content":"\nWhile technically speaking, you cannot generate a 2,000-word article with ChatGPT, it can be done if you break the content into smaller pieces. In addition to the technical limitations, there are also style limitations to consider when generating content with ChatGPT. The style limitations are why I wouldn’t use ChatGPT.\nIn this article, I will explain both sets of limitations. And while I wouldn’t do it myself, I will explain step-by-step how you could generate a 2,000-word article with ChatGPT.\nTechnical Limitations\nThe technical reasoning for why ChatGPT cannot simply generate 2,000 words is max-tokens. Evidence of ChatGPT’s token size points to a max-token size of ~8,000, translating into ~6,000 words based on this quick calculation.\nChatGPT will be more likely to deviate from your desired direction as you try to generate more words in one shot. So even with the ability to generate that much, the number of words you find useful may be much smaller.\nThe required amount of tokens is determined by both the inputs and the outputs, which likely includes behind-the-scenes inputs that we don’t see.\nBut the simple max-tokens calculation doesn’t consider that OpenAI is likely instructing ChatGPT to limit its output in addition to the token limit. Furthermore, any behind-the-scenes instructions will count towards the token count, too!\nAltogether, this means you want to increase your input and decrease your expected output to generate the best text.\nStyle Limitations\nQuality, as in writing style, is an important aspect to consider when deciding how to generate content from an AI. ChatGPT is designed to be “safe.” One way it does that is by producing consistently bland text. This “bland” writing style is great for answering questions in a way anyone can understand. But it’s not so great for writing a hit piece of content.\nRemember that robotic-sounding text-to-speech voice? Millennials probably remember using it to say bad words in their middle school library. Well, that’s the voice equivalent of ChatGPT’s writing style.\nFortunately, not all GPT-3 models have the same writing style as ChatGPT. The trade-off is that they are harder to work with in other ways. While ChatGPT requires the least from its users, it also provides the least flexible output.\nThe more flexible the output, like writing that reflects your unique voice, the more “prompt engineering” is required. The more capable models also lack the easy-to-use conversational interface, but the flexibility means they can be integrated into your custom workflow. If you’re in advanced GPT-3 utilization, I can help.\nHow to generate a 2,000-word article with ChatGPT\nAfter all that, if you still want to generate 2,000 words or more using ChatGPT, here’s how I would do it, step-by-step:\nStart with an expectation of breaking the content into at least 4-5 sections that make up the final 2,000 words. ChatGPT is quite capable of writing 400-500 words at a time. Choosing even smaller sections, and carefully curating which make it into your draft, will make for a much better final result. To do this, begin with an outline. The outline can be created with or without ChatGPT.\nIt’s important to maintain this outline while you instruct ChatGPT to generate each section. This will give ChatGPT the necessary context of the entire article while writing each section without encountering the max-tokens limit.\nI recommend using a numbered outline to make things easier. Then, for each section, you can specify which section is the current focus by number.\nThe ChatGPT interface provides a helpful “try again” button. You should use this to generate multiple outputs for each section. Then, use your intuition to select the best for your article.\nThroughout the process, you should maintain a document for your draft. As you complete the sections, copy &amp; paste the best outputs into your draft document.\nIn the end, your document should contain all the sections generated by ChatGPT to make up your final 2,000-word article."},"vector-dimension-precision-effect-on-cosine-similarity":{"title":"Vector dimension precision effect on cosine similarity","links":[],"tags":["AI","Embeddings","Experiments"],"content":"\nExperiment Outline\nQuestion: How does reducing the precision of vector components to various extents (half, third, quarter, fifth) using different methods (toFixed, Math.round) affect the cosine similarity between two vectors?\nHypothesis: Reducing the precision of vectors will alter the cosine similarity, with more significant reductions leading to larger differences. The method of precision reduction might not significantly impact the cosine similarity.\nExperiment Design:\n\n\nControl Group: Compute cosine similarity between two original vectors.\n\n\nVariable Groups: For each level of precision reduction (half, third, quarter, fifth) and for each method of precision reduction (toFixed, Math.round), compute cosine similarity between precision-reduced vectors.\n\n\nMeasurement: Compare the cosine similarities across different levels of precision reduction and methods.\n\n\nData Collection: Implement JavaScript code to calculate cosine similarities in each case and run multiple iterations to average the results.\nAnalysis: Evaluate how different levels and methods of precision reduction impact the cosine similarity value.\nCode Specification\nFunctions for Cosine Similarity and Vector Generation: Functions to compute the dot product, magnitude, cosine similarity, and generate random vectors with specific bit-depth and dimensions.\nFunctions for Precision Reduction:\n\n\nTwo functions to reduce precision: one using toFixed and another using Math.round.\n\n\nApply these functions to vectors with varying degrees of precision reduction (half, third, quarter, fifth).\n\n\nImplementation Considerations:\n\n\nUse ES2020 standards.\n\n\nFocus on readability and performance optimization.\n\n\nAdapt the code to handle vectors of different dimensions (384 and 1536) and bit-depths (16-bit and 8-bit).\n\n\nFunction for Averaging Differences: A function to calculate the average difference in cosine similarity over multiple iterations for each precision reduction level and method.\nExecution of Experiment: Run the experiment with 1000 iterations for each combination of vector type, dimension, and precision reduction method.\nInterpretation of Results\nThe results of this experiment will help understand the extent to which precision reduction affects the similarity of high-dimensional vectors. This is particularly relevant in applications like data compression or optimization in machine learning, where a balance between precision and computational efficiency is often sought. The findings indicate that while precision reduction does impact cosine similarity, the effects are relatively minor, even with significant reductions. This suggests potential flexibility in the precision of vector representations in certain applications, without substantially compromising their comparative similarity.\n\nCheckout the full code on Github.\nnode embeddings/precision-reduction-impact-on-cosine-similarity.js\n\n\nResults\nAverage differences for 16-bit vectors (384-dim): {\n  precision_half_to_fixed: &#039;0.0000007919%&#039;,\n  precision_half_math_round: &#039;0.0000007919%&#039;,\n  precision_third_to_fixed: &#039;0.0001907019%&#039;,\n  precision_third_math_round: &#039;0.0001907019%&#039;,\n  precision_quarter_to_fixed: &#039;0.0019869799%&#039;,\n  precision_quarter_math_round: &#039;0.0019869799%&#039;,\n  precision_fifth_to_fixed: &#039;0.0182798241%&#039;,\n  precision_fifth_math_round: &#039;0.0182798241%&#039;\n}\nAverage differences for 16-bit vectors (1536-dim): {\n  precision_half_to_fixed: &#039;0.0000009126%&#039;,\n  precision_half_math_round: &#039;0.0000009126%&#039;,\n  precision_third_to_fixed: &#039;0.0001162906%&#039;,\n  precision_third_math_round: &#039;0.0001162906%&#039;,\n  precision_quarter_to_fixed: &#039;0.0010907664%&#039;,\n  precision_quarter_math_round: &#039;0.0010907664%&#039;,\n  precision_fifth_to_fixed: &#039;0.0099245784%&#039;,\n  precision_fifth_math_round: &#039;0.0099245784%&#039;\n}\nAverage differences for 8-bit vectors (384-dim): {\n  precision_half_to_fixed: &#039;0.0000009423%&#039;,\n  precision_half_math_round: &#039;0.0000009423%&#039;,\n  precision_third_to_fixed: &#039;0.0003219478%&#039;,\n  precision_third_math_round: &#039;0.0003219478%&#039;,\n  precision_quarter_to_fixed: &#039;0.0038977933%&#039;,\n  precision_quarter_math_round: &#039;0.0038977933%&#039;,\n  precision_fifth_to_fixed: &#039;0.0331642817%&#039;,\n  precision_fifth_math_round: &#039;0.0331642817%&#039;\n}\nAverage differences for 8-bit vectors (1536-dim): {\n  precision_half_to_fixed: &#039;0.0000012704%&#039;,\n  precision_half_math_round: &#039;0.0000012704%&#039;,\n  precision_third_to_fixed: &#039;0.0001971148%&#039;,\n  precision_third_math_round: &#039;0.0001971148%&#039;,\n  precision_quarter_to_fixed: &#039;0.0021234765%&#039;,\n  precision_quarter_math_round: &#039;0.0021234765%&#039;,\n  precision_fifth_to_fixed: &#039;0.0219500987%&#039;,\n  precision_fifth_math_round: &#039;0.0219500987%&#039;\n}\n\n\nThe experiment results show the average difference in cosine similarity between the original and precision-reduced vectors, for different methods of precision reduction (toFixed and Math.round) and for varying degrees of precision reduction (half, third, quarter, fifth). The experiment was conducted on two types of vectors: 16-bit and 8-bit, with two different dimensions (384 and 1536).\nKey Observations\nImpact of Precision Reduction:\n\n\nAs the precision reduction becomes more aggressive (from half to fifth), the average difference in cosine similarity increases. This indicates that the loss of precision generally has a more pronounced effect as more decimal points are removed.\n\n\nEven at the most aggressive level of precision reduction (to a fifth), the change in cosine similarity is relatively small, in the order of hundredths of a percent.\n\n\nComparison of Reduction Methods:\n\nThere is no noticeable difference between the toFixed and Math.round methods in terms of their impact on cosine similarity. This suggests that both methods of rounding have a similar effect on the precision of the vectors and their resulting cosine similarities.\n\nEffect of Vector Dimensionality:\n\nThe dimensionality of the vectors (384 vs. 1536) seems to have a minor impact on the results. The pattern of increasing differences with more aggressive precision reductions holds in both cases, though the exact values differ slightly.\n\n16-bit vs. 8-bit Vectors:\n\nThere’s a consistent trend across both types of vectors. The differences are very small, but consistently, 8-bit vectors show a slightly higher difference in cosine similarity compared to 16-bit vectors when the precision is reduced. This could be due to the lower initial precision of the 8-bit vectors, which makes further precision reduction more impactful.\n\nInterpretation\nThe experiment’s results suggest that reducing the precision of vectors has a measurable but minor impact on their cosine similarity. This impact becomes slightly more pronounced as the degree of precision reduction increases, but even the most significant changes are relatively small. The method of precision reduction (rounding vs. truncating) does not appear to significantly affect the results.\nThese findings could have practical implications in applications that utilize vector embeddings, where high-dimensional vectors are used to represent complex data. The results suggest that it’s possible to reduce the precision of these vectors (for instance, for storage or computation efficiency) with only a minimal impact on their comparative similarity. However, the degree to which precision can be reduced without significantly affecting the results will depend on the specific requirements and tolerances of the application."},"wfh-homes":{"title":"WFH.homes","links":[],"tags":["Team-WFH","WFH"],"content":"\nI’ve been working from home for over 10 years, and in that time, I’ve learned a lot about what makes a perfect WFH Home.\nI know firsthand how important it is to have a comfortable and functional home that meets all your needs.\nThat’s why I started WFH.homes – to give people the knowledge they need to make informed decisions about their WFH homes.\nAfter researching and writing about WFH real estate trends, I’ve come to believe there is no one-size-fits-all answer to finding the perfect WFH home.\nWhether you’re looking for tips on turning your spare bedroom into a home office, searching for the places with the right amenities, or knowing what kind of property features will make your WFH life easier, WFH.homes is here to help."},"wfh-pkm":{"title":"WFH PKM - Personal Knowlege Management for Work from home","links":[],"tags":["PKM"],"content":"\nFor those who don’t know, I want to tell you about Personal Knowledge Management (PKM).\nOne of my biggest challenges is managing my knowledge. There’s so much information out there, and it’s hard to keep track of everything. But, I know I need to manage my knowledge to succeed. That’s where PKM comes in. It’s about keeping track of what’s important to you and your work.\nPKM is all about organizing and managing your knowledge. It helps you determine the importance of information and how to use it best. PKM can be helpful for WFH workers. Here are some of the benefits:\n• PKM helps me stay organized and keep track of all the different pieces of information that I need to remember.\n• PKM helps me make better decisions by providing a systematic way to gather and review information.\n• PKM helps me share my knowledge with others more.\n• Finally, PKM helps me improve my learning by identifying gaps in my understanding and helping me find the resources I need to fill those gaps.\nHere’s how PKM works: first, I collect information from various sources. Information in your PKM could be anything from news articles to blog posts to conversations with colleagues. Some notes I then organize so I can find them later when needed, while the rest remains in searchable logs. Finally, I use this information to help me make better decisions about my work.\nFor example, let’s say I’m working on a project that requires research on a new topic. Using PKM, I would first collect documents, notes, and other related resources. Next, organize them in a way that makes sense for the project – this might mean creating folders or tagging them with keywords. Finally, when it comes time to do the research, draw upon these resources – rather than starting from scratch – which saves me time and allows me to produce better work.\nThere are many ways to do PKM, but the basics are pretty simple. First, you need to identify the sources of information that are most relevant to your work. Sources could be websites, books, articles, people you know, etc. Once you’ve identified your sources, it’s time to start collecting and organizing the information in a way that makes sense for you.\nDesigning your PKM system is where things can get a bit more personal - there’s no one right way to create a PKM system. Some people like using apps or software programs, while others prefer old-fashioned pen and paper. I like combining digital and analog methods - it works best for me. But whatever method(s) you choose, the important thing is that they work for YOU. The goal is to find a system that helps YOU be more productive and efficient in your work-from-home life! Make sure to revisit the design of your system and update it as needed based on what’s changed in your life or business. That’s it! Those are the basics of Personal Knowledge Management - super simple but effective if you try it.\n4 Step PKM for WFH Workers\nCollect\nWFH knowledge workers need to be able to collect information and knowledge from a variety of sources, both online and offline.\n\nThis includes curating content from the internet, saving emails and chat threads, and maintaining a personal knowledge base.\n\nReflect\nWFH knowledge workers need to be able to take time to reflect on the information and knowledge they have collected.\n\nThis means thinking about what the information means, how you can use it, and how it fits into their current understanding of the world.\n\nOrganize\nWFH knowledge workers need to be able to organize the information and knowledge they have collected in a way that makes sense for them.\n\nThis includes creating systems for storing and retrieving information and methods for classifying and categorizing information.\n\nThis includes tagging, sorting, and creating taxonomies or folksonomies.\n\n\n\nShare\nWFH knowledge workers need to share the information and knowledge they have organized with others who might find it helpful.\n\nThis includes writing blog posts or articles, sharing presentations or case studies, participating in online discussion forums, or collaborating on projects with remote team members\n\nOverall, PKM is an excellent tool if you want to:\n• be more organized\n• make better decisions\n• share knowledge more\n• improve your learning process.\nPKM has helped me immensely when working from home. I can get more done and be more organized since adopting this way of working. Hopefully, PKM can help you too!"},"what-are-dynamic-templates-in-obsidian-and-why-you-should-use-them":{"title":"What are Dynamic templates in Obsidian and why you should use them","links":[],"tags":["Obsidian","PKM"],"content":"\nDynamic templates are the way to go if you’re looking for a more efficient way to manage your templates in Obsidian. With this templating strategy, you can bind a template with the current folder, so that if you trigger the dynamic template, the relevant template will be inserted automatically. This saves you from having to think about which template you need and allows you to use a single hotkey for multiple templates.\nWith dynamic templates set up, you can move your templates into more relevant notes – like canonical notes described in the Context-subject framework – for even greater efficiency.\nWhy Dynamic Templates\nFirst, dynamic templates are a great way to manage your notes. This templating strategy can bind a template with the current folder. So if you trigger the dynamic template, and the note you are working in is in Folder A, then Template A is inserted. Meanwhile, if the note is in Folder B, then Template B is inserted. This allows you to use a single hotkey(keyboard shortcut) for multiple templates and prevents thinking about which template you need.\nIn short, dynamic templates give users more control over their note management while providing an easy way to insert commonly used templates into their documents using keyboard shortcuts.\nHow it works\nImagine you’re working in a note named “My Vacation Plans” stored in a folder named “Travel.” You trigger the dynamic template code, and it starts searching for templates.\nFirst, it searches for a section named “template” within the current note. If it exists, it renders it and returns its content as the template. In this case, there is no section named “template” within the current note, so it moves on to the next step.\nNext, it searches for a note named “+template” in the same folder as the current note. If such a note exists, it returns the entire file as the template. In our example, there is no “+template” note in the Travel folder, so again we move on.\nThe third step is to search for a note with the current folder name, known as the “Canonical Note.” So in our example, obsidian would look for a note named “+Travel” inside of which there might be a section called “template.” If such a file and section existed, that would be returned as our template. But since neither do we continue to step four.\nThe fourth step is to check the parent folder for a “Canonical Note.” So in our example, since the Travel folder is inside of a folder called “Plans”, the dynamic template code would check to see if there was a “+Plans” note which contained a section called “template.” But alas, there is not, so we arrive at our final step.\nThe last step is simply to return a message that no matching template was found.\nHowever, if a template existed in any of the above locations, it would be returned by the dynamic template code.\nWhat you need to get started\n\nYou need to install the Templater plugin to use dynamic templates.\nThe dynamic template code (below) can be copied &amp; pasted into your template file. This should be in the same folder specified by your Templater plugin configuration.\nI recommended naming the template file something with a preceding underscore, like _insert dynamic template, so that it appears first in your list of templates.\nNow you can start moving your templates into more relevant notes, like the “Canonical note” described in the Context-subject framework.\n\n&lt;%*\nlet render_dynamic_template = async function () {\n\tlet dtemplate;\n  if (tp.file.content.contains(&quot;# template&quot;)) {\n    dtemplate = (await tp.file.include(&quot;[[&quot;+tp.file.title+&quot;#template|template]]&quot;));\n    return dtemplate.remove_template_header()\n  }\n  const context_folder_name = tp.file.folder(false);\n  if(tp.file.exists(context_folder_name+&quot;/+template&quot;)) {\n    dtemplate = (await tp.file.include(&quot;[[&quot;+context_folder_name+&quot;/+template]]&quot;));\n    return dtemplate\n      .remove_template_header()\n  }\n  const context_doc_name = has_context_doc();\n\tif (context_doc_name){\n\t\tdtemplate = (await tp.file.include(&quot;[[&quot;+context_doc_name+ &quot;#template|template]]&quot;));\n    if (dtemplate.contains(&quot;# template&quot;)) {\n\t\t\treturn dtemplate\n        .remove_template_header()\n\t\t}\n  }\n  const context_group_doc_name = has_context_group_doc();\n  if(context_group_doc_name){\n    dtemplate = (await tp.file.include(&quot;[[&quot;+context_group_doc_name+ &quot;#template|template]]&quot;));\n\t\tif(dtemplate !== tp.file.content){\n\t\t\treturn dtemplate\n        .remove_template_header()\n\t\t}\n  }\n  return &quot;No matching template.&quot;\n  function has_context_doc() {\n    let context_doc_name = context_folder_name;\n    if(!tp.file.exists(context_doc_name)){\n      context_doc_name = &quot;+&quot; + context_doc_name;\n    }\n    if (tp.file.exists(context_doc_name)) {\n      return context_doc_name\n    }\n    return false\n  }\n  function has_context_group_doc() {\n    let parent = tp.file.folder(true).replace(/\\/[^/]+/, &quot;&quot;);\n    if (!tp.file.exists(parent)) {\n      parent = &quot;+&quot; + parent;\n    }\n    if (tp.file.exists(parent)) {\n      return parent\n    }\n    return false\n  }\n}\nString.prototype.remove_template_header = function() {\n  return this.replace(/^#+ template\\s*/, &quot;&quot;)\n}\n-%&gt;\n&lt;%_ await render_dynamic_template() _%&gt;\n\n\nDynamic templates are extremely useful for managing your notes, allowing you to easily bind a template to the current folder. This prevents you from thinking about which template you need and allows you to use a single hotkey for multiple templates.\nDynamic templates are worth considering if you’re looking for a way to make your note-taking more efficient and organized."},"what-are-embeddings":{"title":"What are Embeddings: A Simple Guide to Understanding Them","links":[],"tags":["AI","Education","Embeddings"],"content":"\nHave you ever wondered how computers can understand complex things like human language or images? The secret is something called “embeddings.” But what are embeddings, and why are they important? In this blog post, we’ll explain embeddings in a simple, easy-to-understand way.\nWhat are Embeddings?\nImagine you’re trying to explain the concept of an apple to a computer. Instead of saying, “an apple is a red or green fruit,” you give it a list of numbers representing the apple. These numbers help the computer understand the apple and how it’s related to other things, like oranges or bananas. This list of numbers is an embedding.\nEmbeddings are like magic recipes that turn complex data, such as words or pictures, into simple lists of numbers called vectors. They help computers understand and process this data by tracking how things are related. The closer the numbers are, the more similar the things they represent.\nWhy are Embeddings Important in AI?\nEmbeddings play a big part in artificial intelligence (AI). They help computers understand human languages for translation, sentiment analysis, and text classification tasks. By turning words, sentences, or even whole documents into embeddings, computers can understand what we’re saying and respond in ways that make sense.\nThere are three main types of embeddings:\n\n\nWord embeddings: These represent individual words as vectors. They help computers understand the meaning of words based on the context they’re used in.\n\n\nSentence embeddings: These represent entire sentences as vectors. They help computers understand the meaning of sentences and the relationships between words in them.\n\n\nDocument embeddings: These represent whole documents as vectors. They help computers understand the overall topic and structure of the document.\n\n\nHow are Embeddings Used?\nEmbeddings have many uses and applications. Here are some examples:\nSearch\n\n\nSemantic search: Embeddings help computers understand the meaning behind what people are looking for, making search results more relevant.\n\n\nQuery expansion: Embeddings can identify related words and phrases, making searches more comprehensive and accurate.\n\n\nClustering\n\n\nTopic modeling: Grouping texts based on their meaning, using embeddings to find similarities.\n\n\nVisual analytics: Making complex data easier to see and understand by showing it in simpler, lower-dimensional spaces.\n\n\nRecommendation Systems\n\n\nCollaborative filtering: Using embeddings to understand user preferences and recommend items based on similarities.\n\n\nContent-based filtering: Identifying similar content by representing items and users with embeddings, allowing for personalized recommendations.\n\n\nAnomaly Detection\n\n\nOutlier detection: Finding unusual data points by comparing their embeddings to the rest of the dataset, helping to detect fraud, spam, or other malicious activities.\n\n\nPredictive maintenance: Monitoring equipment behavior using embeddings and detecting anomalies that might signal potential failures or issues.\n\n\nDiversity Measurement\n\n\nDocument diversity: Analyzing text data using embeddings to measure the variety of topics, styles, or viewpoints within a set of documents.\n\n\nPopulation diversity: Assessing the diversity of a group, like employees, based on various attributes, such as skills or background, by converting these attributes into embeddings.\n\n\nClassification\n\n\nSentiment analysis: Assigning emotions, like positive, negative, or neutral, to text by using embeddings as input features for machine learning algorithms or deep learning models, like neural networks.\n\n\nLabeling: Categorizing text or data points based on their embeddings, which helps in tasks like organizing information, filtering content, or tagging items for easier retrieval.\n\n\nConclusion\nEmbeddings are a powerful tool that helps computers make sense of complex data like human language and images. They play a crucial role in AI by turning words, sentences, and documents into simple lists of numbers, which makes it easier for machines to understand and process. From search engines to recommendation systems and beyond, embeddings have many applications that make our lives easier and more efficient. So, next time you marvel at how an AI can understand your words, remember that embeddings are working their magic behind the scenes!"},"why-prompt-engineering-isnt-going-anywhere":{"title":"Why Prompt Engineering Isn't Going Anywhere","links":[],"tags":["GPT-3","Prompt-Engineering","Prompt-Writing"],"content":"\nThere’s been a lot of talk about the role of prompts in artificial intelligence (AI). Some people believe prompt engineering is temporary because AI models have yet to be sufficiently developed. However, I believe that prompt engineering is here to stay.\nTL;DR:\n\nPrompts explicitly define context, so without a prompt, AI output is bound by its ability to “sense” implicit context.\nAs humans, we believe we have control, and prompt engineering will be one of the last things we continue to control in a world of artificial intelligence.\nPrompt engineering allows an easier way to iterate than fine-tuning or training, and it’s non-destructive, so you can better rely on past prompts performing how you would expect.\n\nPrompts Explicitly Define Context\nIn a world of artificial intelligence, prompts play an important role in defining the context for a particular output. Without a prompt, an output is bound by its ability to “sense” implicit context. This can be quite limiting. Prompts provide a way to explicitly define the context for an output. And right now, prompts are the only context considered by the most popular models, like GPT-3.\nPrompts provide that clarity by defining the parameters within which the model must operate. Prompts are the interface between humans and language models. They provide a way for us to give machines the information they need to perform a task while constraining the possible outcomes. This makes prompts essential for ensuring that an AI system behaves as expected.\nFor example, let’s say you are asking a question. The answer may not be sufficient after your first ask if you don’t provide context. Prompt engineering allows you to get more specific with your question. Without the context of the prompt, a sufficiently advanced AI may return too much, or too little. Or it might use probability to respond to the most likely context, but that loses utility for advanced users that want deeper knowledge.\nImagine you ask your assistant to book a flight for you. Without any prompts, the assistant might assume you want to fly today and book a flight that leaves immediately. However, if you include the prompt “I need a flight next week,” the assistant will book a flight that leaves next week instead. And in this case, incorrect context from the lack of prompting can turn into a costly mistake.\nPrompts help ensure that an AI model’s output is what we expect it to be. They provide a level of control that would otherwise be lacking in a world where machines are making more and more decisions for us.\nHumans Crave Control\nEven as AI models become more advanced, there will always be a need for prompts in some situations. Why? Because prompts offer explicit control over an AI model’s output, humans will continue to crave control. In a future dominated by artificial intelligence, prompt engineering will be one of the last things we continue to control. This is one of the main reasons why prompt engineering is here to stay.\nAs much as we’d like to believe that we can create autonomous systems, the reality is that humans still crave control. Prompts allow us to exert some degree of control over how an AI system behaves while allowing it some autonomy.\nWe like to believe that we’re in control. It’s a core part of our identity as human beings. And when it comes to the technology we use, we want to be able to control it. That’s why prompt engineering will be one of the last things we continue to control in a world of artificial intelligence.\nAs AI models become more advanced, they will increasingly be able to do things without our prompting. But there will always be areas where humans want or need to retain control. For example, let’s say you’re using a language model to generate product descriptions for an e-commerce site. You don’t want the model spitting out overly generalized content when your brand was built for a specific target audience. So you use prompts to constrain the output and ensure that what it produces accurately reflects the brand.\nIn these cases, prompts allow us to take back control and tell the AI what we want it to do. So while AI models may theoretically surpass our ability to provide them with useful prompts, there will always be situations where humans crave – and need –control.\nPrompt Engineering is Easier to Iterate\nPrompt engineering is an easier way to iterate compared to fine-tuning or training. You’re constantly changing your models and making small tweaks when you’re fine-tuning. This can be time-consuming and difficult to keep track of. With prompt engineering, you can rely on past prompts that have performed well and iterate from there.\nWhat’s more, prompt engineering is non-destructive. So if you make a change that doesn’t work out, you can easily go back and try something else without starting from scratch.\nIn the end, prompt engineering allows for faster iteration and better results. And that’s another reason why it’s here to stay.\nSome Prompt Types May Lose Value\nSome prompt types may eventually lose value and become obsolete. For example, k-shot prompts containing a fixed number of examples may become less valuable as zero-shot prompts become better understood and AI models become better trained.\nSo the types of prompts used may change as models become more sophisticated.\nThe Future Of Prompt Engineering\nAs we continue to develop artificial intelligence, it’s important to consider the role that prompt engineering will play in our future.\nSo what does the future hold for prompt engineering? I believe it will play an important role in AI development as we strive to create more autonomous systems. Prompt engineering is a crucial tool that allows us to control the behavior of AI models, and it’s something that we will continue to rely on as AI evolves. In the future, prompt engineering may become even more important as we strive to create smarter and more efficient AI systems.\nAs AI models become more advanced, the need for prompts may lessen for the average user while their value increases for advanced users."},"why-your-medium-sized-business-needs-a-chief-ai-officer":{"title":"Why Your Medium-Sized Business Needs a Chief AI Officer","links":[],"tags":["AI","AI-Your-Business","Future","Prediction"],"content":"\nI want to expand on the recent eye-opening piece Why every Fortune 500 business needs a Chief AI Officer (CAIO).\nDylan Fox makes it crystal clear that even the giants of the business world can’t afford to rest on their laurels when it comes to AI. If they need a CAIO to stay competitive, you can bet that medium size businesses do too. The message here is universal: integrating AI into every aspect of a company is no longer optional—it’s a necessity.\nAs a consultant in the AI space, I’ve been watching how developments in AI have become an unstoppable force in 2023. Medium-size businesses that don’t dive headfirst into AI will be left in the dust. So it’s time to step up your game and hire a Chief AI Officer (CAIO). Here’s why.\nWhy AI is Essential for Medium Size Businesses\nIf you’re not harnessing the power of AI, you’re just not keeping up. AI unlocks insane levels of operational efficiencies and lets you automate those mind-numbing, repetitive tasks. Trust me, the teams that embrace AI will crush those that rely solely on human work. And let’s not forget the consumers—they expect AI-powered features and products from every company they interact with. You can’t afford to disappoint them.\nThe Importance of Hiring a Chief AI Officer\nLook, I get it. You’ve got a team of incredible data scientists and machine-learning researchers. But that’s not enough. AI is more than an afterthought to be tacked onto isolated use cases initiated in an outdated technological environment. You must build it into the core of your company’s strategic direction. That’s where the CAIO comes in.\nA CAIO will help your medium size business stay ahead of the curve, identifying key areas in your operating pipelines where you can effectively apply AI. And guess what? Customers are expecting you to utilize AI. You want to keep that trust and reputation with your customers. So you need a CAIO to make it happen.\nResponsibilities of the CAIO\nThe CAIO’s got a lot on their plate, but it’s all for the greater good. They’ll align the AI strategy with your company’s goals and objectives, working closely with product, marketing, finance, and operations. They’ll identify those golden opportunities where AI can be used to improve efficiency and drive growth across the operating plane.\nYou need a CAIO with experience overseeing large, diverse teams and who’s comfortable working in a fast-paced, dynamic environment. A CAIO must also be able to evaluate potential vendors and pre-built AI solutions. Finding the right person isn’t going to be a walk in the park, but the rewards will be totally worth it.\nNext Steps\nYou can’t afford to sit on the sidelines, folks. A clear AI strategy is crucial for medium size businesses to stay competitive. The CAIO will be the MVP in developing that strategy and positioning your company for long-term success. It’s time to recognize the importance of AI and bring a CAIO on board to lead the way. Don’t just survive—thrive!\nI know what you’re thinking: “This all sounds great, but how do I even begin to find and hire a CAIO?” Don’t worry, my friends—I’ve got your back. With my hands-on AI experience, I’m here to help you navigate the often-daunting process of hiring a top-notch Chief AI Officer.\nTogether, we’ll identify the ideal candidate for your business, ensuring they have the perfect blend of technical expertise, strategic vision, and leadership skills to drive your company’s AI initiatives forward. In addition, I’ll work closely with you to understand your unique needs and goals, so we can find a CAIO who’s not just a perfect fit on paper but also aligns with your company’s culture and values.\nRemember, the future of your medium size business depends on your ability to embrace AI and stay ahead of the curve. So don’t go it alone—reach out to me for expert guidance and support as you embark on this exciting journey."},"zero-shot-prompt-writing-for-large-language-models-like-gpt-3":{"title":"Zero-shot Prompt Writing for Large Language Models Like GPT-3","links":[],"tags":["GPT-3","Prompt-Engineering","Prompt-Writing"],"content":"\nA large language model (LLM) is a computer program that takes in text and outputs predictions (completions) about the next word in the text. For example, if you give the language model the text “The quick brown fox”, it might output the prediction “jumped”.\nWhat is zero-shot prompt engineering?\nZero-shot prompt engineering is the process of creating a writing prompt that can be used without any prior knowledge or training. In other words, the system can generate new examples of what it should do without being given examples beforehand. This is opposed to k-shot prompt engineering, which requires some prior knowledge or training to be used effectively.\nZero-shot prompts are different from k-shot prompts because they do not require training data, to be effective. In a k-shot prompt, you would provide several examples such that the LM completes the task by adding more examples.\nZero-shot learning allows you to learn from a wider variety of data sources. With k-shot learning, your outputs will be limited to the style of the examples you provided in the prompt. This is similar to overfitting, where better outputs are missed because they are overfitted to the examples or training data.\nexample zero-shot prompt\nZero-shot prompt engineering is\n\n\n\nOutput 1: Zero-shot prompt engineering is a field that studies how to generate natural language prompts (e.g., questions or descriptions) for a task such that a model trained on those prompts can be used zero-shot on other tasks.\nOutput 2: Zero-shot prompt engineering is the process of designing a prompt that does not require any specific prior knowledge in order to be answered.\n\nexample k-shot prompt\nBuilding language models is hard.\nDesigning generative models are hard.\nInference is easy.\nZero-shot prompt writing is\n\n\n\nOutput: hard.\n\nWhen are zero-shot prompts best?\nFor most people, zero-shot learning is better than k-shot learning.\nK-shot prompts are when you give the LLM a set of examples as instructions, and it follows the format of those examples to complete a task. This can be useful for highly repetitive tasks that don’t require much creativity.\nZero-shot prompts do not require previous examples. Instead, zero-shot prompts can be written similarly to how you would communicate with another human. This can be useful for people who want to integrate LLMs into any workflow since they don’t have to keep providing a k-shot worth of information whenever they want the LLM to do something new.\nSo, while k-shot prompts are best for highly repetitive tasks, zero-shot prompts allow people to integrate LMs into any workflow.\nHow do you write zero-shot prompts?\nSo how do you do zero-shot prompt engineering? First, you develop a task or question you want the language model to answer. For example, let’s say we want the language model to generate ideas for new businesses. So our task might be: “Generate a list of ideas for new businesses.”\nDesigning a zero-shot prompt requires careful consideration of the structure and wording of the prompt itself. The goal is to make the prompt as clear and concise as possible, while still providing enough information for the respondent to answer accurately. Additionally, zero-shot prompts should be flexible, allowing them to be easily adapted to different contexts and settings.\nHow can I learn more about writing zero-shot prompts?\nPrompt engineering, or “prompt writing” as I call it, is in its very early stages. And just like what to call it, experts have many disagreements about how to do it and whether it’s even necessary.\nWhile some people suggest sitting back and relaxing until AI can predict your every desire, I prefer to adventure out and try to use AI by prompt writing right now.\nTo my surprise, very little was written about how non-researchers can use writing prompts to utilize AI like GPT-3.\nThe most useful content seems to be examples of other people’s prompts. However, these are hard to come by.\nI’ve found one person on the bleeding edge of prompt writing, Riley Goodside, who has posted examples of prompts that stretch the imagination of what is possible.\nBesides Mr. Goodside, an OK source for prompt examples has been scanning research papers that mention the topic of prompts. Many people are already familiar with the “Let’s think step by step” sub-prompt that was proven to increase the accuracy of LLMs by this research paper.\nBesides these two sources, I suspect that the best prompt examples are currently hidden behind software-as-a-service (SaaS) applications like Jasper and paywalled content specific to those SaaS apps."}}